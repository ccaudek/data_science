[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science per psicologi",
    "section": "",
    "text": "Questo è il sito web per “Data Science per psicologi”. Viene qui presentato il materiale delle lezioni dell’insegnamento di Psicometria B000286 (A.A. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. Lo scopo di questo insegnamento è quello di fornire agli studenti un’introduzione all’analisi dei dati psicologici. Le conoscenze/competenze che verranno sviluppate in questo insegnamento sono quelle della Data Science applicata alla psicologia, ovvero, un insieme di conoscenze/competenze che si pongono all’intersezione tra psicologia, statistica e informatica."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Data Science per psicologi",
    "section": "License",
    "text": "License\nThis book was created by Corrado Caudek and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Prefazione",
    "section": "",
    "text": "Questo libro ti insegnerà i principi base della Data Science, con esempi pratici.\nSembra sensato spendere due parole su una domanda che è importante per gli studenti: perché dobbiamo perdere tanto tempo a studiare l’analisi dei dati psicologici quando in realtà quello che ci interessa è tutt’altro? Questa è una bella domanda. C’è una ragione molto semplice che dovrebbe farci capire perché la Data Science sia così importante per la psicologia. Infatti, a ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, in certi casi, predire. In questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili a quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che i problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data Science in psicologia: perché la Data Science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.\nSono sicuro che, leggendo queste righe, a molti studenti sarà venuta in mente la seguente domanda: perché non chiediamo a qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data Science? La risposta a questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data Science. Le tematiche della Data Science non possono essere ignorate né dai ricercatori in psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche i professionisti al di fuori dall’università non possono fare a meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data Science! Basta aprire a caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano i risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.\nLe considerazioni precedenti cercano di chiarire il seguente punto: la Data Science non è qualcosa da studiare a malincuore, in un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data Science in tantissimi ambiti della loro attività professionale: in particolare quando costruiscono, somministrano e interpretano i test psicometrici. È dunque chiaro che possedere delle solide basi di Data Science è un tassello imprescindibile del bagaglio professionale dello psicologo. In questo insegnamento verrano trattati i temi base della Data Science e verrà adottato un punto di vista bayesiano, che corrisponde all’approccio più recente e sempre più diffuso in psicologia."
  },
  {
    "objectID": "preface.html#come-studiare",
    "href": "preface.html#come-studiare",
    "title": "Prefazione",
    "section": "Come studiare",
    "text": "Come studiare\nIl giusto metodo di studio per prepararsi all’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare i concetti via via che essi vengono presentati e verificare in autonomia le procedure presentate a lezione. Incoraggio gli studenti a farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti a utilizzare i forum attivi su Moodle e, soprattutto, a svolgere gli esercizi proposti su Moodle. I problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino a quel punto sono sufficienti rispetto alle richieste dell’esame.\nLa prima fase dello studio, che è sicuramente individuale, è quella in cui lo studente deve acquisire le conoscenze teoriche relative ai problemi che saranno presentati all’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta lo studente ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (\\(\\textsf{R}\\)) per applicare i concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso aiuta a capire meglio."
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Parte 2: Il calcolo delle probabilità",
    "section": "",
    "text": "Nel capitolo ?sec-intro-prob-1 verrà presentata la legge dei grandi numeri, il concetto di variabile casuale e la funzione di massa di probabilità."
  },
  {
    "objectID": "023_cont_rv_distr.html",
    "href": "023_cont_rv_distr.html",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "",
    "text": "Dopo avere introdotto con una simulazione il concetto di funzione di densità nel Capitolo ?sec-intro-density-function, prendiamo ora in esame alcune delle densità di probabilità più note. La più importante di esse è sicuramente la distribuzione Normale."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-gaussiana",
    "href": "023_cont_rv_distr.html#distribuzione-gaussiana",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.1 Distribuzione Gaussiana",
    "text": "1.1 Distribuzione Gaussiana\nNon c’è un’unica distribuzione gaussiana (o normale): la distribuzione gaussiana è una famiglia di distribuzioni. Tali distribuzioni sono dette “gaussiane” in onore di Carl Friedrich Gauss (uno dei più grandi matematici della storia il quale, tra le altre cose, scoprì l’utilità di tale funzione di densità per descrivere gli errori di misurazione). Adolphe Quetelet, il padre delle scienze sociali quantitative, fu il primo ad applicare tale funzione di densità alle misurazioni dell’uomo. Karl Pearson usò per primo il termine “distribuzione normale” anche se ammise che questa espressione “ha lo svantaggio di indurre le persone a credere che le altre distribuzioni, in un senso o nell’altro, non siano normali.”\n\n1.1.1 Limite delle distribuzioni binomiali\nIniziamo con un un breve excursus storico. Nel 1733, Abraham de Moivre notò che, aumentando il numero di prove di una distribuzione binomiale, la distribuzione risultante diventava quasi simmetrica e a forma campanulare. Con 10 prove e una probabilità di successo di 0.9 in ciascuna prova, la distribuzione è chiaramente asimmetrica.\n\nN <- 10\nx <- 0:10\ny <- dbinom(x, N, 0.9)\nbinomial_limit_plot <-\n  tibble(x = x, y = y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_bar(\n    stat = \"identity\", color = \"black\", size = 0.2\n  ) +\n  xlab(\"y\") +\n  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +\n  ylab(\"Binomial(y | 10, 0.9)\")\nbinomial_limit_plot\n\n\n\nFigura 1.1: Probabilità del numero di successi in \\(N = 10\\) prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione \\(\\mbox{Bin}(y \\mid 10, 0.9)\\). Con solo dieci prove, la distribuzione è fortemente asimmetrica negativa.\n\n\n\n\nMa se aumentiamo il numero di prove di un fattore di 100 a N = 1000, senza modificare la probabilità di successo di 0.9, la distribuzione assume una forma campanulare quasi simmetrica. Dunque, de Moivre scoprì che, quando N è grande, la funzione gaussiana (che introdurremo qui sotto), nonostante sia la densità di v.a. continue, fornisce una buona approssimazione alla funzione di massa di probabilità binomiale.\n\nN <- 1000\nx <- 0:1000\ny <- dbinom(x, N, 0.9)\nbinomial_limit_plot <-\n  tibble(x = x, y = y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_bar(stat = \"identity\", color = \"black\", size = 0.2) +\n  xlab(\"y\") +\n  ylab(\"Binomial(y | 1000, 0.9)\") +\n  xlim(850, 950)\nbinomial_limit_plot\n\n\n\nFigura 1.2: Probabilità del numero di successi in \\(N = 1000\\) prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione \\(\\mbox{Bin}(y \\mid 1000, 0.9)\\). Con mille prove, la distribuzione è quasi simmetrica a forma campanulare.\n\n\n\n\nLa distribuzione Normale fu scoperta da Gauss nel 1809. Il Paragrafo successivo illustra come si possa giungere alla Normale mediante una simulazione."
  },
  {
    "objectID": "023_cont_rv_distr.html#normal-random-walk",
    "href": "023_cont_rv_distr.html#normal-random-walk",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.2 La Normale prodotta con una simulazione",
    "text": "1.2 La Normale prodotta con una simulazione\nMcElreath (2020) illustra come sia possibile giungere alla distribuzione Normale mediante una simulazione. Supponiamo che vi siano mille persone tutte allineate su una linea di partenza. Quando viene dato un segnale, ciascuna persona lancia una moneta e fa un passo in avanti oppure all’indietro a seconda che sia uscita testa o croce. Supponiamo che la lunghezza di ciascun passo vari da 0 a 1 metro. Ciascuna persona lancia una moneta 16 volte e dunque compie 16 passi.\nAlla conclusione di queste passeggiate casuali (random walk) non possiamo sapere con esattezza dove si troverà ciascuna persona, ma possiamo conoscere con certezza le caratteristiche della distribuzione delle mille distanze dall’origine. Per esempio, possiamo predire in maniera accurata la proporzione di persone che si sono spostate in avanti oppure all’indietro. Oppure, possiamo predire accuratamente la proporzione di persone che si troveranno ad una certa distanza dalla linea di partenza (es., a 1.5 m dall’origine).\nQueste predizioni sono possibili perché tali distanze si distribuiscono secondo la legge Normale. È facile simulare questo processo usando . I risultati della simulazione sono riportati nella Figura 1.3.\n\npos <-\n  replicate(100, runif(16, -1, 1)) %>%\n  as_tibble() %>%\n  rbind(0, .) %>%\n  mutate(step = 0:16) %>%\n  gather(key, value, -step) %>%\n  mutate(person = rep(1:100, each = 17)) %>%\n  group_by(person) %>%\n  mutate(position = cumsum(value)) %>%\n  ungroup()\n\nggplot(\n  data = pos,\n  aes(x = step, y = position, group = person)\n) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_line(aes(color = person < 2, alpha = person < 2)) +\n  scale_color_manual(values = c(\"gray\", \"black\")) +\n  scale_alpha_manual(values = c(1 / 5, 1)) +\n  scale_x_continuous(\n    \"Numero di passi\",\n    breaks = c(0, 4, 8, 12, 16)\n  ) +\n  labs(y = \"Posizione\") +\n  theme(legend.position = \"none\")\n\n\n\nFigura 1.3: Passeggiata casuale di 4, 8 e 16 passi. La spezzata nera indica la media delle distanze dall’origine come funzione del numero di passi.\n\n\n\n\nUn kernel density plot delle distanze ottenute dopo 4, 8 e 16 passi è riportato nella Figura 1.4. Nel pannello di destra, al kernel density plot è stata sovrapposta una densità Normale di opportuni parametri (linea tratteggiata).\n\np1 <-\n  pos %>%\n  filter(step == 4) %>%\n  ggplot(aes(x = position)) +\n  geom_line(stat = \"density\", color = \"black\") +\n  labs(title = \"4 passi\")\n\np2 <-\n  pos %>%\n  filter(step == 8) %>%\n  ggplot(aes(x = position)) +\n  geom_density(color = \"black\", outline.type = \"full\") +\n  labs(title = \"8 passi\")\n\nsd <-\n  pos %>%\n  filter(step == 16) %>%\n  summarise(sd = sd(position)) %>%\n  pull(sd)\n\np3 <-\n  pos %>%\n  filter(step == 16) %>%\n  ggplot(aes(x = position)) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = 0, sd = sd),\n    linetype = 2\n  ) +\n  geom_density(color = \"black\", alpha = 1 / 2) +\n  labs(\n    title = \"16 passi\",\n    y = \"Densità\"\n  )\n\n(p1 | p2 | p3) & coord_cartesian(xlim = c(-6, 6))\n\n\n\nFigura 1.4: Kernel density plot dei risultati della passeggiata casuale riportata nella figura precente, dopo 4, 8 e 16 passi. Nel pannello di destra, una densità Normale di opportuni parametri è sovrapposta all’istogramma lisciato.\n\n\n\n\nQuesta simulazione mostra che qualunque processo nel quale viene sommato un certo numero di valori casuali, tutti provenienti dalla medesima distribuzione, converge ad una distribuzione Normale. Non importa quale sia la forma della distribuzione di partenza: essa può essere uniforme, come nell’esempio presente, o di qualunque altro tipo. La forma della distribuzione da cui viene realizzato il campionamento determina la velocità della convergenza alla Normale. In alcuni casi la convergenza è lenta; in altri casi la convergenza è molto rapida (come nell’esempio presente).\nDa un punto di vista formale, diciamo che una variabile casuale continua \\(Y\\) ha una distribuzione Normale se la sua densità è\n\\[\nf(y; \\mu, \\sigma) = {1 \\over {\\sigma\\sqrt{2\\pi} }} \\exp \\left\\{-\\frac{(y -  \\mu)^2}{2 \\sigma^2} \\right\\},\n\\tag{1.1}\\]\ndove \\(\\mu \\in \\mathbb{R}\\) e \\(\\sigma > 0\\) sono i parametri della distribuzione.\nLa densità normale è unimodale e simmetrica con una caratteristica forma a campana e con il punto di massima densità in corrispondenza di \\(\\mu\\).\nIl significato dei parametri \\(\\mu\\) e \\(\\sigma\\) che appaiono nell’Equazione 1.1 viene chiarito dalla dimostrazione che\n\\[\n\\mathbb{E}(Y) = \\mu, \\qquad \\mathbb{V}(Y) = \\sigma^2.\n\\]\nLa rappresentazione grafica di quattro densità Normali tutte con media 0 e con deviazioni standard 0.25, 0.5, 1 e 2 è fornita nella Figura 1.5.\n\n\n\n\nFigura 1.5: Alcune distribuzioni Normali.\n\n\n\n\n\n1.2.1 Concentrazione\nÈ istruttivo osservare il grado di concentrazione della distribuzione Normale attorno alla media:\n\\[\n\\begin{align}\nP(\\mu - \\sigma < Y < \\mu + \\sigma) &= P (-1 < Z < 1) \\simeq 0.683, \\notag\\\\\nP(\\mu - 2\\sigma < Y < \\mu + 2\\sigma) &= P (-2 < Z < 2) \\simeq 0.956, \\notag\\\\\nP(\\mu - 3\\sigma < Y < \\mu + 3\\sigma) &= P (-3 < Z < 3) \\simeq 0.997. \\notag\n\\end{align}\n\\]\nSi noti come un dato la cui distanza dalla media è superiore a 3 volte la deviazione standard presenti un carattere di eccezionalità perché meno del 0.3% dei dati della distribuzione Normale presentano questa caratteristica.\nPer indicare la distribuzione Normale si usa la notazione \\(\\mathcal{N}(\\mu, \\sigma)\\).\n\n1.2.2 Funzione di ripartizione\nIl valore della funzione di ripartizione di \\(Y\\) nel punto \\(y\\) è l’area sottesa alla curva di densità \\(f(y)\\) nella semiretta \\((-\\infty, y]\\). Non esiste alcuna funzione elementare per la funzione di ripartizione\n\\[\nF(y) = \\int_{-\\infty}^y {1 \\over {\\sigma\\sqrt{2\\pi} }} \\exp \\left\\{-\\frac{(y - \\mu)^2}{2\\sigma^2} \\right\\} dy,\n\\tag{1.2}\\]\npertanto le probabilità \\(P(Y < y)\\) vengono calcolate mediante integrazione numerica approssimata. I valori della funzione di ripartizione di una variabile casuale Normale sono dunque forniti da un software.\n\nEsercizio 1.1 Usiamo \\(\\mathsf{R}\\) per calcolare la funzione di ripartizione della Normale. La funzione pnorm(q, mean, sd) restituisce la funzione di ripartizione della Normale con media mean e deviazione standard sd, ovvero l’area sottesa alla funzione di densità di una Normale con media mean e deviazione standard sd nell’intervallo \\([-\\infty, q]\\).\nPer esempio, in precedenza abbiamo detto che il 68% circa dell’area sottesa ad una Normale è compresa nell’intervallo \\(\\mu \\pm \\sigma\\). Verifichiamo per la distribuzione del QI \\(\\sim \\mathcal{N}(\\mu = 100, \\sigma = 15)\\):\n\npnorm(100+15, 100, 15) - pnorm(100-15, 100, 15)\n#> [1] 0.6826895\n\nIl 95% dell’area è compresa nell’intervallo \\(\\mu \\pm 1.96 \\cdot\\sigma\\):\n\npnorm(100 + 1.96 * 15, 100, 15) - pnorm(100 - 1.96 * 15, 100, 15)\n#> [1] 0.9500042\n\nQuasi tutta la distribuzione è compresa nell’intervallo \\(\\mu \\pm 3 \\cdot\\sigma\\):\n\npnorm(100 + 3 * 15, 100, 15) - pnorm(100 - 3 * 15, 100, 15)\n#> [1] 0.9973002\n\n\n\n1.2.3 Distribuzione Normale standard\nLa distribuzione Normale di parametri \\(\\mu = 0\\) e \\(\\sigma = 1\\) viene detta distribuzione Normale standard. La famiglia Normale è l’insieme avente come elementi tutte le distribuzioni Normali con parametri \\(\\mu\\) e \\(\\sigma\\) diversi. Tutte le distribuzioni Normali si ottengono dalla Normale standard mediante una trasformazione lineare: se \\(Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y)\\) allora\n\\[\nX = a + b Y \\sim \\mathcal{N}(\\mu_X = a+b \\mu_Y, \\sigma_X = \\left|b\\right|\\sigma_Y).\n\\]\nL’area sottesa alla curva di densità di \\(\\mathcal{N}(\\mu, \\sigma)\\) nella semiretta \\((-\\infty, y]\\) è uguale all’area sottesa alla densità Normale standard nella semiretta \\((-\\infty, z]\\), in cui \\(z = (y -\\mu_Y )/\\sigma_Y\\) è il punteggio standard di \\(Y\\). Per la simmetria della distribuzione, l’area sottesa nella semiretta \\([1, \\infty)\\) è uguale all’area sottesa nella semiretta \\((-\\infty, 1]\\) e quest’ultima coincide con \\(F(-1)\\). Analogamente, l’area sottesa nell’intervallo \\([y_a, y_b]\\), con \\(y_a < y_b\\), è pari a \\(F(z_b) - F(z_a)\\), dove \\(z_a\\) e \\(z_b\\) sono i punteggi standard di \\(y_a\\) e \\(y_b\\).\nSi ha anche il problema inverso rispetto a quello del calcolo delle aree: dato un numero \\(0 \\leq p \\leq 1\\), il problema è quello di determinare un numero \\(z \\in \\mathbb{R}\\) tale che \\(P(Z < z) = p\\). Il valore \\(z\\) cercato è detto quantile di ordine \\(p\\) della Normale standard e può essere trovato mediante un software.\n\nEsercizio 1.2 \nSupponiamo che l’altezza degli individui adulti segua la distribuzione Normale di media \\(\\mu = 1.7\\) m e deviazione standard \\(\\sigma = 0.1\\) m. Vogliamo sapere la proporzione di individui adulti con un’altezza compresa tra \\(1.7\\) e \\(1.8\\) m.\n\n\nSoluzione. Il problema ci chiede di trovare l’area sottesa alla distribuzione \\(\\mathcal{N}(\\mu = 1.7, \\sigma = 0.1)\\) nell’intervallo \\([1.7, 1.8]\\):\n\ndf <- tibble(x = seq(1.4, 2.0, length.out = 100)) %>%\n  mutate(y = dnorm(x, mean = 1.7, sd = 0.1))\n\nggplot(df, aes(x, y)) +\n  geom_area(fill = \"sky blue\") +\n  gghighlight(x < 1.8 & x > 1.7) +\n  labs(\n    x = \"Altezza\",\n    y = \"Densità\"\n  )\n\n\n\n\n\n\n\nLa risposta si trova utilizzando la funzione di ripartizione \\(F(X)\\) della legge \\(\\mathcal{N}(1.7, 0.1)\\) in corrispondenza dei due valori forniti dal problema: \\(F(X = 1.8) - F(X = 1.7)\\). Utilizzando la seguente istruzione\n\npnorm(1.8, 1.7, 0.1) - pnorm(1.7, 1.7, 0.1)\n#> [1] 0.3413447\n\notteniamo il \\(31.43\\%\\).\nIn maniera equivalente, possiamo standardizzare i valori che delimitano l’intervallo considerato e utilizzare la funzione di ripartizione della normale standardizzata. I limiti inferiore e superiore dell’intervallo sono\n\\[\nz_{\\text{inf}} = \\frac{1.7 - 1.7}{0.1} = 0, \\quad z_{\\text{sup}} = \\frac{1.8 - 1.7}{0.1} = 1.0,\n\\]\nquindi otteniamo\n\npnorm(1.0, 0, 1) - pnorm(0, 0, 1)\n#> [1] 0.3413447\n\nIl modo più semplice per risolvere questo problema resta comunque quello di rendersi conto che la probabilità richiesta non è altro che la metà dell’area sottesa dalle distribuzioni Normali nell’intervallo \\([\\mu - \\sigma, \\mu + \\sigma]\\), ovvero \\(0.683/2\\).\n\n\n1.2.3.1 Funzione di ripartizione della normale standard e funzione logistica\nSi noti che la funzione logistica (in blu), pur essendo del tutto diversa dalla Normale dal punto di vista formale, assomiglia molto alla Normale standard quando le due cdf hanno la stessa varianza.\n\ntibble(x = c(-3, 3)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = pnorm) +\n  stat_function(\n    fun = plogis,\n    args = list(scale = 0.56)\n  )"
  },
  {
    "objectID": "023_cont_rv_distr.html#teorema-del-limite-centrale",
    "href": "023_cont_rv_distr.html#teorema-del-limite-centrale",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.3 Teorema del limite centrale",
    "text": "1.3 Teorema del limite centrale\nLaplace dimostrò il teorema del limite centrale (TLC) nel 1812. Il TLC ci dice che se prendiamo una sequenza di variabili casuali indipendenti e le sommiamo, tale somma tende a distribuirsi come una Normale. Il TLC specifica inoltre, sulla base dei valori attesi e delle varianze delle v.c. che vengono sommate, quali sono i parametri della distribuzione Normale così ottenuta.\n\nTeorema 1.1 Si supponga che \\(Y = Y_1, \\dots, Y_i, \\ldots, Y_n\\) sia una sequenza di v.a. i.i.d. con \\(\\mathbb{E}(Y_i) = \\mu\\) e \\(\\mbox{SD}(Y_i) = \\sigma\\). Si definisca una nuova v.c. come:\n\\[\nZ = \\frac{1}{n} \\sum_{i=1}^n Y_i.\n\\]\nCon \\(n \\rightarrow \\infty\\), \\(Z\\) tenderà ad una Normale con lo stesso valore atteso di \\(Y_i\\) e una deviazione standard che sarà più piccola della deviazione standard originaria di un fattore pari a \\(\\frac{1}{\\sqrt{n}}\\):\n\\[\np_Z(z) \\rightarrow \\mathcal{N}\\left(z \\ \\Bigg| \\ \\mu, \\, \\frac{1}{\\sqrt{n}} \\cdot \\sigma \\right).\n\\]\n\nIl TLC può essere generalizzato a variabili che non hanno la stessa distribuzione purché siano indipendenti e abbiano aspettative e varianze finite. Molti fenomeni naturali, come l’altezza dell’uomo adulto di entrambi i generi, sono il risultato di una serie di effetti additivi relativamente piccoli, la cui combinazione porta alla normalità, indipendentemente da come gli effetti additivi sono distribuiti. Questo è il motivo per cui la distribuzione normale forniscre una buona rappresentazione della distribuzione di molti fenomeni naturali."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-chi-quadrato",
    "href": "023_cont_rv_distr.html#distribuzione-chi-quadrato",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.4 Distribuzione Chi-quadrato",
    "text": "1.4 Distribuzione Chi-quadrato\nDalla Normale deriva la distribuzione \\(\\chi^2\\). La distribuzione \\(\\chi^2_{~k}\\) con \\(k\\) gradi di libertà descrive la variabile casuale\n\\[\nZ_1^2 + Z_2^2 + \\dots + Z_k^2,\n\\]\ndove \\(Z_1, Z_2, \\dots, Z_k\\) sono variabili casuali i.i.d. che seguono la distribuzione Normale standard \\(\\mathcal{N}(0, 1)\\). La variabile casuale chi-quadrato dipende dal parametro intero positivo \\(\\nu = k\\) che ne identifica il numero di gradi di libertà. La densità di probabilità di \\(\\chi^2_{~\\nu}\\) è\n\\[\nf(x) = C_{\\nu} x^{\\nu/2-1} \\exp (-x/2), \\qquad \\text{se } x > 0,\n\\]\ndove \\(C_{\\nu}\\) è una costante positiva.\nLa Figura 1.6 mostra alcune distribuzioni Chi-quadrato variando il parametro \\(\\nu\\).\n\n\n\n\nFigura 1.6: Alcune distribuzioni Chi-quadrato.\n\n\n\n\n\n1.4.1 Proprietà\n\nLa distribuzione di densità \\(\\chi^2_{~\\nu}\\) è asimmetrica.\nIl valore atteso di una variabile \\(\\chi^2_{~\\nu}\\) è uguale a \\(\\nu\\).\nLa varianza di una variabile \\(\\chi^2_{~\\nu}\\) è uguale a \\(2\\nu\\).\nPer \\(k \\rightarrow \\infty\\), la \\(\\chi^2_{~\\nu} \\rightarrow \\mathcal{N}\\).\nSe \\(X\\) e \\(Y\\) sono due variabili casuali chi-quadrato indipendenti con \\(\\nu_1\\) e \\(\\nu_2\\) gradi di libertà, ne segue che \\(X + Y \\sim \\chi^2_m\\), con \\(m = \\nu_1 + \\nu_2\\). Tale principio si estende a qualunque numero finito di variabili casuali chi-quadrato indipendenti.\n\n\nEsercizio 1.3 Usiamo \\(\\mathsf{R}\\) per disegnare la densità chi-quadrato con 3 gradi di libertà dividendo l’area sottesa alla curva di densità in due parti uguali.\n\ndf <- tibble(x = seq(0, 15.0, length.out = 100)) %>%\n  mutate(y = dchisq(x, 3))\n\nggplot(df, aes(x, y)) +\n  geom_area(fill = \"sky blue\") +\n  gghighlight(x < 3) +\n  labs(\n    x = \"V.a. chi-quadrato con 3 gradi di libertà\",\n    y = \"Densità\"\n  )"
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-t-di-student",
    "href": "023_cont_rv_distr.html#distribuzione-t-di-student",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.5 Distribuzione \\(t\\) di Student",
    "text": "1.5 Distribuzione \\(t\\) di Student\nDalle distribuzioni Normale e Chi-quadrato deriva un’altra distribuzione molto nota, la \\(t\\) di Student. Se \\(Z \\sim \\mathcal{N}\\) e \\(W \\sim \\chi^2_{~\\nu}\\) sono due variabili casuali indipendenti, allora il rapporto\n\\[\nT = \\frac{Z}{\\Big( \\frac{W}{\\nu}\\Big)^{\\frac{1}{2}}}\n\\tag{1.3}\\]\ndefinisce la distribuzione \\(t\\) di Student con \\(\\nu\\) gradi di libertà. Si usa scrivere \\(T \\sim t_{\\nu}\\). L’andamento della distribuzione \\(t\\) di Student è simile a quello della distribuzione Normale, ma ha una maggiore dispersione (ha le code più pesanti di una Normale, ovvero ha una varianza maggiore di 1).\nLa Figura 1.7 mostra alcune distribuzioni \\(t\\) di Student variando il parametro \\(\\nu\\).\n\n\n\n\nFigura 1.7: Alcune distribuzioni \\(t\\) di Student.\n\n\n\n\n\n1.5.1 Proprietà\nLa variabile casuale \\(t\\) di Student soddisfa le seguenti proprietà:\n\nPer \\(\\nu \\rightarrow \\infty\\), \\(t_{\\nu}\\) tende alla normale standard \\(\\mathcal{N}(0, 1)\\).\nLa densità della \\(t_{\\nu}\\) è una funzione simmetrica con valore atteso nullo.\nPer \\(\\nu > 2\\), la varianza della \\(t_{\\nu}\\) vale \\(\\nu/(\\nu - 2)\\); pertanto è sempre maggiore di 1 e tende a 1 per \\(\\nu \\rightarrow \\infty\\)."
  },
  {
    "objectID": "023_cont_rv_distr.html#funzione-beta",
    "href": "023_cont_rv_distr.html#funzione-beta",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.6 Funzione beta",
    "text": "1.6 Funzione beta\nLa funzione beta di Eulero è una funzione matematica, non una densità di probabilità. La menzioniamo qui perché viene utilizzata nella distribuzione Beta. La funzione beta si può scrivere in molti modi diversi; per i nostri scopi la scriveremo così:\n\\[\nB(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\,,\n\\tag{1.4}\\]\ndove \\(\\Gamma(x)\\) è la funzione Gamma, ovvero il fattoriale discendente, cioè\n\\[\n(x-1)(x-2)\\ldots (x-n+1)\\notag\\,.\n\\]\n\nEsercizio 1.4 \nPer esempio, posti \\(\\alpha = 3\\) e \\(\\beta = 9\\), la funzione beta assume il valore\n\nalpha <- 3\nbeta <- 9\nbeta(alpha, beta)\n#> [1] 0.002020202\n\nPer chiarire, lo stesso risultato si ottiene con\n\n((2) * (8 * 7 * 6 * 5 * 4 * 3 * 2)) / \n  (11 * 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2)\n#> [1] 0.002020202\n\novvero\n\ngamma(alpha) * gamma(beta) / gamma(alpha + beta)\n#> [1] 0.002020202"
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-beta",
    "href": "023_cont_rv_distr.html#distribuzione-beta",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.7 Distribuzione Beta",
    "text": "1.7 Distribuzione Beta\nLa distribuzione Beta è una distribuzione usata per modellare percentuali e proporzioni in quanto è definita sull’intervallo \\((0; 1)\\) – ma non include i valori 0 o 1.\n\nDefinizione 1.1 Sia \\(\\pi\\) una variabile casuale che può assumere qualsiasi valore compreso tra 0 e 1, cioè \\(\\pi \\in [0, 1]\\). Diremo che \\(\\pi\\) segue la distribuzione Beta di parametri \\(\\alpha\\) e \\(\\beta\\), \\(\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\), se la sua densità è\n\\[\n\\begin{align}\n\\text{Beta}(\\pi \\mid \\alpha, \\beta) &= \\frac{1}{B(\\alpha, \\beta)}\\pi^{\\alpha-1} (1-\\pi)^{\\beta-1}\\notag\\\\\n&=  \\frac{\\Gamma(\\alpha+ \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\quad \\text{per } \\pi \\in [0, 1]\\,,\n\\end{align}\n\\tag{1.5}\\]\nladdove \\(B(\\alpha, \\beta)\\) è la funzione beta.\n\nI termini \\(\\alpha\\) e \\(\\beta\\) sono i parametri della distribuzione Beta e devono essere entrambi positivi. Tali parametri possono essere interpretati come l’espressione delle nostre credenze a priori relative ad una sequenza di prove Bernoulliane. Il parametro \\(\\alpha\\) rappresenta il numero di “successi” e il parametro \\(\\beta\\) il numero di “insuccessi”:\n\\[\n\\frac{\\text{Numero di successi}}{\\text{Numero di successi} + \\text{Numero di insuccessi}} = \\frac{\\alpha}{\\alpha + \\beta}\\notag\\,.\n\\]\nIl rapporto \\(\\frac{1}{B(\\alpha, \\beta)} = \\frac{\\Gamma(\\alpha+b)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\) è una costante di normalizzazione:\n\\[\n\\int_0^1 \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} = \\frac{\\Gamma(\\alpha+b)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\,.\n\\]\nAd esempio, con \\(\\alpha = 3\\) e \\(\\beta = 9\\) abbiamo\n\na <- 3\nb <- 9\nintegrand <- function(p) {p^{a - 1}*(1 - p)^{b - 1}}\nintegrate(integrand, lower = 0, upper = 1)\n#> 0.002020202 with absolute error < 2.2e-17\n\novvero\n\n1 / (gamma(a + b) / (gamma(a) * gamma(b)))\n#> [1] 0.002020202\n\novvero\n\nbeta(alpha, beta)\n#> [1] 0.002020202\n\nIl valore atteso, la moda e la varianza di una distribuzione Beta sono dati dalle seguenti equazioni:\n\\[\n\\mathbb{E}(\\pi) = \\frac{\\alpha}{\\alpha+\\beta}\\,,\n\\tag{1.6}\\]\n\\[\n\\mbox{Mo}(\\pi) = \\frac{\\alpha-1}{\\alpha+\\beta-2}\\,,\n\\tag{1.7}\\]\n\\[\n\\mathbb{V}(\\pi) = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)}\\,.\n\\tag{1.8}\\]\nAl variare di \\(\\alpha\\) e \\(\\beta\\) si ottengono molte distribuzioni di forma diversa; un’illustrazione è fornita dalla seguente GIF animata.\nSi può ottenere una rappresentazione grafica della distribuzione \\(\\mbox{Beta}(\\pi \\mid \\alpha, \\beta)\\) con la funzione plot_beta() del pacchetto bayesrules. Per esempio:\n\nbayesrules::plot_beta(alpha = 3, beta = 9)\n\n\n\n\n\n\n\nLa funzione bayesrules::summarize_beta() ci restituisce la media, moda e varianza della distribuzione Beta. Per esempio:\n\nbayesrules::summarize_beta(alpha = 3, beta = 9)\n#>   mean mode        var        sd\n#> 1 0.25  0.2 0.01442308 0.1200961\n\n\nNota. Attenzione alle parole: in questo contesto, il termine “beta” viene utilizzato con tre significati diversi:\n\nla distribuzione di densità Beta,\nla funzione matematica beta,\nil parametro \\(\\beta\\).\n\n\n\nEsercizio 1.5 \nNel disturbo depressivo la recidiva è definita come la comparsa di un nuovo episodio depressivo che si manifesta dopo un prolungato periodo di recupero (6-12 mesi) con stato di eutimia (umore relativamente normale). Supponiamo che una serie di studi mostri una comparsa di recidiva in una proporzione che va dal 20% al 60% dei casi, con una media del 40% (per una recente discussione, si veda Nuggerud-Galeas et al., 2020). Sulla base di queste informazioni, si usi la distribuzione Beta per rappresentare le credenze a priori relativamente alla probabilità di recidiva.\n\n\nSoluzione. Il problema richiede di trovare i parametri della distribuzione Beta tali per cui la massa della densità sia compresa tra 0.2 e 0.6, con la media in corrispondenza di 0.4. Procedendo per tentativi ed errori, ed usando la funzione bayesrules::plot_beta(), un risultato possibile è \\(\\mbox{Beta}(16, 24)\\).\nLa funzione find_pars() prende in input la media e \\(\\alpha + \\beta\\), ritorna i valori dei parametri:\n\nfind_pars <- function(ev, n) {\n  a <- ev * n\n  b <- n - a\n  return(c(round(a), round(b)))\n}\n\npars <- find_pars(.4, 40)\npars\n#> [1] 16 24\nbayesrules::plot_beta(pars[1], pars[2])\n\n\n\n\n\n\n\nVerifichiamo il valore della media della distribuzione:\n\n16 / (16 + 24)\n#> [1] 0.4\n\nLa moda è\n\n(16 - 1) / (16 + 24 - 2)\n#> [1] 0.3947368\n\nLa deviazione standard della distribuzione è uguale a circa 8 punti percentuali:\n\nsqrt((16 * 24) / ((16 + 24)^2 * (16 + 24 + 1)))\n#> [1] 0.07650921\n\nGli stessi risultati si ottengono usando la funzione bayesrules::summarize_beta():\n\nbayesrules::summarize_beta(alpha = 16, beta = 24)\n#>   mean      mode         var         sd\n#> 1  0.4 0.3947368 0.005853659 0.07650921\n\nPossiamo concludere dicendo che, se utilizziamo la distribuzione \\(\\mbox{Beta}(16, 24)\\) per rappresentare le nostre credenze (a priori) rispetto la possibilità di recidiva, ciò significa che pensiamo che la nostra incertezza sia quantificabile nei termini di una deviazione standard di circa 8 punti percentuali rispetto a tutti i valori possibili di recidiva, per i quali il valore più verosimile (ovvero, la media della distribuzione) è 0.40.\n\n\nEsercizio 1.6 \nPoniamoci ora il problema di verificare la nostra comprensione delle funzioni \\(\\mathsf{R}\\) che possono essere usate per la funzione Beta. Si risolva l’Esercizio 1.5 usando \\(\\mathsf{R}\\).\n\n\nSoluzione. Utilizziamo i seguenti parametri per la distribuzione Beta:\n\nalpha <- 16\nbeta <- 24\n\nLa media di una \\(\\mbox{Beta}(16, 24)\\) è\n\nalpha / (alpha + beta)\n#> [1] 0.4\n\nIn corrispondenza della media la densità della funzione è\n\ndbeta(pi, alpha, beta)\n#> [1] 0\n\novvero\n\ngamma(alpha + beta) / (gamma(alpha) * gamma(beta)) * \n  pi^(alpha - 1) * (1 - pi)^(beta - 1)\n#> [1] -6.99499e+26\n\nUsando la funzione dbeta() possiamo costruire un grafico della funzione \\(\\mbox{Beta}(16, 24)\\) nel modo seguente:\n\nx <- seq(0, 1, length.out = 1e4)\ntibble(x) %>% \n  ggplot(aes(x, dbeta(x, alpha, beta))) +\n  geom_line() +\n  labs(\n    x = \"Probabilità di recidiva\",\n    y = \"Densità Beta(16, 24)\"\n  )"
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-di-cauchy",
    "href": "023_cont_rv_distr.html#distribuzione-di-cauchy",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.8 Distribuzione di Cauchy",
    "text": "1.8 Distribuzione di Cauchy\nLa distribuzione di Cauchy è un caso speciale della distribuzione di \\(t\\) di Student con 1 grado di libertà. È definita da una densità di probabilità che corrisponde alla seguente funzione, dipendente da due parametri \\(\\theta\\) e \\(d\\) (con la condizione \\(d > 0\\)),\n\\[\nf(x; \\theta, d) = \\frac{1}{\\pi d} \\frac{1}{1 + \\left(\\frac{x - \\theta}{d} \\right)^2},\n\\tag{1.9}\\]\ndove \\(\\theta\\) è la mediana della distribuzione e \\(d\\) ne misura la larghezza a metà altezza."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-log-normale",
    "href": "023_cont_rv_distr.html#distribuzione-log-normale",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.9 Distribuzione log-normale",
    "text": "1.9 Distribuzione log-normale\nSia \\(y\\) una variabile casuale avente distribuzione normale con media \\(\\mu\\) e varianza \\(\\sigma^2\\). Definiamo poi una nuova variabile casuale \\(x\\) attraverso la relazione\n\\[\nx = e^y \\quad \\Longleftrightarrow \\quad y = \\log x.\n\\] Il dominio di definizione della \\(x\\) è il semiasse \\(x > 0\\) e la densità di probabilità \\(f(x)\\) è data da\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\frac{1}{x} \\exp \\left\\{-\\frac{(\\log x -  \\mu)^2}{2 \\sigma^2} \\right\\}.\n\\tag{1.10}\\]\nQuesta funzione di densità è chiamata log-normale.\nIl valore atteso e la varianza di una distribuzione log-normale sono dati dalle seguenti equazioni:\n\\[\n\\mathbb{E}(x) = \\exp \\left\\{\\mu + \\frac{\\sigma^2}{2} \\right\\}.\n\\]\n\\[\n\\mathbb{V}(x) = \\exp \\left\\{2 \\mu + \\sigma^2 \\right\\} \\left(\\exp \\left\\{\\sigma^2 \\right\\}  -1\\right).\n\\]\nSi può dimostrare che il prodotto di variabili casuali log-normali ed indipendenti segue una distribuzione log-normale."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-di-pareto",
    "href": "023_cont_rv_distr.html#distribuzione-di-pareto",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.10 Distribuzione di Pareto",
    "text": "1.10 Distribuzione di Pareto\nLa distribuzione paretiana (o distribuzione di Pareto) è una distribuzione di probabilità continua e così chiamata in onore di Vilfredo Pareto. La distribuzione di Pareto è una distribuzione di probabilità con legge di potenza utilizzata nella descrizione di fenomeni sociali e molti altri tipi di fenomeni osservabili. Originariamente applicata per descrivere la distribuzione del reddito in una società, adattandosi alla tendenza che una grande porzione di ricchezza è detenuta da una piccola frazione della popolazione, la distribuzione di Pareto è diventata colloquialmente nota e indicata come il principio di Pareto, o “regola 80-20”. Questa regola afferma che, ad esempio, l’80% della ricchezza di una società è detenuto dal 20% della sua popolazione. Viene spesso applicata nello studio della distribuzione del reddito, della dimensione dell’impresa, della dimensione di una popolazione e nelle fluttuazioni del prezzo delle azioni.\nLa densità di una distribuzione di Pareto è\n\\[\nf(x)=(x_m/x)^\\alpha,\n\\tag{1.11}\\]\ndove \\(x_m\\) (parametro di scala) è il minimo (necessariamente positivo) valore possibile di \\(X\\) e \\(\\alpha\\) è un parametro di forma.\n\n\n\n\n\n\n\n\nLa distribuzione di Pareto ha una asimmetria positiva. Il supporto della distribuzione di Pareto è la retta reale positiva. Tutti i valori devono essere maggiori del parametro di scala \\(x_m\\), che è in realtà un parametro di soglia."
  },
  {
    "objectID": "023_cont_rv_distr.html#commenti-e-considerazioni-finali",
    "href": "023_cont_rv_distr.html#commenti-e-considerazioni-finali",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nIn questa dispensa le densità continue che useremo più spesso sono la distribuzione gaussiana e la distribuzione Beta. Faremo un uso limitato della distribuzione \\(t\\) di Student e della distribuzione di Cauchy. Le altre distribuzioni qui descritte sono stato presentate per completezza.\n\n\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nNuggerud-Galeas, S., Sáez-Benito Suescun, L., Berenguer Torrijo, N., Sáez-Benito Suescun, A., Aguilar-Latorre, A., Magallón Botaya, R., & Oliván Blázquez, B. (2020). Analysis of depressive episodes, their recurrence and pharmacologic treatment in primary care patients: A retrospective descriptive study. Plos One, 15(5), e0233454."
  },
  {
    "objectID": "999_refs.html",
    "href": "999_refs.html",
    "title": "Riferimenti bibliografici",
    "section": "",
    "text": "McElreath, R. (2020). Statistical rethinking: A\nBayesian course with examples in R and\nStan (2nd Edition). CRC Press.\n\n\nNuggerud-Galeas, S., Sáez-Benito Suescun, L., Berenguer Torrijo, N.,\nSáez-Benito Suescun, A., Aguilar-Latorre, A., Magallón Botaya, R., &\nOliván Blázquez, B. (2020). Analysis of depressive episodes, their\nrecurrence and pharmacologic treatment in primary care patients: A\nretrospective descriptive study. Plos One, 15(5),\ne0233454."
  }
]