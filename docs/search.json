[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science per psicologi",
    "section": "",
    "text": "Questo è il sito web per “Data Science per psicologi”. Viene qui presentato il materiale delle lezioni dell’insegnamento di Psicometria B000286 (A.A. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. Lo scopo di questo insegnamento è quello di fornire agli studenti un’introduzione all’analisi dei dati psicologici. Le conoscenze/competenze che verranno sviluppate in questo insegnamento sono quelle della Data Science applicata alla psicologia, ovvero, un insieme di conoscenze/competenze che si pongono all’intersezione tra psicologia, statistica e informatica."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Data Science per psicologi",
    "section": "License",
    "text": "License\nThis book was created by Corrado Caudek and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Prefazione",
    "section": "",
    "text": "Questo libro ti insegnerà i principi base della Data Science, con esempi pratici.\nSembra sensato spendere due parole su una domanda che è importante per gli studenti: perché dobbiamo perdere tanto tempo a studiare l’analisi dei dati psicologici quando in realtà quello che ci interessa è tutt’altro? Questa è una bella domanda. C’è una ragione molto semplice che dovrebbe farci capire perché la Data Science sia così importante per la psicologia. Infatti, a ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, in certi casi, predire. In questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili a quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che i problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data Science in psicologia: perché la Data Science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.\nSono sicuro che, leggendo queste righe, a molti studenti sarà venuta in mente la seguente domanda: perché non chiediamo a qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data Science? La risposta a questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data Science. Le tematiche della Data Science non possono essere ignorate né dai ricercatori in psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche i professionisti al di fuori dall’università non possono fare a meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data Science! Basta aprire a caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano i risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.\nLe considerazioni precedenti cercano di chiarire il seguente punto: la Data Science non è qualcosa da studiare a malincuore, in un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data Science in tantissimi ambiti della loro attività professionale: in particolare quando costruiscono, somministrano e interpretano i test psicometrici. È dunque chiaro che possedere delle solide basi di Data Science è un tassello imprescindibile del bagaglio professionale dello psicologo. In questo insegnamento verrano trattati i temi base della Data Science e verrà adottato un punto di vista bayesiano, che corrisponde all’approccio più recente e sempre più diffuso in psicologia."
  },
  {
    "objectID": "preface.html#come-studiare",
    "href": "preface.html#come-studiare",
    "title": "Prefazione",
    "section": "Come studiare",
    "text": "Come studiare\nIl giusto metodo di studio per prepararsi all’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare i concetti via via che essi vengono presentati e verificare in autonomia le procedure presentate a lezione. Incoraggio gli studenti a farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti a utilizzare i forum attivi su Moodle e, soprattutto, a svolgere gli esercizi proposti su Moodle. I problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino a quel punto sono sufficienti rispetto alle richieste dell’esame.\nLa prima fase dello studio, che è sicuramente individuale, è quella in cui lo studente deve acquisire le conoscenze teoriche relative ai problemi che saranno presentati all’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta lo studente ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (\\(\\textsf{R}\\)) per applicare i concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso aiuta a capire meglio."
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Parte 2: Il calcolo delle probabilità",
    "section": "",
    "text": "Nel capitolo ?sec-intro-prob-1 verrà presentata la legge dei grandi numeri, il concetto di variabile casuale e la funzione di massa di probabilità."
  },
  {
    "objectID": "022_discr_rv_distr.html",
    "href": "022_discr_rv_distr.html",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "",
    "text": "In questo Capitolo verranno esaminate le principali distribuzioni di probabilità delle variabili casuali discrete. Un esperimento casuale che può dare luogo a solo due possibili esiti (successo, insuccesso) è modellabile con una variabile casuale di Bernoulli. Una sequenza di prove di Bernoulli costituisce un processo Bernoulliano. Il numero di successi dopo \\(n\\) prove di Bernoulli corrisponde ad una variabile casuale che segue la legge binomiale. La distribuzione binomiale risulta da un insieme di prove di Bernoulli solo se il numero totale \\(n\\) è fisso per disegno. Se il numero di prove è esso stesso una variabile casuale, allora il numero di successi nella corrispondente sequenza di prove bernoulliane segue al distribuzione di Poisson. Concluderemo con la distribuzione discreta uniforme."
  },
  {
    "objectID": "022_discr_rv_distr.html#una-prova-bernoulliana",
    "href": "022_discr_rv_distr.html#una-prova-bernoulliana",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "\n1.1 Una prova Bernoulliana",
    "text": "1.1 Una prova Bernoulliana\nSe un esperimento casuale ha solo due esiti possibili, allora le repliche indipendenti di questo esperimento sono chiamate “prove Bernoulliane” (il lancio di una moneta è il tipico esempio).\n\nDefinizione 1.1 Viene detta variabile di Bernoulli una variabile casuale discreta \\(Y = \\{0, 1\\}\\) con la seguente distribuzione di probabilità:\n\\[\nP(Y \\mid \\theta) =\n  \\begin{cases}\n    \\theta     & \\text{se $Y = 1$}, \\\\\n    1 - \\theta & \\text{se $Y = 0$},\n  \\end{cases}\n\\]\ncon \\(0 \\leq \\theta \\leq 1\\). Convenzionalmente l’evento \\(\\{Y = 1\\}\\) con probabilità \\(\\theta\\) viene chiamato “successo” mentre l’evento \\(\\{Y = 0\\}\\) con probabilità \\(1-\\theta\\) viene chiamato “insuccesso”.\n\nApplicando l’operatore di valore atteso e di varianza, otteniamo\n\\[\n\\begin{align}\n\\mathbb{E}(Y) &= 0 \\cdot P(Y=0) + 1 \\cdot P(Y=1) = \\theta, \\\\\n\\mathbb{V}(Y) &= (0 - \\theta)^2 \\cdot P(Y=0) + (1 - \\theta)^2 \\cdot P(Y=1) = \\theta(1-\\theta).\n\\end{align}\n\\tag{1.1}\\]\nScriviamo \\(Y \\sim \\mbox{Bernoulli}(\\theta)\\) per indicare che la variabile casuale \\(Y\\) ha una distribuzione Bernoulliana di parametro \\(\\theta\\).\n\nEsercizio 1.1 \nNel caso del lancio di una moneta equilibrata la variabile casuale di Bernoulli assume i valori \\(0\\) e \\(1\\). La distribuzione di massa di probabilità è pari a \\(\\frac{1}{2}\\) in corrispondenza di entrambi i valori. La funzione di distribuzione vale \\(\\frac{1}{2}\\) per \\(Y = 0\\) e \\(1\\) per \\(Y = 1\\)."
  },
  {
    "objectID": "022_discr_rv_distr.html#una-sequenza-di-prove-bernoulliane",
    "href": "022_discr_rv_distr.html#una-sequenza-di-prove-bernoulliane",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "\n1.2 Una sequenza di prove Bernoulliane",
    "text": "1.2 Una sequenza di prove Bernoulliane\nLa distribuzione binomiale è rappresentata dall’elenco di tutti i possibili numeri di successi \\(Y = \\{0, 1, 2, \\dots n\\}\\) che possono essere osservati in \\(n\\) prove Bernoulliane indipendenti di probabilità \\(\\theta\\), a ciascuno dei quali è associata la relativa probabilità. Esempi di una distribuzione binomiale sono i risultati di una serie di lanci di una stessa moneta o di una serie di estrazioni da un’urna (con reintroduzione). La distribuzione binomiale di parametri \\(n\\) e \\(\\theta\\) è in realtà una famiglia di distribuzioni: al variare dei parametri \\(\\theta\\) e \\(n\\) variano le probabilità.\n\nDefinizione 1.2 La probabilità di ottenere \\(y\\) successi e \\(n-y\\) insuccessi in \\(n\\) prove Bernoulliane è data dalla distribuzione binomiale:\n\\[\n\\begin{align}\nP(Y=y) &= \\binom{n}{y} \\theta^{y} (1-\\theta)^{n-y} \\notag \\\\\n&= \\frac{n!}{y!(n-y)!} \\theta^{y} (1-\\theta)^{n-y},\n\\end{align}\n\\tag{1.2}\\]\ndove \\(n\\) = numero di prove Bernoulliane, \\(\\theta\\) = probabilità di successo in ciascuna prova e \\(y\\) = numero di successi.\n\n\nDimostrazione. L’Equazione 1.2 può essere derivata nel modo seguente. Indichiamo con \\(S\\) il successo e con \\(I\\) l’insuccesso di ciascuna prova. Una sequenza di \\(n\\) prove Bernoulliane darà come esito una sequenza di \\(n\\) elementi \\(S\\) e \\(I\\). Ad esempio, una sequenza che contiene \\(y\\) successi è la seguente:\n\\[\n\\overbrace{SS\\dots S}^\\text{$y$ volte} \\overbrace{II\\dots I}^\\text{$n-y$ volte}\n\\]\nEssendo \\(\\theta\\) la probabilità di \\(S\\) e \\(1-\\theta\\) la probabilità di \\(I\\), la probabilità di ottenere la specifica sequenza riportata sopra è\n\\[\n\\begin{equation}\n\\overbrace{\\theta \\theta\\dots \\theta}^\\text{$y$ volte} \\overbrace{(1-\\theta)(1-\\theta)\\dots (1-\\theta)}^\\text{$n-y$ volte} = \\theta^y \\cdot (1-\\theta)^{n-y}.\n\\end{equation}\n\\tag{1.3}\\]\nNon siamo però interessati alla probabilità di una specifica sequenza di \\(S\\) e \\(I\\) ma, bensì, alla probabilità di osservare una qualsiasi sequenza di \\(y\\) successi in \\(n\\) prove. In altre parole, vogliamo la probabilità dell’unione di tutti gli eventi corrispondenti a \\(y\\) successi in \\(n\\) prove.\nÈ immediato notare che una qualsiasi altra sequenza contenente esattamente \\(y\\) successi avrà sempre come probabilità \\(\\theta^y \\cdot (1-\\theta)^{n-y}\\): il prodotto infatti resta costante anche se cambia l’ordine dei fattori.1 Per trovare il risultato cercato dobbiamo moltiplicare l’Equazione 1.3 per il numero di sequenze possibili di \\(y\\) successi in \\(n\\) prove.\nIl numero di sequenze che contengono esattamente \\(y\\) successi in \\(n\\) prove. La risposta è fornita dal coefficiente binomiale:\n\\[\n\\begin{equation}\n\\binom{n}{y} = \\frac{n!}{y!(n-y)!},\n\\end{equation}\n\\tag{1.4}\\]\ndove il simbolo \\(n!\\) si legge \\(n\\) fattoriale ed è uguale al prodotto di \\(n\\) numeri interi decrescenti a partire da \\(n\\). Per definizione \\(0! = 1\\).\nEssendo la probabilità dell’unione di \\(K\\) elementi incompatibili uguale alla somma delle loro rispettive probabilità, e dato che le sequenze di \\(y\\) successi in \\(n\\) prove hanno tutte la stessa probabilità, per trovare la formula della distributione binomiale Equazione 1.2 è sufficiente moltiplicare l’Equazione 1.3 per l’Equazione 1.4.\n\nLa distribuzione di probabilità di alcune distribuzioni binomiali, per due valori di \\(n\\) e \\(\\theta\\), è fornita nella Figura 1.1.\n\n\n\n\nFigura 1.1: Alcune distribuzioni binomiali. Nella figura, il parametro \\(\\theta\\) è indicato con \\(p\\).\n\n\n\n\n\nEsercizio 1.2 \nUsando l’Equazione 1.2, si trovi la probabilità di \\(y = 2\\) successi in \\(n = 4\\) prove Bernoulliane indipendenti con \\(\\theta = 0.2\\)\n\n\nSoluzione. Abbiamo\n\\[\n\\begin{aligned}\nP(Y=2) &= \\frac{4!}{2!(4-2)!} 0.2^{2} (1-0.2)^{4-2} \\notag  \\\\\n&= \\frac{4 \\cdot 3 \\cdot 2 \\cdot 1}{(2 \\cdot 1)(2 \\cdot 1)}\n0.2^{2} 0.8^{2} = 0.1536. \\notag\n\\end{aligned}\n\\]\nRipetendo i calcoli per i valori \\(y = 0, \\dots, 4\\) troviamo la distribuzione binomiale di parametri \\(n = 4\\) e \\(\\theta = 0.2\\):\n\n\ny\nP(Y = y)\n\n\n\n0\n0.4096\n\n\n1\n0.4096\n\n\n2\n0.1536\n\n\n3\n0.0256\n\n\n4\n0.0016\n\n\nsum\n1.0\n\n\n\nLo stesso risultato si ottiene usando la sequente istruzione R:\n\ndbinom(0:4, 4, 0.2)\n#> [1] 0.4096 0.4096 0.1536 0.0256 0.0016\n\n\n\nEsercizio 1.3 \nLanciando \\(5\\) volte una moneta onesta, qual è la probabilità che esca testa almeno tre volte?\n\n\nSoluzione. In R, la soluzione si trova con\n\ndbinom(3, 5, 0.5) + dbinom(4, 5, 0.5) + dbinom(5, 5, 0.5)\n#> [1] 0.5\n\nAlternativamente, possiamo trovare la probabilità dell’evento complementare a quello definito dalla funzione di ripartizione calcolata mediante pbinom(), ovvero\n\n1 - pbinom(2, 5, 0.5)\n#> [1] 0.5\n\n\n\n1.2.1 Valore atteso e deviazione standard\nLa media (numero atteso di successi in \\(n\\) prove) e la deviazione standard di una distribuzione binomiale si trovano nel modo seguente:\n\\[\n\\begin{align}\n\\mu    &= n\\theta,  \\notag \\\\\n\\sigma &= \\sqrt{n\\theta(1-\\theta)}.\n\\end{align}\n\\tag{1.5}\\]\n\nDimostrazione. Essendo \\(Y\\) la somma di \\(n\\) prove Bernoulliane indipendenti \\(Y_i\\), è facile vedere che\n\\[\\begin{align}\n\\mathbb{E}(Y) &= \\mathbb{E}\\left( \\sum_{i=1}^n Y_i \\right) = \\sum_{i=1}^n \\mathbb{E}(Y_i) = n\\theta, \\\\\n\\mathbb{V}(Y) &= \\mathbb{V} \\left( \\sum_{i=1}^n Y_i \\right) = \\sum_{i=1}^n \\mathbb{V}(Y_i) = n \\theta (1-\\theta).\n\\end{align}\\]\n\n\nEsercizio 1.4 \nSi trovino il valore atteso e la varianza del lancio di quattro monete con probabilità di successo pari a \\(\\theta = 0.2\\).\n\n\nSoluzione. Il valore atteso è \\(\\mu = n \\theta = 4 \\cdot 0.2 = 0.8.\\) Ciò significa che, se l’esperimento casuale venisse ripetuto infinite volte, l’esito testa verrebbe osservato un numero medio di volte pari a 0.8. La varianza è \\(n \\theta (1-\\theta) = 4 \\cdot 0.2 \\cdot (1 - 0.2) = 0.64\\)."
  },
  {
    "objectID": "022_discr_rv_distr.html#distribuzione-di-poisson",
    "href": "022_discr_rv_distr.html#distribuzione-di-poisson",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "\n1.3 Distribuzione di Poisson",
    "text": "1.3 Distribuzione di Poisson\nLa distribuzione di Poisson è una distribuzione di probabilità discreta che esprime le probabilità per il numero di eventi che si verificano successivamente ed indipendentemente in un dato intervallo di tempo, sapendo che mediamente se ne verifica un numero \\(\\lambda\\). La distribuzione di Poisson serve dunque per contare il numero di volte in cui un evento ha luogo in un determinato intervallo di tempo. La stessa distribuzione può essere estesa anche per contare gli eventi che hanno luogo in una determinata porzione di spazio.\n\nLa distribuzione di Poisson può essere intesa come limite della distribuzione binomiale, dove la probabilità di successo \\(\\theta\\) è pari a \\(\\frac{\\lambda}{n}\\) con \\(n\\) che tende a \\(\\infty\\):\n\\[\\begin{equation}\n\\lim_{y \\rightarrow \\infty} \\binom{n}{y} \\theta^y (1-\\theta)^{n-y} = \\frac{\\lambda^y}{y!}e^{-\\lambda}.\n\\end{equation}\\]\n\nAlcune distribuzioni di Poisson sono riportate nella figura @ref(fig:examples-poisson-distrib).\n\n\n\n\nAlcune distribuzioni di Poisson.\n\n\n\n\n\nSupponiamo che un evento accada 300 volte all’ora e si vuole determinare la probabilità che in un minuto accadano esattamente 3 eventi.\nIl numero medio di eventi in un minuto è pari a\n\nlambda <- 300 / 60\nlambda\n#> [1] 5\n\nQuindi la probabilità che in un minuto si abbiano 3 eventi è pari a\n\ny <- 3\n(lambda^y / factorial(y)) * exp(-lambda)\n#> [1] 0.1403739\n\n\n\nPer i dati dell’esempio precedente, si trovi la probabilità che un evento accada almeno 8 volte in un minuto.\nLa probabilità cercata è\n\\[\np(y \\geq 8) = 1 - p (y \\leq 7) = 1- \\sum_{i = 0}^7 \\frac{\\lambda^7}{7!}e^{-\\lambda},\n\\] con \\(\\lambda = 5\\).\nSvolgendo i calcoli in otteniamo:\n\n1 - ppois(q = 7, lambda = 5)\n#> [1] 0.1333717\nppois(q = 7, lambda = 5, lower.tail = FALSE)\n#> [1] 0.1333717\n\n\n\nSapendo che un evento avviene in media 6 volte al minuto, si calcoli (a) la probabilità di osservare un numero di eventi uguale o inferiore a 3 in un minuto, e (b) la probabilità di osservare esattamente 2 eventi in 30 secondi.\n\nIn questo caso \\(\\lambda = 6\\) e la probabilità richiesta è\n\n\nppois(q = 3, lambda = 6, lower.tail = TRUE)\n#> [1] 0.1512039\n\n\nIn questo caso \\(\\lambda = 6 / 2\\) e la probabilità richiesta è\n\n\ndpois(x = 2, lambda = 3)\n#> [1] 0.2240418\n\n\n\n1.3.1 Alcune proprietà della variabile di Poisson\n\nIl valore atteso, la moda e la varianza della variabile di Poisson sono uguali a \\(\\lambda\\).\nLa somma \\(Y_1 + \\dots + Y_n\\) di \\(n\\) variabili casuali indipendenti con distribuzioni di Poisson di parametri \\(\\lambda_{1},\\dots,\\lambda_{n}\\) segue una distribuzione di Poisson di parametro \\(\\lambda = \\lambda_{1}+\\dots+\\lambda_{n}\\).\nLa differenze di due variabili di Poisson non è una variabile di Poisson. Basti infatti pensare che può assumere valori negativi."
  },
  {
    "objectID": "022_discr_rv_distr.html#distribuzione-discreta-uniforme",
    "href": "022_discr_rv_distr.html#distribuzione-discreta-uniforme",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "\n1.4 Distribuzione discreta uniforme",
    "text": "1.4 Distribuzione discreta uniforme\nUna distribuzione discreta uniforme è una distribuzione di probabilità discreta che è uniforme su un insieme, ovvero che attribuisce ad ogni elemento dell’insieme discreto e finito \\(S\\) su cui è definita la stessa probabilità \\(p\\) di verificarsi.\nConsideriamo la variabile casuale \\(X\\) con supporto \\(1, 2, \\dots, m\\). Un esperimento casuale in cui si verifica questa distribuzione è la scelta casuale di un intero compreso tra 1 e \\(m\\) inclusi. Sia \\(X\\) il numero scelto. Allora\n\\[\nP(X = x) = \\frac{1}{m}, \\quad x = 1, \\dots, m.\n\\]\nIl valore atteso è\n\\[\n\\mathbb{E}(X) = \\sum_{x=1}^m x f_X(x) = \\sum_{x=1}^m x \\frac{1}{m} = \\frac{1}{m} (1 + 2 + \\dots + m) = \\frac{m+1}{2},\n\\] dove abbiamo utilizzato l’identità \\(1+2+···+m = m(m+1)/2\\).\nPer trovare la varianza, prima calcoliamo\n\\[\n\\mathbb{E}(X^2) = \\frac{1}{m} \\sum_{x=1}^m x^2,\n\\] e poi troviamo\n\\[\n\\mathbb{V}(X) = \\mathbb{E}(X^2) - \\left[\\mathbb{E}(X)\\right]^2.\n\\]\n\n1.4.1 Usiamo \\(\\textsf{R}\\)\n\nLa sintassi generale per simulare una variabile casuale uniforme discreta è sample(x, size, replace = TRUE). L’argomento x identifica i numeri da cui campionare casualmente. Se x è un numero, il campionamento viene eseguito da 1 a x. L’argomento size indica quanto dovrebbe essere grande la dimensione del campione e replace indica se i numeri devono essere reintrodotti o meno nell’urna dopo essere stati estratti. L’opzione di default è replace = FALSE ma per le uniformi discrete i valori estratti devono essere sostituiti. Seguono alcuni esempi.\n\nPer lanciare un dado equilibrato 3000 volte: sample(6, size = 3000, replace = TRUE);\nper scegliere 27 numeri casuali da 30 a 70: sample(30:70, size = 27, replace = TRUE);\nper lanciare una moneta equa 1000 volte: sample(c(\"H\",\"T\"), size = 1000, replace = TRUE)."
  },
  {
    "objectID": "022_discr_rv_distr.html#distribuzione-beta-binomiale",
    "href": "022_discr_rv_distr.html#distribuzione-beta-binomiale",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "\n1.5 Distribuzione beta-binomiale",
    "text": "1.5 Distribuzione beta-binomiale\nLa distribuzione beta-binomiale di parametri \\(N\\), \\(\\alpha\\) e \\(\\beta\\) è una distribuzione discreta con una funzione di massa di probabilità uguale a\n\\[\\begin{equation}\n\\mbox{BetaBinomial}(y \\mid N, \\alpha, \\beta) = \\binom{N}{y} \\frac{B(y + \\alpha, N-y+\\beta)}{B(\\alpha, \\beta)},\n\\end{equation}\\]\ndove la funzione beta è \\(B(u, v) = \\frac{\\Gamma(u)\\Gamma(v)}{\\Gamma(u+v)}\\).\nSenza entrare nei dettagli, ci accontentiamo di sapere che tale distribuzione è implementata nella funzione dbbinom() del pacchetto extraDistr."
  },
  {
    "objectID": "022_discr_rv_distr.html#commenti-e-considerazioni-finali",
    "href": "022_discr_rv_distr.html#commenti-e-considerazioni-finali",
    "title": "1  Distribuzioni di v.c. discrete",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nLa distribuzione binomiale è una distribuzione di probabilità discreta che descrive il numero di successi in un processo di Bernoulli, ovvero la variabile aleatoria \\(Y = Y_1 + \\dots + Y_n\\) che somma \\(n\\) variabili casuali indipendenti di uguale distribuzione di Bernoulli \\(\\mathcal{B}(\\theta)\\), ognuna delle quali può fornire due soli risultati: il successo con probabilità \\(\\theta\\) e il fallimento con probabilità \\(1 - \\theta\\).\nLa distribuzione binomiale è molto importante per le sue molte applicazioni. Nelle presenti dispense, dedicate all’analisi bayesiana, è soprattutto importante perché costituisce il fondamento del caso più semplice del cosiddetto “aggiornamento bayesiano”, ovvero il caso Beta-Binomiale. Il modello Beta-Binomiale ci fornirà infatti un esempio paradigmatico dell’approccio bayesiano all’inferenza e sarà trattato in maniera analitica. È dunque importante che le proprietà della distribuzione binomiale risultino ben chiare."
  }
]