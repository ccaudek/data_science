[
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Prefazione",
    "section": "",
    "text": "Questo libro ti insegnerà i principi base della Data Science, con esempi pratici."
  },
  {
    "objectID": "preface.html#lanalisi-dei-dati-psicologici",
    "href": "preface.html#lanalisi-dei-dati-psicologici",
    "title": "Prefazione",
    "section": "L’analisi dei dati psicologici",
    "text": "L’analisi dei dati psicologici\nSembra sensato spendere due parole su una domanda che è importante per gli studenti: perché dobbiamo perdere tanto tempo a studiare l’analisi dei dati psicologici quando in realtà quello che ci interessa è tutt’altro? Questa è una bella domanda. C’è una ragione molto semplice che dovrebbe farci capire perché la Data Science sia così importante per la psicologia. Infatti, a ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, in certi casi, predire. In questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili a quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che i problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data Science in psicologia: perché la Data Science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.\nSono sicuro che, leggendo queste righe, a molti studenti sarà venuta in mente la seguente domanda: perché non chiediamo a qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data Science? La risposta a questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data Science. Le tematiche della Data Science non possono essere ignorate né dai ricercatori in psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche i professionisti al di fuori dall’università non possono fare a meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data Science! Basta aprire a caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano i risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.\nLe considerazioni precedenti cercano di chiarire il seguente punto: la Data Science non è qualcosa da studiare a malincuore, in un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data Science in tantissimi ambiti della loro attività professionale: in particolare quando costruiscono, somministrano e interpretano i test psicometrici. È dunque chiaro che possedere delle solide basi di Data Science è un tassello imprescindibile del bagaglio professionale dello psicologo. In questo insegnamento verrano trattati i temi base della Data Science e verrà adottato un punto di vista bayesiano, che corrisponde all’approccio più recente e sempre più diffuso in psicologia."
  },
  {
    "objectID": "preface.html#come-studiare",
    "href": "preface.html#come-studiare",
    "title": "Prefazione",
    "section": "Come studiare",
    "text": "Come studiare\nIl giusto metodo di studio per prepararsi all’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare i concetti via via che essi vengono presentati e verificare in autonomia le procedure presentate a lezione. Incoraggio gli studenti a farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti a utilizzare i forum attivi su Moodle e, soprattutto, a svolgere gli esercizi proposti su Moodle. I problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino a quel punto sono sufficienti rispetto alle richieste dell’esame.\nLa prima fase dello studio, che è sicuramente individuale, è quella in cui lo studente deve acquisire le conoscenze teoriche relative ai problemi che saranno presentati all’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta lo studente ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (\\(\\textsf{R}\\)) per applicare i concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso aiuta a capire meglio."
  },
  {
    "objectID": "007_freq_distr.html",
    "href": "007_freq_distr.html",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "",
    "text": "Le analisi esplorative dei dati e la statistica descrittiva costituiscono la prima fase dell’analisi dei dati psicologici. Consentono di capire come i dati sono distribuiti, ci aiutano ad individuare le osservazioni anomale e gli errori di tabulazione. Consentono di riassumere le distribuzioni dei dati mediante indici sintetici. Consentono di visualizzare e di studiare le relazioni tra le variabili. In questo Capitolo, dopo avere presentato gli obiettivi dell’analisi esplorative dei dati, discuteremo il problema della descrizione numerica e della rappresentazione grafica delle distribuzioni di frequenza."
  },
  {
    "objectID": "007_freq_distr.html#chapter-descript",
    "href": "007_freq_distr.html#chapter-descript",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.1 Introduzione all’esplorazione dei dati",
    "text": "3.1 Introduzione all’esplorazione dei dati\nLe analisi esplorative dei dati sono indispensabili per condurre in modo corretto una qualsiasi analisi statistica, dal livello base a quello avanzato. Si parla di analisi descrittiva se l’obiettivo è quello di descrivere le caratteristiche di un campione. Si parla di analisi esplorativa dei dati (Exploratory Data Analysis o EDA) se l’obiettivo è quello di esplorare i dati alla ricerca di nuove informazioni e relazioni tra variabili. Questa distinzione, seppur importante a livello teorico, nella pratica è più fumosa perché spesso entrambe le situazioni si verificano contemporaneamente nella stessa indagine statistica e le metodologie di analisi che si utilizzano sono molto simili.\nNé il calcolo delle statistiche descrittive né l’analisi esplorativa dei dati possono essere condotte senza utilizzare un software. Le descrizioni dei concetti di base della EDA saranno dunque fornite di pari passo alla spiegazione di come le quantità discusse possono essere calcolate in pratica utilizzando \\(\\mathsf{R}\\)."
  },
  {
    "objectID": "007_freq_distr.html#un-excursus-storico",
    "href": "007_freq_distr.html#un-excursus-storico",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.2 Un excursus storico",
    "text": "3.2 Un excursus storico\nNel 1907 Francis Galton, cugino di Charles Darwin, matematico e statistico autodidatta, geografo, esploratore, teorico della dattiloscopia (ovvero, dell’uso delle impronte digitali a fini identificativi) e dell’eugenetica, scrisse una lettera alla rivista scientifica Nature sulla sua visita alla Fat Stock and Poultry Exhibition di Plymouth. Lì vide alcuni membri del pubblico partecipare ad un gioco il cui scopo era quello di indovinare il peso della carcassa di un grande bue che era appena stato scuoiato. Galton si procurò i 787 dei biglietti che erano stati compilati dal pubblico e considerò il valore medio di 547 kg come la “scelta democratica” dei partecipanti, in quanto “ogni altra stima era stata giudicata troppo alta o troppo bassa dalla maggioranza dei votanti”. Il punto interessante è che il peso corretto di 543 kg si dimostrò essere molto simile alla “scelta democratica” basata sulle stime dei 787 partecipanti. Galton intitolò la sua lettera a Nature Vox Populi (voce del popolo), ma questo processo decisionale è ora meglio conosciuto come la “saggezza delle folle” (wisdom of crowds). Possiamo dire che, nel suo articolo del 1907, Galton effettuò quello che ora chiamiamo un riepilogo dei dati, ovvero calcolò un indice sintetico a partire da un insieme di dati. In questo capitolo esamineremo le tecniche che sono state sviluppate nel secolo successivo per riassumere le grandi masse di dati con cui sempre più spesso ci dobbiamo confrontare. Vedremo come calcolare e interpretare gli indici di posizione e di dispersione, discuteremo le distribuzioni di frequenze e le relazioni tra variabili. Vedremo inoltre quali sono le tecniche di visualizzazione che ci consentono di rappresentare questi sommari dei dati mediante dei grafici. Ma prima di entrare nei dettagli, prendiamoci un momento per capire perché abbiamo bisogno della statistica e, per ciò che stiamo discutendo qui, della statistica descrittiva.\nIn generale, che cos’è la statistica? Ci sono molte definizioni. Fondamentalmente, la statistica è un insieme di tecniche che ci consentono di dare un senso al mondo attraverso i dati. Ciò avviene tramite il processo di analisi statistica. L’analisi statistica traduce le domande che abbiamo a proposito del mondo in modelli matematici, utilizza i dati per scegliere i modelli matematici che sono apppropriati per descrivere il mondo e, infine, applica tali modelli per trovare una risposta alle domande che ci siamo posti. La statistica consente quindi di collegare le nostre domande a proposito del mondo ai dati, di utilizzare i dati per trovare le risposte alle domande che ci siamo posti e di valutare l’impatto delle risposte che abbiamo trovato."
  },
  {
    "objectID": "007_freq_distr.html#riassumere-i-dati",
    "href": "007_freq_distr.html#riassumere-i-dati",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.3 Riassumere i dati",
    "text": "3.3 Riassumere i dati\nIniziamo a porci una domanda. Quando riassumiamo i dati, necessariamente buttiamo via delle informazioni; ma è una buona idea procedere in questo modo? Non sarebbe meglio conservare le informazioni specifiche di ciascun soggetto che partecipa ad un esperimento psicologico, al di là di ciò che viene trasmesso dagli indici riassuntivi della statistica descrittiva? Che dire delle informazioni che descrivono come sono stati raccolti i dati, come l’ora del giorno o l’umore del partecipante? Tutte queste informazioni vengono perdute quando riassumiamo i dati. La risposta alla domanda che ci siamo posti è che, in generale, non è una buona idea conservare tutti i dettagli di ciò che sappiamo. È molto più utile riassumere le informazioni perché la semplificazione risultante consente i processi di generalizzazione.\nIn un contesto letterario, l’importanza della generalizzazione è stata sottolineata da Jorge Luis Borges nel suo racconto “Funes o della memoria”, che descrive un individuo che perde la capacità di dimenticare. Borges si concentra sulla relazione tra generalizzazione e pensiero:\n\nPensare è dimenticare una differenza, generalizzare, astrarre. Nel mondo troppo pieno di Funes, c’erano solo dettagli.\n\nCome possiamo ben capire, la vita di Funes non è facile. Se facciamo riferimento alla psicologia possiamo dire che gli psicologi hanno studiato a lungo l’utilità della generalizzazione per il pensiero. Un esempio è fornito dal fenomeno della formazione dei concetti e lo psicologo che viene in mente a questo proposito è sicuramente Eleanor Rosch, la quale ha studiato i principi di base della categorizzazione. I concetti ci forniscono uno strumento potente per organizzare le conoscenze. Noi siamo in grado di riconoscere facilmente i diversi esemplare di un concetto – per esempio, “gli uccelli” – anche se i singoli esemplari che fanno parte di una categoria sono molto diversi tra loro (l’aquila, il gabbiano, il pettirosso). L’uso dei concetti, cioè la generalizzazione, è utile perché ci consente di fare previsioni sulle proprietà dei singoli esemplari che appartengono ad una categoria, anche se non abbiamo mai avuto esperienza diretta con essi – per esempio, possiamo fare la predizione che tutti gli uccelli possono volare e mangiare vermi, ma non possono guidare un’automobile o parlare in inglese. Queste previsioni non sono sempre corrette, ma sono utili.\nLe statistiche descrittive, in un certo senso, ci fornisco l’analogo dei “prototipi” che, secondo Eleanor Rosch, stanno alla base del processo psicologico di creazione dei concetti. Un prototipo è l’esemplare più rappresentativo di una categoria. In maniera simile, una statistica descrittiva come la media, ad esempio, potrebbe essere intesa come l’osservazione “tipica”.\nLa statistica descrittiva ci fornisce gli strumenti per riassumere i dati che abbiamo a disposizione in una forma visiva o numerica. Le rappresentazioni grafiche più usate della statistica descrittiva sono gli istogrammi, i diagrammi a dispersione o i box-plot, e gli indici sintetici più comuni sono la media, la mediana, la varianza e la deviazione standard."
  },
  {
    "objectID": "007_freq_distr.html#i-dati-grezzi",
    "href": "007_freq_distr.html#i-dati-grezzi",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.4 I dati grezzi",
    "text": "3.4 I dati grezzi\nPer introdurre i principali strumenti della statistica descrittiva considereremo qui i dati raccolti da Zetsche et al. (2019). Questi ricercatori hanno studiato le aspettative negative quale meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello studio, Zetsche et al. (2019) si sono chiesti se individui depressi maturino delle aspettative accurate sul loro umore futuro, oppure se tali aspettative sono distorte negativamente.1. In uno studio viene esaminato un campione costituito da 30 soggetti con almeno un episodio depressivo maggiore e da 37 controlli sani. Gli autori hanno misurato il livello depressivo con il Beck Depression Inventory (BDI-II). Questi sono i dati che considereremo qui.\n\nEsercizio 3.1 \nQual è la la gravità della depressione riportata dai soggetti nel campione esaminato da Zetsche et al. (2019)?\n\n\nSoluzione. Per rispondere alla domanda del problema, iniziamo a leggere in \\(\\mathsf{R}\\) i dati, assumendo che il file data.mood.csv si trovi nella cartella data contenuta nella working directory.\n\nCodicelibrary(\"rio\")\ndf <- rio::import(\n  here(\"data\", \"data.mood.csv\"),\n  header = TRUE\n)\n\n\nC’è un solo valore BDI-II per ciascun soggetto ma tale valore viene ripetuto tante volte quante volte sono le righe del data.frame associate ad ogni soggetto (ciascuna riga corrispondente ad una prova diversa). È dunque necessario trasformare il data.frame in modo tale da avere un’unica riga per ciascun soggetto, ovvero un unico valore BDI-II per soggetto.\n\nCodicebysubj <- df %>%\n  group_by(esm_id) %>%\n  summarise(\n    bdi = mean(bdi)\n  ) %>%\n  na.omit()\n\n\nCi sono dunque 66 soggetti i quali hanno ottenuto i valori sulla scala del BDI-II stampati di seguito. Per semplicità, li presentiamo ordinati dal più piccolo al più grande.\n\nCodicesort(bysubj$bdi)\n#>  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1\n#> [26]  2  2  2  2  3  3  3  5  7  9 12 19 22 22 24 25 25 26 26 26 27 27 28 28 30\n#> [51] 30 30 31 31 33 33 34 35 35 35 36 39 41 43 43 44"
  },
  {
    "objectID": "007_freq_distr.html#distribuzioni-di-frequenze",
    "href": "007_freq_distr.html#distribuzioni-di-frequenze",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.5 Distribuzioni di frequenze",
    "text": "3.5 Distribuzioni di frequenze\nÈ chiaro che i dati grezzi sono di difficile lettura. Poniamoci dunque il problema di creare una rappresentazione sintetica e comprensibile di questo insieme di valori. Uno dei modi che ci consentono di effettuare una sintesi dei dati è quello di generare una distribuzione di frequenze.\n\nDefinizione 3.1 \nUna distribuzione di frequenze è un riepilogo del conteggio della frequenza con cui le modalità osservate in un insieme di dati si verificano in un intervallo di valori.\n\nIn altre parole, la distribuzione di frequenze della variabile \\(X\\) corrisponde all’insieme delle frequenze assegnate a ciascun possibile valore di \\(X\\).\nPer creare una distribuzione di frequenze possiamo procedere effettuando una partizione delle modalità della variabile di interesse in \\(m\\) classi (denotate con \\(\\Delta_i\\)) tra loro disgiunte. In tale partizione, la classe \\(i\\)-esima coincide con un intervallo di valori aperto a destra \\([a_i, b_i)\\) o aperto a sinistra \\((a_i, b_i]\\). Ad ogni classe \\(\\Delta_i\\) avente \\(a_i\\) e \\(b_i\\) come limite inferiore e superiore associamo l’ampiezza \\(b_i - a_i\\) (non necessariamente uguale per ogni classe) e il valore centrale \\(\\bar{x}_i\\). La scelta delle classi è arbitraria, ma è buona norma non definire classi con un numero troppo piccolo (\\(< 5\\)) di osservazioni. Poiché ogni elemento dell’insieme \\(\\{x_i\\}_{i=1}^n\\) appartiene ad una ed una sola classe \\(\\Delta_i\\), possiamo calcolare le quantità elencate di seguito.\n\n\nLa frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).\n\nProprietà: \\(n_1 + n_2 + \\dots + n_m = n\\).\n\n\n\nLa frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe.\n\nProprietà: \\(f_1+f_2+\\dots+f_m =1\\).\n\n\nLa frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(i\\)-esima compresa: \\(N_i = \\sum_{i=1}^m n_i.\\)\nLa frequenza cumulata relativa \\(F_i\\), ovvero \\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{i=1}^m f_i.\\)\n\n\nEsercizio 3.2 \nSi calcoli la distribuzione di frequenza assoluta e la distribuzione di frequenza relativa per i valori del BDI-II del campione clinico di Zetsche et al. (2019).\n\n\nSoluzione. Per costruire una distribuzione di frequenza è innanzitutto necessario scegliere gli intervalli delle classi. Facendo riferimento ai cut-off usati per l’interpretazione del BDI-II, definiamo i seguenti intervalli aperti a destra:\n\ndepressione minima: [0, 13.5),\ndepressione lieve: [13.5, 19.5),\ndepressione moderata: [19.5, 28.5),\ndepressione severa: [28.5, 63).\n\nEsaminando i dati, possiamo notare che 36 soggetti cadono nella prima classe, uno nella seconda classe, e così via. La distribuzione di frequenza della variabile bdi2 è riportata nella tabella seguente. Questa distribuzione di frequenza ci aiuta a capire meglio cosa sta succedendo. Se consideriamo la frequenza relativa, ad esempio, possiamo notare che ci sono due valori maggiormente ricorrenti e tali valori corrispondono alle due classi più estreme. Questo ha senso nel caso presente, in quanto il campione esaminato da Zetsche et al. (2019) includeva due gruppi di soggetti: soggetti sani (con valori BDI-II bassi) e soggetti depressi (con valori BDI-II alti).2 In una distribuzione di frequenza tali valori tipici vanno sotto il nome di mode della distribuzione.\n\n\n\nLim. classi\nFr. ass.\nFr. rel.\nFr. ass. cum.\nFr. rel. cum.\n\n\n\n\\([0, 13.5)\\)\n36\n36/66\n36\n36/66\n\n\n\\([13.5, 19.5)\\)\n1\n1/66\n37\n37/66\n\n\n\\([19.5, 28.5)\\)\n12\n12/66\n49\n49/66\n\n\n\\([28.5, 63)\\)\n17\n17/66\n66\n66/66\n\n\n\n\nPoniamoci ora il problema di costruire la tabella precedente utilizzando \\(\\mathsf{R}\\). Usando la funzione cut(), dividiamo il campo di variazione (ovvero, la differenza tra il valore massimo di una distribuzione ed il valore minimo) di una variabile continua x in intervalli e codifica ciascun valore x nei termini dell’intervallo a cui appartiene. Così facendo otteniamo:\n\nCodicebysubj$bdi_level <- cut(\n  bysubj$bdi,\n  breaks = c(0, 13.5, 19.5, 28.5, 63),\n  include.lowest = TRUE,\n  labels = c(\n    \"minimal\", \"mild\", \"moderate\", \"severe\"\n  )\n)\n\nbysubj$bdi_level\n#>  [1] moderate severe   severe   moderate severe   severe   severe   severe  \n#>  [9] moderate severe   moderate mild     severe   minimal  minimal  minimal \n#> [17] severe   moderate minimal  minimal  minimal  minimal  minimal  moderate\n#> [25] minimal  minimal  minimal  minimal  minimal  minimal  minimal  severe  \n#> [33] minimal  minimal  severe   minimal  moderate minimal  minimal  minimal \n#> [41] severe   minimal  minimal  severe   severe   moderate severe   severe  \n#> [49] minimal  moderate minimal  moderate severe   moderate moderate minimal \n#> [57] minimal  minimal  minimal  minimal  minimal  minimal  minimal  minimal \n#> [65] minimal  minimal \n#> Levels: minimal mild moderate severe\n\n\nPossiamo ora usare la funzione table() la quale ritorna un elenco che associa la frequenza assoluta a ciascuna modalità della variabile – ovvero, ritorna la distribuzione di frequenza assoluta.\n\nCodicetable(bysubj$bdi_level)\n#> \n#>  minimal     mild moderate   severe \n#>       36        1       12       17\n\n\nLa distribuzione di frequenza relativa si ottiene dividendo ciascuna frequenza assoluta per il numero totale di osservazioni:\n\nCodicetable(bysubj$bdi_level) / sum(table(bysubj$bdi_level))\n#> \n#>    minimal       mild   moderate     severe \n#> 0.54545455 0.01515152 0.18181818 0.25757576\n\n\n\n\nLimiti delle classi\nFrequenza assoluta\nFrequenza relativa\n\n\n\n[0, 13.5)\n36\n36/66\n\n\n[13.5, 19.5)\n1\n1/66\n\n\n[19.5, 28.5)\n12\n12/66\n\n\n[28.5, 63]\n17\n17/66\n\n\n\n\nInsiemi di variabili possono anche avere distribuzioni di frequenze, dette distribuzioni congiunte. La distribuzione congiunta di un insieme di variabili \\(V\\) è l’insieme delle frequenze di ogni possibile combinazione di valori delle variabili in \\(V\\). Ad esempio, se \\(V\\) è un insieme di due variabili, \\(X\\) e \\(Y\\), ciascuna delle quali può assumere due valori, 1 e 2, allora una possibile distribuzione congiunta di frequenze relative per \\(V\\) è \\(f(X = 1, Y = 1) = 0.2\\), \\(f(X = 1, Y = 2) = 0.1\\), \\(f(X = 2, Y = 1) = 0.5\\), \\(f(X = 2, Y = 2) = 0.2\\). Proprio come con le distribuzioni di frequenze relative di una singola variabile, le frequenze relative di una distribuzione congiunta devono sommare a 1."
  },
  {
    "objectID": "007_freq_distr.html#istogramma",
    "href": "007_freq_distr.html#istogramma",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.6 Istogramma",
    "text": "3.6 Istogramma\nI dati che sono stati sintetizzati in una distribuzione di frequenze possono essere rappresentati graficamente in un istogramma. Un istogramma si costruisce riportando sulle ascisse i limiti delle classi \\(\\Delta_i\\) e sulle ordinate i valori della funzione costante a tratti\n\\[\n\\varphi_n(x)= \\frac{f_i}{b_i-a_i}, \\quad x\\in \\Delta_i,\\, i=1, \\dots, m\n\\]\nche misura la densità della frequenza relativa della variabile \\(X\\) nella classe \\(\\Delta_i\\), ovvero il rapporto fra la frequenza relativa \\(f_i\\) e l’ampiezza (\\(b_i - a_i\\)) della classe. In questo modo il rettangolo dell’istogramma associato alla classe \\(\\Delta_i\\) avrà un’area proporzionale alla frequenza relativa \\(f_i\\). Si noti che l’area totale dell’istogramma delle frequenze relative è data della somma delle aree dei singoli rettangoli e quindi vale 1.0.\n\nEsercizio 3.3 \nSi utilizzi \\(\\mathsf{R}\\) per costruire un istogramma per i valori BDI-II riportati da Zetsche et al. (2019).\n\n\nSoluzione. Con i quattro intervalli individuati dai cut-off del BDI-II otteniamo la rappresentazione riportata nella Figura 3.1. Per chiarezza, precisiamo che ggplot() utilizza intervalli aperti a destra. Nel caso della prima barra dell’istogramma, l’ampiezza dell’intervallo è pari a 13.5 e l’area della barra (ovvero, la frequenza relativa) è uguale a 36/66. Dunque l’altezza della barra è uguale a \\((36 / 66) / 13.5 = 0.040\\). Lo stesso procedimento si applica per il calcolo dell’altezza degli altri rettangoli.\n\nCodicebysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1)\n    # il valore BDI-II massimo è 44\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1)\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )\n\n\n\nFigura 3.1: Istogramma per i valori BDI-II riportati da Zetsche et al. (2019).\n\n\n\n\nAnche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, in generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale. Questo è il caso dell’istogramma della Figura 3.2.\n\nCodicebysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  scale_x_continuous(\n    breaks = c(0.00, 7.35, 14.70, 22.05, 29.40, 36.75, 44.10)\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequanza\"\n  )\n\n\n\nFigura 3.2: Una rappresentazione più comune per l’istogramma dei valori BDI-II nella quale gli intervalli delle classi hanno ampiezze uguali."
  },
  {
    "objectID": "007_freq_distr.html#kernel-density-plot",
    "href": "007_freq_distr.html#kernel-density-plot",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.7 Kernel density plot",
    "text": "3.7 Kernel density plot\nIl confronto tra la Figura 3.1 e la Figura 3.2 rende chiaro il limite dell’istogramma: il profilo dell’istogramma è arbitrario, in quanto dipende dal numero e dall’ampiezza delle classi. Questo rende difficile l’interpretazione.\nIl problema precedente può essere alleviato utilizzando una rappresentazione alternativa della distribuzione di frequenza, ovvero la stima della densità della frequenza dei dati (detta anche stima kernel di densità). Un modo semplice per pensare a tale rappresentazione, che in inglese va sotto il nome di kernel density plot (cioè i grafici basati sulla stima kernel di densità), è quello di immaginare un grande campione di dati, in modo che diventi possibile definire un enorme numero di classi di equivalenza di ampiezza molto piccola, le quali non risultino vuote. In tali circostanze, la funzione di densità empirica non è altro che il profilo lisciato dell’istogramma. La stessa idea si applica anche quando il campione è piccolo. In tali circostanze, invece di raccogliere le osservazioni in barre come negli istogrammi, lo stimatore di densità kernel colloca una piccola “gobba” (bump), determinata da un fattore \\(K\\) (kernel) e da un parametro \\(h\\) di smussamento detto ampiezza di banda (bandwidth), in corrispondenza di ogni osservazione, quindi somma le gobbe risultanti generando una curva smussata.\nL’interpretazione che possiamo attribuire al kernel density plot è simile a quella che viene assegnata agli istogrammi: l’area sottesa al kernel density plot in un certo intervallo rappresenta la proporzione di casi della distribuzione che hanno valori compresi in quell’intervallo.\n\nEsercizio 3.4 All’istogramma dei valori BDI-II di Zetsche et al. (2019) si sovrapponga un kernel density plot.\n\nCodicebysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  geom_density(\n    aes(x = bdi),\n    adjust = 0.5,\n    size = 0.8,\n    #fill = colors[2],\n    alpha = 0.5\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )\n\n\n\nFigura 3.3: Kernel density plot e corrispondente istogramma per i valori BDI-II."
  },
  {
    "objectID": "007_freq_distr.html#forma-di-una-distribuzione",
    "href": "007_freq_distr.html#forma-di-una-distribuzione",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.8 Forma di una distribuzione",
    "text": "3.8 Forma di una distribuzione\nIn generale, la forma di una distribuzione descrive come i dati si distribuiscono intorno ai valori centrali. Distinguiamo tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali o multimodali. Un’illustrazione grafica è fornita nella ?fig-distrib-shapes. Nel pannello 1 la distribuzione è unimodale con asimmetria negativa; nel pannello 2 la distribuzione è unimodale con asimmetria positiva; nel pannello 3 la distribuzione è simmetrica e unimodale; nel pannello 4 la distribuzione è bimodale.\n\n\n\n\nFigura 3.4: 1: Asimmetria negativa. 2: Asimmetria positiva. 3: Distribuzione unimodale. 4: Distribuzione bimodale.\n\n\n\n\n\nEsercizio 3.5 \nIl kernel density plot della Figura 3.3 indica che la distribuzione dei valori del BDI-II nel campione di Zetsche et al. (2019) è bimodale. Ciò indica che le osservazioni della distribuzione si addensano in due cluster ben distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l’altro gruppo tende ad avere BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da Zetsche et al. (2019)."
  },
  {
    "objectID": "007_freq_distr.html#indici-di-posizione",
    "href": "007_freq_distr.html#indici-di-posizione",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "\n3.9 Indici di posizione",
    "text": "3.9 Indici di posizione\n\n3.9.1 Quantili\nLa descrizione della distribuzione dei valori BDI-II di Zetsche et al. (2019) può essere facilitata dalla determinazione di alcuni valori caratteristici che sintetizzano le informazioni contenute nella distribuzione di frequenze. Si dicono quantili (o frattili) quei valori caratteristici che hanno le seguenti proprietà. I quartili sono quei valori che ripartiscono i dati \\(x_i\\) in quattro parti ugualmente numerose (pari ciascuna al 25% del totale). Il primo quartile, \\(q_1\\), lascia alla sua sinistra il 25% del campione pensato come una fila ordinata (a destra quindi il 75%). Il secondo quartile \\(q_2\\) lascia a sinistra il 50% del campione (a destra quindi il 50%). Esso viene anche chiamato mediana. Il terzo quartile lascia a sinistrail 75% del campione (a destra quindi il 25%). Secondo lo stesso criterio, si dicono decili i quantili di ordine \\(p\\) multiplo di 0.10 e percentili i quantili di ordine \\(p\\) multiplo di 0.01.\nCome si calcolano i quantili? Consideriamo la definizione di quantile non interpolato di ordine \\(p\\) \\((0 < p < 1)\\). Si procede innanzitutto ordinando i dati in ordine crescente, \\(\\{x_1, x_2, \\dots, x_n\\}\\). Ci sono poi due possibilità. Se il valore \\(np\\) non è intero, sia \\(k\\) l’intero tale che \\(k < np < k + 1\\) – ovvero, la parte intera di \\(np\\). Allora \\(q_p = x_{k+1}.\\) Se \\(np = k\\) con \\(k\\) intero, allora \\(q_p = \\frac{1}{2}(x_{k} + x_{k+1}).\\) Se vogliamo calcolare il primo quartile \\(q_1\\), ad esempio, utilizziamo \\(p = 0.25\\). Dovendo calcolare gli altri quantili basta sostituire a \\(p\\) il valore appropriato.\nGli indici di posizione, tra le altre cose, hanno un ruolo importante, ovvero vengono utilizzati per creare una rappresentazione grafica di una distribuzione di valori che è molto popolare e può essere usata in alternativa ad un istogramma (in realtà vedremo poi come possa essere combinata con un istogramma). Tale rappresentazione va sotto il nome di box-plot.\n\nEsercizio 3.6 \nPer fare un esempio, consideriamo i nove soggetti del campione clinico di Zetsche et al. (2019) che hanno riportato un unico episodio di depressione maggiore. Per tali soggetti i valori ordinati del BDI-II (per semplicità li chiameremo \\(x\\)) sono i seguenti: 19, 26, 27, 28, 28, 33, 33, 41, 43. Per il calcolo del secondo quartile (non interpolato), ovvero per il calcolo della mediana, dobbiamo considerare la quantità \\(np = 9 \\cdot 0.5 = 4.5\\), non intero. Quindi, \\(q_1 = x_{4 + 1} = 27\\). Per il calcolo del quantile (non interpolato) di ordine \\(p = 2/3\\) dobbiamo considerare la quantità \\(np = 9 \\cdot 2/3 = 6\\), intero. Quindi, \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).\n\n\n3.9.2 Diagramma a scatola\nIl diagramma a scatola (o box plot) è uno strumento grafico utile al fine di ottenere informazioni circa la dispersione e l’eventuale simmetria o asimmetria di una distribuzione. Per costruire un box-plot si rappresenta sul piano cartesiano un rettangolo (cioè la “scatola”) di altezza arbitraria la cui base corrisponde alla dist intanza interquartile (IQR = \\(q_{0.75} - q_{0.25}\\)). La linea interna alla scatola rappresenta la mediana \\(q_{0.5}\\). Si tracciano poi ai lati della scatola due segmenti di retta i cui estremi sono detti “valore adiacente” inferiore e superiore. Il valore adiacente inferiore è il valore più piccolo tra le osservazioni che risulta maggiore o uguale al primo quartile meno la distanza corrispondente a 1.5 volte la distanza interquartile. Il valore adiacente superiore è il valore più grande tra le osservazioni che risulta minore o uguale a \\(Q_3+1.5\\) IQR. I valori esterni ai valori adiacenti (chiamati valori anomali) vengono rappresentati individualmente nel box-plot per meglio evidenziarne la presenza e la posizione.\n\n\n\n\nFigura 3.5: Box-plot: \\(M\\) è la mediana, \\(\\bar{x}\\) è la media aritmetica e IQR è la distanza interquartile (\\(Q_3 - Q_1\\)).\n\n\n\n\n\nEsercizio 3.7 Per i dati di Zetsche et al. (2019), si utilizzi un box-plot per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo.\nNella Figura 3.6 sinistra sono rappresentati i dati grezzi. La linea curva che circonda (simmetricamente) le osservazioni è l’istogramma lisciato (kernel density plot) che abbiamo descritto in precedenza. Nella Figura 3.6 destra sono rappresentanti gli stessi dati: il kernel density plot è lo stesso di prima, ma al suo interno è stato collocato un box-plot. Entrambe le rappresentazioni suggeriscono che la distribuzione dei dati è all’incirca simmetrica nel gruppo clinico. Il gruppo di controllo mostra invece un’asimmetria positiva.\n\nCodicebysubj <- df %>%\n  group_by(esm_id, group) %>%\n  summarise(\n    bdi = mean(bdi),\n    nr_of_episodes = mean(nr_of_episodes, na.rm = TRUE)\n  ) %>%\n  na.omit() %>%\n  ungroup()\n\nbysubj$group <- forcats::fct_recode(\n  bysubj$group,\n  \"Controlli\\n sani\" = \"ctl\",\n  \"Depressione\\n maggiore\" = \"mdd\"\n)\n\np1 <- bysubj %>%\n  ggplot(aes(x = group, y = bdi)) +\n  geom_violin(trim = FALSE) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.7) +\n  labs(\n    x = \"\",\n    y = \"BDI-II\"\n  )\np2 <- bysubj %>%\n  ggplot(aes(x = group, y = bdi)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.05) +\n  labs(\n    x = \"\",\n    y = \"BDI-II\"\n  )\np1 + p2\n\n\n\nFigura 3.6: Due versioni di un violin plot per i valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019).\n\n\n\n\n\n\n3.9.3 Sina plot\nSi noti che i box plot non sono necessariamente la rappresentazione migliore della distribuzione di una variabile. Infatti, richiedono la comprensione di concetti complessi (quali i quantili e la differenza interquantile) che non sono necessari se vogliamo presentare in maniera grafica la distribuzione della variabile e, in generale, non sono compresi da un pubblico di non specialisti. Inoltre, i box plot nascondono informazioni che di solito sono importanti. È dunque preferibile presentare direttamente i dati.\nNella Figura 3.7 viene presentato un cosiddetto “sina plot”. In tale rappresentazione grafica vengono mostrate le singole osservazioni divise in classi. Ai punti viene aggiunto un jitter, così da evitare sovrapposizioni. L’ampiezza del jitter lungo l’asse \\(x\\) è determinata dalla distribuzione della densità dei dati all’interno di ciascuna classe; quindi il grafico mostra lo stesso contorno di un violin plot, ma trasmette informazioni sia sul numero di punti dati, sia sulla distribuzione della densità, sui valori anomali e sulla distribuzione dei dati in un formato molto semplice, comprensibile e sintetico. Per un esempio in una recente pubblicazione, possiamo considerare le figure 3 e 6 di Lazic et al. (2020).\n\nEsercizio 3.8 Si generi un sina plot per i dati della Figura 3.6. Si aggiunga alla figura una rappresentazione della mediana.\n\nCodicezetsche_summary <- bysubj %>%\n  group_by(group) %>%\n  summarize(\n    bdi_mean = mean(bdi),\n    bdi_sd = sd(bdi),\n    bdi_median = median(bdi)\n  ) %>%\n  ungroup()\n\nbysubj %>%\n  ggplot(\n    aes(x = group, y = bdi, color = group)\n  ) +\n  ggforce::geom_sina(aes(color = group, size = 1, alpha = .5)) +\n  geom_errorbar(\n    aes(y = bdi_median, ymin = bdi_median, ymax = bdi_median),\n    data = zetsche_summary, width = 0.3, size = 1\n  ) +\n  labs(\n    x = \"\",\n    y = \"BDI-II\",\n    color = \"Gruppo\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_colour_grey(start = 0.7, end = 0)\n\n\n\nFigura 3.7: Sina plot per i valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019) con l’indicazione della mediana per ciascun gruppo.\n\n\n\n\n\n\n3.9.4 L’eccellenza grafica\nNon c’è un unico modo “corretto” per la rappresentazione grafica dei dati. Ciascuno dei grafici che abbiamo discusso in precedenza ha i suoi pregi e i suoi difetti. Un ricercatore che ha molto influenzato il modo in cui viene realizzata la visualizzazione dei dati scientifici è Edward Tufte, soprannominato dal New York Times il “Leonardo da Vinci dei dati.” Secondo Tufte, “l’eccellenza nella grafica consiste nel comunicare idee complesse in modo chiaro, preciso ed efficiente”. Nella visualizzazione delle informazioni, l’“eccellenza grafica” ha l’obiettivo di comunicare al lettore il maggior numero di idee nella maniera più diretta e semplice possibile. Secondo Tufte (2001), le rappresentazioni grafiche dovrebbero:\n\nmostrare i dati;\nindurre l’osservatore a riflettere sulla sostanza piuttosto che sulla progettazione grafica, o qualcos’altro;\nevitare di distorcere quanto i dati stanno comunicando (“integrità grafica”);\npresentare molte informazioni in forma succinta;\nrivelare la coerenza tra le molte dimensioni dei dati;\nincoraggiare l’osservatore a confrontare differenti sottoinsiemi di dati;\nrivelare i dati a diversi livelli di dettaglio, da una visione ampia alla struttura di base;\nservire ad uno scopo preciso (descrizione, esplorazione, o la risposta a qualche domanda);\nessere fortemente integrate con le descrizioni statistiche e verbali dei dati fornite nel testo.\n\nIn base a questi principi, la Figura 3.7 sembra fornire la rappresentazione migliore dei dati di Zetsche et al. (2019). Il seguente link fornisce alcune illustrazioni dei principi elencati sopra."
  },
  {
    "objectID": "007_freq_distr.html#commenti-e-considerazioni-finali",
    "href": "007_freq_distr.html#commenti-e-considerazioni-finali",
    "title": "3  Variabili e distribuzioni di frequenza",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nUna distribuzione è una rappresentazione del modo in cui le diverse modalità di una variabile si distribuiscono nelle unità statistiche che compongono il campione o la popolazione oggetto di studio. Il modo più diretto per trasmettere descrivere le proprietà della distribuzione di una variabile discreta è quello di fornire una rappresentazione grafica della distribuzione di frequenza. In seguito vedremo la corrispondente rappresentazione che viene usata nel caso delle variabili continue.\n\n\n\n\n\n\nLazic, S. E., Semenova, E., & Williams, D. P. (2020). Determining organ weight toxicity with bayesian causal models: Improving on the analysis of relative organ weights. Scientific Reports, 10(1), 1–12.\n\n\nTufte, E. R. (2001). The visual display of quantitative information. Graphics press Cheshire, CT.\n\n\nZetsche, U., Bürkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: Biased or realistic? Journal of Abnormal Psychology, 128(7), 678–688."
  },
  {
    "objectID": "999_refs.html",
    "href": "999_refs.html",
    "title": "Riferimenti bibliografici",
    "section": "",
    "text": "Albert, J., & Hu, J. (2019). Probability and bayesian\nmodeling. Chapman; Hall/CRC.\n\n\nBechdel, A. (1986). Dykes to watch out for. Firebrand Books.\n\n\nBergh, D. van den, Van Doorn, J., Marsman, M., Draws, T., Van Kesteren,\nE.-J., Derks, K., Dablander, F., Gronau, Q. F., Kucharskỳ, Š., Gupta, A.\nR. K. N., et al. (2020). A tutorial on conducting and interpreting a\nbayesian ANOVA in JASP. L’Année Psychologique,\n120(1), 73–96.\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B.,\nBetancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017).\nStan: A probabilistic programming language. Journal of Statistical\nSoftware, 76(1), 1–32.\n\n\nCaudek, C., & Luccio, R. (2001). Statistica per psicologi.\n\n\nEckhardt, R. (1987). Stan Ulam, John Von Neumann\nand the Monte Carlo Method. Los Alamos Science Special\nIssue.\n\n\nFinetti, B. de. (1931). Probabilismo. Logos, 163–219.\n\n\nGautret, P., Lagier, J. C., Parola, P., Meddeb, L., Mailhe, M., Doudier,\nB., & Honoré, S. (2020). Hydroxychloroquine and azithromycin as a\ntreatment of COVID-19: Results of an open-label non-randomized clinical\ntrial. International Journal of Antimicrobial Agents.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995).\nBayesian data analysis. Chapman; Hall/CRC.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other\nstories. Cambridge University Press.\n\n\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding\npredictive information criteria for bayesian models. Statistics and\nComputing, 24(6), 997–1016.\n\n\nGibson, E., & Wu, H.-H. I. (2013). Processing chinese relative\nclauses in context. Language and Cognitive Processes,\n28(1-2), 125–155.\n\n\nHoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T.\n(1999). Bayesian model averaging: A tutorial (with comments by m. Clyde,\ndavid draper and EI george, and a rejoinder by the authors.\nStatistical Science, 14(4), 382–417.\n\n\nHorstmann, A. C., Bock, N., Linhuber, E., Szczuka, J. M., Straßmann, C.,\n& Krämer, N. C. (2018). Do a robot’s social skills and its objection\ndiscourage interactants from switching the robot off? PloS One,\n13(7), e0201581.\n\n\nHulme, O. J., Wagenmakers, E. J., Damkier, P., Madelung, C. F., Siebner,\nH. R., Helweg-Larsen, J., & Madsen, K. H. (2020). Reply to gautret\net al. 2020: A bayesian reanalysis of the effects of hydroxychloroquine\nand azithromycin on viral carriage in patients with COVID-19.\nmedRxiv.\n\n\nJohnson, A. A., Ott, M., & Dogucu, M. (2022). Bayes Rules! An Introduction to Bayesian Modeling with\nR. CRC Press.\n\n\nKennedy-Shaffer, L. (2019). Before p< 0.05 to beyond p< 0.05:\nUsing history to contextualize p-values and significance testing.\nThe American Statistician, 73(sup1), 82–90.\n\n\nKruschke, J. (2014). Doing bayesian data analysis: A\ntutorial with R, JAGS, and Stan. Academic\nPress.\n\n\nLazic, S. E., Semenova, E., & Williams, D. P. (2020). Determining\norgan weight toxicity with bayesian causal models: Improving on the\nanalysis of relative organ weights. Scientific Reports,\n10(1), 1–12.\n\n\nLee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cognitive\nmodeling: A practical course. Cambridge university press.\n\n\nLord, F. M. (1950). Efficiency of prediction when a regression equation\nfrom one sample is used in a new sample. ETS Research Bulletin\nSeries, 1950(2), 1–6.\n\n\nMartin, O. A., Kumar, R., & Lao, J. (2022). Bayesian modeling\nand computation in python. CRC Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A\nBayesian course with examples in R and\nStan (2nd Edition). CRC Press.\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H.,\n& Teller, E. (1953). Equation of state calculations by fast\ncomputing machines. The Journal of Chemical Physics,\n21(6), 1087–1092.\n\n\nMilgram, S. (1963). Behavioral study of obedience. The Journal of\nAbnormal and Social Psychology, 67(4), 371–378.\n\n\nNavarro, D. J. (2019). Between the devil and the deep blue sea: Tensions\nbetween scientific judgement and statistical model selection.\nComputational Brain & Behavior, 2(1), 28–34.\n\n\nNuggerud-Galeas, S., Sáez-Benito Suescun, L., Berenguer Torrijo, N.,\nSáez-Benito Suescun, A., Aguilar-Latorre, A., Magallón Botaya, R., &\nOliván Blázquez, B. (2020). Analysis of depressive episodes, their\nrecurrence and pharmacologic treatment in primary care patients: A\nretrospective descriptive study. Plos One, 15(5),\ne0233454.\n\n\nRubin, D. B. (1981). Estimation in parallel randomized experiments.\nJournal of Educational Statistics, 6(4), 377–401.\n\n\nSawilowsky, S. S. (2009). New effect size rules of thumb. Journal of\nModern Applied Statistical Methods, 8(2), 26.\n\n\nSchmettow, M. (2021). New statistics for design researchers.\nSpringer.\n\n\nSchoot, R. van de, Depaoli, S., King, R., Kramer, B., Märtens, K.,\nTadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., &\nYau, C. (2021). Bayesian statistics and modelling. Nature Reviews\nMethods Primer, 1(1), 1–26.\n\n\nSong, Q. C., Tang, C., & Wee, S. (2021). Making sense of model\ngeneralizability: A tutorial on cross-validation in r and shiny.\nAdvances in Methods and Practices in Psychological Science,\n4(1), 2515245920947067.\n\n\nSorensen, T., & Vasishth, S. (2015). Bayesian linear mixed models\nusing stan: A tutorial for psychologists, linguists, and cognitive\nscientists. arXiv Preprint arXiv:1506.06201.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680.\n\n\nTufte, E. R. (2001). The visual display of quantitative\ninformation. Graphics press Cheshire, CT.\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical bayesian\nmodel evaluation using leave-one-out cross-validation and WAIC.\nStatistics and Computing, 27(5), 1413–1432.\n\n\nZetsche, U., Bürkner, P.-C., & Renneberg, B. (2019). Future\nexpectations in clinical depression: Biased or realistic?\nJournal of Abnormal Psychology, 128(7), 678–688."
  },
  {
    "objectID": "a01_math_symbols.html",
    "href": "a01_math_symbols.html",
    "title": "Appendix A — Simbologia di base",
    "section": "",
    "text": "\\(\\log(x)\\): il logaritmo naturale di \\(x\\).\nL’operatore logico booleano \\(\\land\\) significa “e” (congiunzione forte) mentre il connettivo di disgiunzione \\(\\lor\\) significa “o” (oppure) (congiunzione debole).\nIl quantificatore esistenziale \\(\\exists\\) vuol dire “esiste almeno un” e indica l’esistenza di almeno una istanza del concetto/oggetto indicato. Il quantificatore esistenziale di unicità \\(\\exists!\\) (“esiste soltanto un”) indica l’esistenza di esattamente una istanza del concetto/oggetto indicato. Il quantificatore esistenziale \\(\\nexists\\) nega l’esistenza del concetto/oggetto indicato.\nIl quantificatore universale \\(\\forall\\) vuol dire “per ogni.”\n\\(\\mathcal{A, S}\\): insiemi.\n\\(x \\in A\\): \\(x\\) è un elemento dell’insieme \\(A\\).\nL’implicazione logica “\\(\\Rightarrow\\)” significa “implica” (se …allora). \\(P \\Rightarrow Q\\) vuol dire che \\(P\\) è condizione sufficiente per la verità di \\(Q\\) e che \\(Q\\) è condizione necessaria per la verità di \\(P\\).\nL’equivalenza matematica “\\(\\iff\\)” significa “se e solo se” e indica una condizione necessaria e sufficiente, o corrispondenza biunivoca.\nIl simbolo \\(\\vert\\) si legge “tale che.”\nIl simbolo \\(\\triangleq\\) (o \\(:=\\)) si legge “uguale per definizione.”\nIl simbolo \\(\\Delta\\) indica la differenza fra due valori della variabile scritta a destra del simbolo.\nIl simbolo \\(\\propto\\) si legge “proporzionale a.”\nIl simbolo \\(\\approx\\) si legge “circa.”\nIl simbolo \\(\\in\\) della teoria degli insiemi vuol dire “appartiene” e indica l’appartenenza di un elemento ad un insieme. Il simbolo \\(\\notin\\) vuol dire “non appartiene.”\nIl simbolo \\(\\subseteq\\) si legge “è un sottoinsieme di” (può coincidere con l’insieme stesso). Il simbolo \\(\\subset\\) si legge “è un sottoinsieme proprio di.”\nIl simbolo \\(\\#\\) indica la cardinalità di un insieme.\nIl simbolo \\(\\cap\\) indica l’intersezione di due insiemi. Il simbolo \\(\\cup\\) indica l’unione di due insiemi.\nIl simbolo \\(\\emptyset\\) indica l’insieme vuoto o evento impossibile.\nIn matematica, \\(\\mbox{argmax}\\) identifica l’insieme dei punti per i quali una data funzione raggiunge il suo massimo. In altre parole, \\(\\mbox{argmax}_x f(x)\\) è l’insieme dei valori di \\(x\\) per i quali \\(f(x)\\) raggiunge il valore più alto.\n\\(a, c, \\alpha, \\gamma\\): scalari.\n\\(\\boldsymbol{x}, \\boldsymbol{y}\\): vettori.\n\\(\\boldsymbol{X}, \\boldsymbol{Y}\\): matrici.\n\\(X \\sim p\\): la variabile casuale \\(X\\) si distribuisce come \\(p\\).\n\\(p(\\cdot)\\): distribuzione di massa o di densità di probabilità.\n\\(p(y \\mid \\boldsymbol{x})\\): la probabilità o densità di \\(y\\) dato \\(\\boldsymbol{x}\\), ovvero \\(p(y = \\boldsymbol{Y} \\mid x = \\boldsymbol{X})\\).\n\\(f(x)\\): una funzione arbitraria di \\(x\\).\n\\(f(\\boldsymbol{X}; \\theta, \\gamma)\\): \\(f\\) è una funzione di \\(\\boldsymbol{X}\\) con parametri \\(\\theta, \\gamma\\). Questa notazione indica che \\(\\boldsymbol{X}\\) sono i dati che vengono passati ad un modello di parametri \\(\\theta, \\gamma\\).\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\): distribuzione gaussiana di media \\(\\mu\\) e varianza \\(sigma^2\\).\n\\(\\mbox{Beta}(\\alpha, \\beta)\\): distribuzione Beta di parametri \\(\\alpha\\) e \\(\\beta\\).\n\\(\\mathcal{U}(a, b)\\): distribuzione uniforme con limite inferiore \\(a\\) e limite superiore \\(b\\).\n\\(\\mbox{Cauchy}(\\alpha, \\beta)\\): distribuzione di Cauchy di parametri \\(\\alpha\\) (posizione: media) e \\(\\beta\\) (scala: radice quadrata della varianza).\n\\(\\mathcal{B}(p)\\): distribuzione di Bernoulli di parametro \\(p\\) (probabilità di successo).\n\\(\\mbox{Bin}(n, p)\\): distribuzione binomiale di parametri \\(n\\) (numero di prove) e \\(p\\) (probabilità di successo).\n\\(\\mathbb{KL} (p \\mid\\mid q)\\): la divergenza di Kullback-Leibler da \\(p\\) a \\(q\\)."
  },
  {
    "objectID": "a02_number_sets.html",
    "href": "a02_number_sets.html",
    "title": "Appendix B — Numeri binari, interi, razionali, irrazionali e reali",
    "section": "",
    "text": "I numeri più semplici sono quelli binari, cioè zero o uno. Useremo spesso numeri binari per indicare se qualcosa è vero o falso, presente o assente. I numeri binari sono molto utili per ottenere facilmente delle statistiche riassuntive in \\(\\mathsf{R}\\).Supponiamo di chiedere a 10 studenti “Ti piacciono i mirtilli?” Poniamo che le risposte siano le seguenti:\n\nCodiceopinion <- c(\n  \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\",\n  \"Yes\", \"Yes\", \"Yes\"\n)\nopinion\n\n [1] \"Yes\" \"No\"  \"Yes\" \"No\"  \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n\n\nTali risposte possono essere ricodificate nei termini di valori di verità, ovvero, vero e falso, generalmente denotati rispettivamente come 1 e 0. In \\(\\R\\) tale ricodifica può essere effettuata mediante l’operatore == che è un test per l’uguaglianza e restituisce il valore logico VERO se i due oggetti valutati sono uguali e FALSO se non lo sono:\n\nCodiceopinion <- opinion == \"Yes\"\nopinion\n\n [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nR considera i valori di verità e i numeri binari in modo equivalente, con TRUE uguale a 1 e FALSE uguale a zero. Di conseguenza, possiamo effettuare operazioni algebriche sui valori logici VERO e FALSO. Nell’esempio, possiamo sommare i valori di verità e dividere per 10\n\nCodicesum(opinion) / length(opinion)\n\n[1] 0.7\n\n\nin modo tale da calcolare una propozione, il che ci consente di concludere che 7 risposte su 10 sono positive."
  },
  {
    "objectID": "a02_number_sets.html#numeri-interi",
    "href": "a02_number_sets.html#numeri-interi",
    "title": "Appendix B — Numeri binari, interi, razionali, irrazionali e reali",
    "section": "\nB.2 Numeri interi",
    "text": "B.2 Numeri interi\nUn numero intero è un numero senza decimali. Si dicono naturali i numeri che servono a contare, come 1, 2, … L’insieme dei numeri naturali si indica con il simbolo \\(\\mathbb{N}\\). È anche necessario introdurre i numeri con il segno per poter trattare grandezze negative. Si ottengono così l’insieme numerico dei numeri interi relativi: \\(\\mathbb{Z} = \\{0, \\pm 1, \\pm 2, \\dots \\}\\)"
  },
  {
    "objectID": "a02_number_sets.html#numeri-razionali",
    "href": "a02_number_sets.html#numeri-razionali",
    "title": "Appendix B — Numeri binari, interi, razionali, irrazionali e reali",
    "section": "\nB.3 Numeri razionali",
    "text": "B.3 Numeri razionali\nI numeri razionali sono i numeri frazionari \\(m/n\\), dove \\(m, n \\in N\\), con \\(n \\neq 0\\). Si ottengono così i numeri razionali: \\(\\mathbb{Q} = \\{\\frac{m}{n} \\,\\vert\\, m, n \\in \\mathbb{Z}, n \\neq 0\\}\\). È evidente che \\(\\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{Q}\\). Anche in questo caso è necessario poter trattare grandezze negative. I numeri razionali non negativi sono indicati con \\(\\mathbb{Q^+} = \\{q \\in \\mathbb{Q} \\,\\vert\\, q \\geq 0\\}\\)."
  },
  {
    "objectID": "a02_number_sets.html#numeri-irrazionali",
    "href": "a02_number_sets.html#numeri-irrazionali",
    "title": "Appendix B — Numeri binari, interi, razionali, irrazionali e reali",
    "section": "\nB.4 Numeri irrazionali",
    "text": "B.4 Numeri irrazionali\nTuttavia, non tutti i punti di una retta \\(r\\) possono essere rappresentati mediante i numeri interi e razionali. È dunque necessario introdurre un’altra classe di numeri. Si dicono irrazionali, e sono denotati con \\(\\mathbb{R}\\), i numeri che possono essere scritti come una frazione \\(a / b\\), con \\(a\\) e \\(b\\) interi e \\(b\\) diverso da 0. I numeri irrazionali sono i numeri illimitati e non periodici che quindi non possono essere espressi sotto forma di frazione. Per esempio, \\(\\sqrt{2}\\), \\(\\sqrt{3}\\) e \\({\\displaystyle \\pi =3,141592\\ldots}\\) sono numeri irrazionali."
  },
  {
    "objectID": "a02_number_sets.html#numeri-reali",
    "href": "a02_number_sets.html#numeri-reali",
    "title": "Appendix B — Numeri binari, interi, razionali, irrazionali e reali",
    "section": "\nB.5 Numeri reali",
    "text": "B.5 Numeri reali\nI punti della retta \\(r\\) sono quindi “di più” dei numeri razionali. Per poter rappresentare tutti i punti della retta abbiamo dunque bisogno dei numeri reali. I numeri reali possono essere positivi, negativi o nulli e comprendono, come casi particolari, i numeri interi, i numeri razionali e i numeri irrazionali. Spesso in statisticac il numero dei decimali indica il grado di precisione della misurazione."
  },
  {
    "objectID": "a02_number_sets.html#intervalli",
    "href": "a02_number_sets.html#intervalli",
    "title": "Appendix B — Numeri binari, interi, razionali, irrazionali e reali",
    "section": "\nB.6 Intervalli",
    "text": "B.6 Intervalli\nUn intervallo si dice chiuso se gli estremi sono compresi nell’intervallo, aperto se gli estremi non sono compresi. Le caratteristiche degli intervalli sono riportate nella tabella seguente.\n\n\nIntervallo\n\n\n\n\n\nchiuso\n\\([a, b]\\)\n\\(a \\leq x \\leq b\\)\n\n\naperto\n\\((a, b)\\)\n\\(a < x < b\\)\n\n\nchiuso a sinistra e aperto a destra\n\\([a, b)\\)\n\\(a \\leq x < b\\)\n\n\naperto a sinistra e chiuso a destra\n\\((a, b]\\)\n\\(a < x \\leq b\\)"
  },
  {
    "objectID": "a03_set_theory.html",
    "href": "a03_set_theory.html",
    "title": "Appendix C — Insiemi",
    "section": "",
    "text": "Un insieme (o collezione, classe, gruppo, …) è un concetto primitivo, ovvero è un concetto che già possediamo. Georg Cantor l’ha definito nel modo seguente: un insieme è una collezione di oggetti, determinati e distinti, della nostra percezione o del nostro pensiero, concepiti come un tutto unico; tali oggetti si dicono elementi dell’insieme.\nMentre non è rilevante la natura degli oggetti che costituiscono l’insieme, ciò che importa è distinguere se un dato oggetto appartenga o meno ad un insieme. Deve essere vera una delle due possibilità: il dato oggetto è un elemento dell’insieme considerato oppure non è elemento dell’insieme considerato. Due insiemi \\(A\\) e \\(B\\) si dicono uguali se sono formati dagli stessi elementi, anche se disposti in ordine diverso: \\(A=B\\). Due insiemi \\(A\\) e \\(B\\) si dicono diversi se non contengono gli stessi elementi: \\(A \\neq B\\). Ad esempio, i seguenti insiemi sono uguali:\n\\[\n\\{1, 2, 3\\} = \\{3, 1, 2\\} = \\{1, 3, 2\\}= \\{1, 1, 1, 2, 3, 3, 3\\}.\n\\]\nGli insiemi sono denotati da una lettera maiuscola, mentre le lettere minuscole, di solito, designano gli elementi di un insieme. Per esempio, un generico insieme \\(A\\) si indica con\n\\[\nA = \\{a_1, a_2, \\dots, a_n\\}, \\quad \\text{con~} n > 0.\n\\]\nLa scrittura \\(a \\in A\\) dice che \\(a\\) è un elemento di \\(A\\). Per dire che \\(b\\) non è un elemento di \\(A\\) si scrive \\(b \\notin A.\\)\nPer quegli insiemi i cui elementi soddisfano una certa proprietà che li caratterizza, tale proprietà può essere usata per descrivere più sinteticamente l’insieme:\n\\[\nA = \\{x ~\\vert~ \\text{proprietà posseduta da~} x\\},\n\\]\nche si legge come “\\(A\\) è l’insieme degli elementi \\(x\\) per cui è vera la proprietà indicata.” Per esempio, per indicare l’insieme \\(A\\) delle coppie di numeri reali \\((x,y)\\) che appartengono alla parabola \\(y = x^2 + 1\\) si può scrivere:\n\\[\nA = \\{(x,y) ~\\vert~ y = x^2 + 1\\}.\n\\]\nDati due insiemi \\(A\\) e \\(B\\), diremo che \\(A\\) è un sottoinsieme di \\(B\\) se e solo se tutti gli elementi di \\(A\\) sono anche elementi di \\(B\\):\n\\[\nA \\subseteq B \\iff (\\forall x \\in A \\Rightarrow x \\in B).\n\\]\nSe esiste almeno un elemento di \\(B\\) che non appartiene ad \\(A\\) allora diremo che \\(A\\) è un sottoinsieme proprio di \\(B\\):\n\\[\nA \\subset B \\iff (A \\subseteq B, \\exists~ x \\in B ~\\vert~ x \\notin A).\n\\]\nUn altro insieme, detto insieme delle parti, o insieme potenza, che si associa all’insieme \\(A\\) è l’insieme di tutti i sottoinsiemi di \\(A\\), inclusi l’insieme vuoto e \\(A\\) stesso. Per esempio, per l’insieme \\(A = \\{a, b, c\\}\\), l’insieme delle parti è:\n\\[\n\\mathcal{P}(A) = \\{\n\\emptyset, \\{a\\}, \\{b\\}, \\{c\\},\n\\{a, b\\}, \\{a, c\\}, \\{c, b\\},\n\\{a, b, c\\}\n\\}.\n\\]"
  },
  {
    "objectID": "a03_set_theory.html#operazioni-tra-insiemi",
    "href": "a03_set_theory.html#operazioni-tra-insiemi",
    "title": "Appendix C — Insiemi",
    "section": "\nC.1 Operazioni tra insiemi",
    "text": "C.1 Operazioni tra insiemi\nSi definisce intersezione di \\(A\\) e \\(B\\) l’insieme \\(A \\cap B\\) di tutti gli elementi \\(x\\) che appartengono ad \\(A\\) e contemporaneamente a \\(B\\):\n\\[\nA \\cap B = \\{x ~\\vert~ x \\in A \\land x \\in B\\}.\n\\]\nSi definisce unione di \\(A\\) e \\(B\\) l’insieme \\(A \\cup B\\) di tutti gli elementi \\(x\\) che appartengono ad \\(A\\) o a \\(B\\), cioè\n\\[\nA \\cup B = \\{x ~\\vert~ x \\in A \\lor x \\in B\\}.\n\\]\nDifferenza. Si indica con \\(A \\setminus B\\) l’insieme degli elementi di \\(A\\) che non appartengono a \\(B\\):\n\\[\nA \\setminus B = \\{x ~\\vert~ x \\in A \\land x \\notin B\\}.\n\\]\nInsieme complementare. Nel caso che sia \\(B \\subseteq A\\), l’insieme differenza \\(A \\setminus B\\) è detto insieme complementare di \\(B\\) in \\(A\\) e si indica con \\(B^C\\).\nDato un insieme \\(S\\), una partizione di \\(S\\) è una collezione di sottoinsiemi di \\(S\\), \\(S_1, \\dots, S_k\\), tali che\n\\[\nS = S_1 \\cup S_2 \\cup \\dots S_k\n\\]\ne\n\\[\nS_i \\cap S_j, \\quad \\text{con~} i \\neq j.\n\\]\nLa relazione tra unione, intersezione e insieme complementare è data dalle leggi di DeMorgan:\n$$ (A B)^c = A^c B^c,\n\\[ \\]\n(A B)^c = A^c B^c. $$"
  },
  {
    "objectID": "a03_set_theory.html#diagrammi-di-eulero-venn",
    "href": "a03_set_theory.html#diagrammi-di-eulero-venn",
    "title": "Appendix C — Insiemi",
    "section": "\nC.2 Diagrammi di Eulero-Venn",
    "text": "C.2 Diagrammi di Eulero-Venn\nIn molte situazioni è utile servirsi dei cosiddetti diagrammi di Eulero-Venn per rappresentare gli insiemi e verificare le proprietà delle operazioni tra insiemi (si veda la figura @ref(fig:sets-venn-diagrams). I diagrammi di Venn sono così nominati in onore del matematico inglese del diciannovesimo secolo John Venn anche se Leibnitz e Eulero avevano già in precedenza utilizzato rappresentazioni simili. In tale rappresentazione, gli insiemi sono individuati da regioni del piano delimitate da una curva chiusa. Nel caso di insiemi finiti, è possibile evidenziare esplicitamente alcuni elementi di un insieme mediante punti, quando si possono anche evidenziare tutti gli elementi degli insiemi considerati.\n\n\n\n\nIn tutte le figure \\(S\\) è la regione delimitata dal rettangolo, \\(L\\) è la regione all’interno del cerchio di sinistra e \\(R\\) è la regione all’interno del cerchio di destra. La regione evidenziata mostra l’insieme indicato sotto ciascuna figura.\n\n\n\n\nI diagrammi di Eulero-Venn che forniscono una dimostrazione delle leggi di DeMorgan sono forniti nella figura @ref(fig:demorgan).\n\n\n\n\nDimostrazione delle leggi di DeMorgan."
  },
  {
    "objectID": "a03_set_theory.html#coppie-ordinate-e-prodotto-cartesiano",
    "href": "a03_set_theory.html#coppie-ordinate-e-prodotto-cartesiano",
    "title": "Appendix C — Insiemi",
    "section": "\nC.3 Coppie ordinate e prodotto cartesiano",
    "text": "C.3 Coppie ordinate e prodotto cartesiano\nUna coppia ordinata \\((x,y)\\) è l’insieme i cui elementi sono \\(x \\in A\\) e \\(y \\in B\\) e nella quale \\(x\\) è la prima componente (o prima coordinata), \\(y\\) la seconda. L’insieme di tutte le coppie ordinate costruite a partire dagli insiemi \\(A\\) e \\(B\\) viene detto prodotto cartesiano:\n\\[\nA \\times B = \\{(x, y) ~\\vert~ x \\in A \\land y \\in B\\}.\n\\]\nAd esempio, sia \\(A = \\{1, 2, 3\\}\\) e \\(B = \\{a, b\\}\\). Allora,\n\\[\n\\{1, 2\\} \\times \\{a, b, c\\} = \\{(1, a), (1, b), (1, c), (2, a), (2, b), (2, c)\\}.\n\\]"
  },
  {
    "objectID": "a03_set_theory.html#cardinalità",
    "href": "a03_set_theory.html#cardinalità",
    "title": "Appendix C — Insiemi",
    "section": "\nC.4 Cardinalità",
    "text": "C.4 Cardinalità\nSi definisce cardinalità (o potenza) di un insieme finito il numero degli elementi dell’insieme. Viene indicata con \\(\\vert A\\vert, \\#(A)\\) o \\(\\text{c}(A)\\)."
  },
  {
    "objectID": "a04_summation_notation.html",
    "href": "a04_summation_notation.html",
    "title": "Appendix D — Simbolo di somma (sommatorie)",
    "section": "",
    "text": "Le somme si incontrano costantemente in svariati contesti matematici e statistici quindi abbiamo bisogno di una notazione adeguata che ci consenta di gestirle. La somma dei primi \\(n\\) numeri interi può essere scritta come \\(1+2+\\dots+(n-1)+n\\), dove `\\(\\dots\\)’ ci dice di completare la sequenza definita dai termini che vengono prima e dopo. Ovviamente, una notazione come \\(1+7+\\dots+73.6\\) non avrebbe alcun senso senza qualche altro tipo di precisazione. In generale, nel seguito incontreremo delle somme nella forma\ndove \\(x_n\\) è un numero che è stato definito altrove. La notazione precedente, che fa uso dei tre puntini di sospensione, è utile in alcuni contesti ma in altri risulta ambigua. Pertanto la notazione di uso corrente è del tipo\ne si legge “sommatoria per \\(i\\) che va da \\(1\\) a \\(n\\) di \\(x_i\\)”. Il simbolo \\(\\sum\\) (lettera sigma maiuscola dell’alfabeto greco) indica l’operazione di somma, il simbolo \\(x_i\\) indica il generico addendo della sommatoria, le lettere \\(1\\) ed \\(n\\) indicano i cosiddetti estremi della sommatoria, ovvero l’intervallo (da \\(1\\) fino a \\(n\\) estremi inclusi) in cui deve variare l’indice \\(i\\) allorché si sommano gli addendi \\(x_i\\). Solitamente l’estremo inferiore è \\(1\\) ma potrebbe essere qualsiasi altri numero \\(m < n\\). Quindi\n\\[\n\\sum_{i=1}^n x_i = x_1 + x_{2} + \\dots + x_{n}.\n\\]\nPer esempio, se i valori \\(x\\) sono \\(\\{3, 11, 4, 7\\}\\), si avrà\n\\[\n\\sum_{i=1}^4 x_i = 3+11+4+7 = 25\n\\]\nladdove \\(x_1 = 3\\), \\(x_2 = 11\\), eccetera. La quantità \\(x_i\\) nella formula precedente si dice l’argomento della sommatoria, mentre la variabile \\(i\\), che prende i valori naturali successivi indicati nel simbolo, si dice indice della sommatoria.\nLa notazione di sommatoria può anche essere fornita nella forma seguente\n\\[\n\\sum_{P(i)} x_i\\notag\n\\]\ndove \\(P(i)\\) è qualsiasi proposizione riguardante \\(i\\) che può essere vera o falsa. Quando è ovvio che si vogliono sommare tutti i valori di \\(n\\) osservazioni, la notazione può essere semplificata nel modo seguente: \\(\\sum_{i} x_i\\) oppure \\(\\sum x_i\\). Al posto di \\(i\\) si possono trovare altre lettere: \\(k, j, l, \\dots\\),."
  },
  {
    "objectID": "a04_summation_notation.html#manipolazione-di-somme",
    "href": "a04_summation_notation.html#manipolazione-di-somme",
    "title": "Appendix D — Simbolo di somma (sommatorie)",
    "section": "\nD.1 Manipolazione di somme",
    "text": "D.1 Manipolazione di somme\nÈ conveniente utilizzare le seguenti regole per semplificare i calcoli che coinvolgono l’operatore della sommatoria.\n\nD.1.1 Proprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa:\n\\[\n\\sum_{i=1}^{n} a = \\underbrace{a + a + \\dots + a} = {n\\text{ volte } a} = n a.\n\\]\n\nD.1.2 Proprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con\n\\[\n\\sum_{i=1}^{n} a x_i = a x_1 + a x_2 + \\dots + a x_n\n\\]\nè possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere\n\\[\n\\sum_{i=1}^{n} a x_i = a \\sum_{i=1}^{n} x_i.\n\\]\n\nD.1.3 Proprietà 3 (proprietà associativa)\nNel caso in cui\n\\[\n\\sum_{i=1}^{n} (a + x_i) = (a + x_1) + (a + x_1) + \\dots  (a + x_n)\n\\]\nsi ha che\n\\[\n\\sum_{i=1}^{n} (a + x_i) = n a + \\sum_{i=1}^{n} x_i.\n\\]\nÈ dunque chiaro che in generale possiamo scrivere\n\\[\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i.\n\\]\n\nD.1.4 Proprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio,\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots + x_n^2 \\neq \\left(\\sum_{i=1}^{n} x_i \\right)^2.\n\\]\n\nD.1.5 Proprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo:\n\\[\n\\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n,\n\\]\ninfatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\)."
  },
  {
    "objectID": "a04_summation_notation.html#doppia-sommatoria",
    "href": "a04_summation_notation.html#doppia-sommatoria",
    "title": "Appendix D — Simbolo di somma (sommatorie)",
    "section": "\nD.2 Doppia sommatoria",
    "text": "D.2 Doppia sommatoria\nÈ possibile incontrare la seguente espressione in cui figurano una doppia sommatoria e un doppio indice:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{m} x_{ij}.\n\\]\nLa doppia sommatoria comporta che per ogni valore dell’indice esterno, \\(i\\) da \\(1\\) ad \\(n\\), occorre sviluppare la seconda sommatoria per \\(j\\) da \\(1\\) ad \\(m\\). Quindi,\n\\[\n\\sum_{i=1}^{3}\\sum_{j=4}^{6} x_{ij} = (x_{1, 4} + x_{1, 5} + x_{1, 6}) + (x_{2, 4} + x_{2, 5} + x_{2, 6}) + (x_{3, 4} + x_{3, 5} + x_{3, 6}).\n\\]\nUn caso particolare interessante di doppia sommatoria è il seguente:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j\n\\]\nSi può osservare che nella sommatoria interna (quella che dipende dall’indice \\(j\\)), la quantità \\(x_i\\) è costante, ovvero non dipende dall’indice (che è \\(j\\)). Allora possiamo estrarre \\(x_i\\) dall’operatore di sommatoria interna e scrivere\n\\[\n\\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right).\n\\]\nAllo stesso modo si può osservare che nell’argomento della sommatoria esterna la quantità costituita dalla sommatoria in \\(j\\) non dipende dall’indice \\(i\\) e quindi questa quantità può essere estratta dalla sommatoria esterna. Si ottiene quindi\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j = \\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right) = \\sum_{i=1}^{n} x_i \\sum_{j=1}^{n} y_j.\n\\]\n\nSi verifichi quanto detto sopra nel caso particolare di \\(x = \\{2, 3, 1\\}\\) e \\(y = \\{1, 4, 9\\}\\), svolgendo prima la doppia sommatoria per poi verificare che quanto così ottenuto sia uguale al prodotto delle due sommatorie.\n\\[\\begin{align}\n\\sum_{i=1}^3 \\sum_{j=1}^3 x_i y_j &= x_1y_1 + x_1y_2 + x_1y_3 +\nx_2y_1 + x_2y_2 + x_2y_3 +\nx_3y_1 + x_3y_2 + x_3y_3 \\notag\\\\\n&= 2 \\times (1+4+9) + 3 \\times (1+4+9) + 2 \\times (1+4+9) = 84,\\notag\n\\end{align}\\]\novvero\n[ (2 + 3 + 1) (1+4+9) = 84. ]"
  },
  {
    "objectID": "a04_summation_notation.html#sommatorie-e-produttorie-e-operazioni-vettoriali-in-r",
    "href": "a04_summation_notation.html#sommatorie-e-produttorie-e-operazioni-vettoriali-in-r",
    "title": "Appendix D — Simbolo di somma (sommatorie)",
    "section": "\nD.3 Sommatorie (e produttorie) e operazioni vettoriali in R\n",
    "text": "D.3 Sommatorie (e produttorie) e operazioni vettoriali in R\n\nSi noti che la notazione\n\\[\n\\sum_{n=0}^4 3n\n\\]\nnon è altro che un ciclo for:\n\nCodicesum <- 0\nfor (n in 0:4) {\n  sum = sum + 3 * n\n}\nsum\n\n[1] 30\n\n\nIn maniera equivalente, e più semplice, possiamo scrivere\n\nCodicesum(3 * (0:4))\n\n[1] 30\n\n\nAllo stesso modo, la notazione\n\\[\n\\prod_{n=1}^{4} 2n\n\\] è anch’essa equivalente al ciclo for\n\nCodiceprod <- 1\nfor (n in 1:4) {\n  prod <- prod * 2 * n\n}\nprod\n\n[1] 384\n\n\nche si può scrivere, più semplicemente, come\n\nCodiceprod(2 * (1:4))\n\n[1] 384\n\n\nIn entrambi i casi precedenti, abbiamo sostituito le operazioni aritmetiche eseguite all’interno di un ciclo for con le stesse operazioni aritmetiche eseguite sui vettori elemento per elemento."
  },
  {
    "objectID": "051_reglin1.html",
    "href": "051_reglin1.html",
    "title": "25  Introduzione",
    "section": "",
    "text": "Il fine primario dei ricercatori è quello di scoprire associazioni tra variabili e di fare confronti fra condizioni sperimentali. Nel caso della psicologia, il ricercatore vuole descrivere le relazioni tra i costrutti psicologici e le relazioni tra fenomeni psicologici e non psicologici (sociali, economici, storici, …). Abbiamo già visto come la correlazione di Pearson sia uno strumento adatto a descrivere le relazioni tra variabili. Infatti, la correlazione ci informa sulla direzione e sull’intensità della relazione lineare tra due variabili. Tuttavia, la correlazione non è sufficiente, in quanto il ricercatore ha a disposizione solo i dati di un campione, mentre vorrebbe descrivere la relazione tra le variabili nella popolazione. A causa della variabilità campionaria, le proprietà dei campioni sono necessariamente diverse da quelle della popolazione: ciò che si può osservare nella popolazione potrebbe non emergere nel campione e, al contrario, il campione può manifestare caratteristiche che non sono presenti nella popolazione. È dunque necessario chiarire, dal punto di vista statistico, il legame che intercorre tra le proprietà del campione e le proprietà della popolazione da cui esso è stato estratto. Questo è l’obiettivo del modello di regressione lineare. Tale modello utilizza la funzione matematica più semplice per descrivere la relazione fra due variabili, ovvero la funzione lineare. In questo Capitolo vedremo come il modello di regressione lineare possa essere usato per fare inferenza sulla relazione tra variabili. Inizieremo a descrivere le proprietà geometriche della funzione lineare per poi utilizzare questa semplice funzione per costruire un modello statistico secondo un approccio bayesiano."
  },
  {
    "objectID": "051_reglin1.html#la-funzione-lineare",
    "href": "051_reglin1.html#la-funzione-lineare",
    "title": "25  Introduzione",
    "section": "\n25.1 La funzione lineare",
    "text": "25.1 La funzione lineare\nIniziamo con un ripasso sulla funzione di lineare. Si chiama funzione lineare una funzione del tipo\n\\[\nf(x) = a + b x,\n\\]\ndove \\(a\\) e \\(b\\) sono delle costanti. Il grafico di tale funzione è una retta di cui il parametro \\(b\\) è detto coefficiente angolare e il parametro \\(a\\) è detto intercetta con l’asse delle \\(y\\) [infatti, la retta interseca l’asse \\(y\\) nel punto \\((0,a)\\), se \\(b \\neq 0\\)].\nPer assegnare un’interpretazione geometrica alle costanti \\(a\\) e \\(b\\) si consideri la funzione\n\\[\ny = b x.\n\\]\nTale funzione rappresenta un caso particolare, ovvero quello della proporzionalità diretta tra \\(x\\) e \\(y\\). Il caso generale della linearità\n\\[\ny = a + b x\n\\]\nnon fa altro che sommare una costante \\(a\\) a ciascuno dei valori \\(y = b x\\). Nella funzione lineare \\(y = a + b x\\), se \\(b\\) è positivo allora \\(y\\) aumenta al crescere di \\(x\\); se \\(b\\) è negativo \\(y\\) diminuisce al crescere di \\(x\\); se \\(b=0\\) la retta è orizzontale, ovvero \\(y\\) non muta al variare di \\(x\\).\nConsideriamo ora più in dettaglio il coefficiente \\(b\\). Si consideri un punto \\(x_0\\) e un incremento arbitrario \\(\\varepsilon\\), come indicato nella Figura 25.1. Le differenze \\(\\Delta x = (x_0 + \\varepsilon) - x_0\\) e \\(\\Delta y = f(x_0 + \\varepsilon) - f(x_0)\\) sono detti incrementi di \\(x\\) e \\(y\\). Il coefficiente angolare \\(b\\) è uguale al rapporto\n\\[\nb = \\frac{\\Delta y}{\\Delta x} = \\frac{f(x_0 + \\varepsilon) - f(x_0)}{(x_0 + \\varepsilon) - x_0},\n\\]\nindipendentemente dalla grandezza degli incrementi \\(\\Delta x\\) e \\(\\Delta y\\). Il modo più semplice per assegnare un’interpretazione geometrica al coefficiente angolare (o pendenza) della retta è quello di porre \\(\\Delta x = 1\\). In tali circostanze, \\(b = \\Delta y\\).\n\n\n\n\nFigura 25.1: La funzione lineare \\(y = a + bx\\).\n\n\n\n\nPossiamo dunque dire che la pendenza \\(b\\) di un retta è uguale all’incremento \\(\\Delta y\\) associato ad un incremento unitario nella \\(x\\)."
  },
  {
    "objectID": "051_reglin1.html#una-media-per-ciascuna-osservazione",
    "href": "051_reglin1.html#una-media-per-ciascuna-osservazione",
    "title": "25  Introduzione",
    "section": "\n25.2 Una media per ciascuna osservazione",
    "text": "25.2 Una media per ciascuna osservazione\nIn precedenza abbiamo visto come stimare i parametri di un modello bayesiano nel quale le osservazioni sono indipendenti e identicamente distribuite secondo una densità gaussiana,\n\\[\nY_i \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\mu, \\sigma), \\quad i = 1, \\dots, n.\n\\tag{25.1}\\]\nIl modello dell’Equazione 25.1 assume che ogni \\(Y_i\\) sia la realizzazione di una v.c. distribuita come \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Da un punto di vista bayesiano, questo modello può essere implementato assegnando delle distribuzioni a priori ai parametri \\(\\mu\\) e \\(\\sigma\\) e generando la verosimiglianza in base ai dati osservati.\n\\[\n\\begin{align}\nY_i \\mid \\mu, \\sigma & \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)\\notag\\\\\n\\mu    & \\sim \\mathcal{N}(\\mu_0, \\tau^2) \\notag\\\\\n\\sigma & \\sim \\mbox{Cauchy}(x_0, \\gamma) \\notag\n\\end{align}\n\\]\nCon queste informazioni, possono poi essere trovate le distribuzioni a posteriori dei parametri (Gelman et al., 2020). Vediamo ora come sia possibile estendere questo modello bayesiano in modo che possa descrivere la relazione lineare tra due variabili."
  },
  {
    "objectID": "051_reglin1.html#relazione-lineare-tra-la-media-y-mid-x-e-il-predittore",
    "href": "051_reglin1.html#relazione-lineare-tra-la-media-y-mid-x-e-il-predittore",
    "title": "25  Introduzione",
    "section": "\n25.3 Relazione lineare tra la media \\(y \\mid x\\) e il predittore",
    "text": "25.3 Relazione lineare tra la media \\(y \\mid x\\) e il predittore\nIl ricercatore si trova spesso nella condizione in cui osserva altre variabili di interesse associate a ciascuna risposta \\(y_i\\). Chiamiamo \\(x\\) una di tali variabili. Nel contesto del modello di regressione, la variabile \\(x\\) viene chiamata predittore (o variabile indipendente), in quanto il ricercatore è tipicamente interessato a predire \\(y_i\\) a partire dal valore assunto da \\(x_i\\). Chiediamoci dunque come si può estende il modello dell’Equazione 25.1 per lo studio della relazione tra \\(y_i\\) e \\(x_i\\).\nL’Equazione 25.1 assume una media \\(\\mu\\) comune per tutte le osservazioni \\(Y_i\\). Dal momento che desideriamo introdurre una nuova variabile \\(x_i\\) che assume un diverso valore per ciascuna osservazione \\(y_i\\), l’Equazione 25.1 può essere modificata così da sostituire alla media comune \\(\\mu\\) una media \\(\\mu_i\\) specifica a ciascuna \\(i\\)-esima osservazione:\n\\[\nY_i \\mid \\mu_i, \\sigma \\stackrel{ind}{\\sim} \\mathcal{N}(\\mu_i, \\sigma), \\quad i = 1, \\dots, n.\n\\tag{25.2}\\]\nSi noti che le osservazioni \\(Y_1, \\dots, Y_n\\) non sono più identicamente distribuite poiché hanno medie diverse, ma sono ancora indipendenti come indicato dalla notazione ind posta sopra il simbolo \\(\\sim\\) nell’Equazione 25.2.\nL’Equazione 25.2 afferma che ciascuna osservazione \\(Y_i\\) è estratta a caso dalla corrispondente distribuzione \\(\\mathcal{N}(\\mu_i, \\sigma)\\). Al fine di potere descrivere la relazione tra il predittore \\(x_i\\) e la risposta \\(Y_i\\), il modello di regressione assume che la media della distribuzione da cui abbiamo estratto \\(Y_i\\), ovvero \\(\\mu_i\\), sia una funzione lineare del predittore \\(x_i\\), ovvero\n\\[\n\\mu_i = \\beta_0 + \\beta_ 1 x_i, \\quad i = 1, \\dots, n.\n\\tag{25.3}\\]\nNell’Equazione 25.3, ciascuna \\(x_i\\) è una costante nota (ecco perché viene usata una lettera minuscola per la \\(x\\)) e \\(\\beta_0\\) e \\(\\beta_ 1\\) sono parametri incogniti. Questi parametri rappresentano l’intercetta e la pendenza della retta di regressione e sono delle variabili casuali.1 L’inferenza bayesiana procede assegnando una distribuzione a priori a \\(\\beta_0\\) e a \\(\\beta_1\\), trovando la verosimiglianza dei dati e calcolando la distribuzione a priori dei parametri \\(\\beta_0\\) e a \\(\\beta_1\\).\nNel modello dell’Equazione 25.3, la funzione lineare \\(\\beta_0 + \\beta_1 x_i\\) è interpretata come il valore atteso della \\(Y_i\\) per ciascun valore \\(x_i\\). L’intercetta \\(\\beta_0\\) rappresenta il valore atteso della \\(Y_i\\) quando \\(x_i = 0\\). La pendenza \\(\\beta_1\\) rappresenta l’incremento atteso della \\(Y_i\\) quando \\(x_i\\) aumenta di un’unità.\nÈ importante notare che la relazione lineare dell’Equazione 25.2 di parametri \\(\\beta_0\\) e \\(\\beta_1\\) descrive l’associazione tra la media \\(\\mu_i\\) e il predittore \\(x_i\\). In altri termini, tale relazione lineare fornisce una predizione sul valore atteso \\(\\mu_i\\), non sul valore effettivo di ciascuna osservazione \\(Y_i\\)."
  },
  {
    "objectID": "051_reglin1.html#il-modello-lineare",
    "href": "051_reglin1.html#il-modello-lineare",
    "title": "25  Introduzione",
    "section": "\n25.4 Il modello lineare",
    "text": "25.4 Il modello lineare\nSostituendo l’Equazione 25.3 nell’Equazione 25.2 otteniamo il modello lineare:\n\\[\nY_i \\mid \\beta_0, \\beta_ 1, \\sigma \\stackrel{ind}{\\sim} \\mathcal{N}(\\beta_0 + \\beta_ 1 x_i, \\sigma), \\quad i = 1, \\dots, n.\n\\tag{25.4}\\]\nL’Equazione 25.4 è dunque un caso speciale del modello di campionamento Normale, dove le \\(Y_i\\) seguono indipendentemente una densità Normale di media (\\(\\beta_0 + \\beta_ 1 x_i\\)) specifica per ciascuna osservazione, con una deviazione standard (\\(\\sigma\\)) comune a tutte le osservazioni. Poiché include un solo predittore (\\(x\\)), questo modello è chiamato modello di regressione lineare bivariato.\nIl modello di regressione lineare bivariato può essere rappresentato in forma geometrica come indicato nella Figura 25.2. La figura illustra che, in tale modello statistico, la variabile \\(x\\) è fissa per disegno – in altre parole, i valori \\(x\\) restano immutati tra campioni diversi. Potendo ipotizzare infiniti campioni tutti con gli stessi valori \\(x\\), in corrispondenza di ciascun valore \\(x_i\\) vi sarà una distribuzione di valori \\(y\\). La Figura 25.2 illustra il caso di tre valori \\(x\\). A ciascun valore \\(x_i\\), con \\(i = 1, 2, 3\\), corrisponde una distribuzione di valori \\(y\\) condizionati a \\(x_i\\), \\(p(y \\mid x_i)\\).\n\n\n\n\nFigura 25.2: Modello statistico di regressione lineare bivariato.\n\n\n\n\nIl modello statistico di regressione lineare assume che le distribuzioni condizionate \\(p(y \\mid x_i)\\) sono\n\\[\ny_i \\sim \\mathcal{N}(\\mu_i, \\sigma),\n\\]\n(assunzione di normalità), laddove\n\\[\n\\mu_i = \\mathbb{E}(y \\mid x_i) = \\alpha + \\beta x_i.\n\\]\nL’equazione precedente descrive l’assunzione di linearità.\nSi noti che il parametro \\(\\sigma\\) non ha un pedice: questo significa che il modello ipotizza una dispersione costante delle distribuzioni \\(p(y \\mid x_i), \\forall i\\). Tale assunzione va sotto il nome di omoschedasticità.\nSe questa è la struttura della popolazione, possiamo pensare ad un campione casuale di ampiezza \\(n\\) come ad una serie di coppie \\(x_i, y_i\\), con \\(i = 1, \\dots, n\\), nelle quali i valori \\(x\\) sono fissi per disegno e ciascun valore \\(y_i\\) è una realizzazione della variabile casuale \\(Y = y_i \\mid X = x_i\\). Questa è l’ultima assunzione del modello statistico lineare: l’indipendenza. In maniera equivalente possiamo dire che gli errori \\(\\varepsilon_i = y_i - \\hat{y}_i = y_i - (\\beta_0 + \\beta_1 x_i)\\) sono variabili casuali distribuite secondo la legge Normale di parametri \\(\\mathcal{N}(0, \\sigma)\\)."
  },
  {
    "objectID": "051_reglin1.html#commenti-e-considerazioni-finali",
    "href": "051_reglin1.html#commenti-e-considerazioni-finali",
    "title": "25  Introduzione",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nIl modello di regressione lineare bivariato viene usato per descrivere la relazione tra due variabili \\(x\\) e \\(Y\\), e per determinare il segno e l’intensità di tale relazione. Inoltre, il modello di regressione lineare consente di prevedere il valore della variabile dipendente \\(Y\\) in base al valore assunto dalla variabile indipendente \\(x\\).\n\n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press."
  },
  {
    "objectID": "055_reglin5.html",
    "href": "055_reglin5.html",
    "title": "29  Confronto tra due gruppi indipendenti",
    "section": "",
    "text": "Il problema del confronto tra due gruppi indipendenti può essere formulato nei termini di un modello lineare nel quale la variabile \\(X\\) è dicotomica, ovvero assume solo due valori."
  },
  {
    "objectID": "055_reglin5.html#modello-lineare-con-una-variabile-dicotomica",
    "href": "055_reglin5.html#modello-lineare-con-una-variabile-dicotomica",
    "title": "29  Confronto tra due gruppi indipendenti",
    "section": "\n29.1 Modello lineare con una variabile dicotomica",
    "text": "29.1 Modello lineare con una variabile dicotomica\nSe \\(X\\) è una variabile dicotomica con valori 0 e 1, allora per il modello lineare \\(\\mu_i = \\alpha + \\beta x_i\\) abbiamo quanto segue. Quando \\(x=0\\), il modello diventa\n\\[\n\\mu_i = \\alpha\n\\]\nmentre, quando \\(x=1\\), il modello diventa\n\\[\n\\mu_i = \\alpha + \\beta.\n\\]\nCiò significa che il parametro \\(\\alpha\\) è uguale al valore atteso del gruppo codificato con \\(X=0\\) e il parametro \\(\\beta\\) è uguale alla differenza tra le medie dei due gruppi (essendo la media del secondo gruppo uguale a \\(\\alpha + \\beta\\)). Il parametro \\(\\beta\\), dunque, codifica l’effetto di una manipolazione sperimentale o di un trattamento, e l’inferenza su \\(\\beta\\) corrisponde direttamente all’inferenza sull’efficacia di un trattamento o di un effetto sperimentale. L’inferenza su \\(\\beta\\), dunque, viene utilizzata per capire quanto “credibile” può essere considerato l’effetto di un trattamento o di una manipolazione sperimentale.\n\n29.1.1 Confronti, non effetti\nPer “effetto di un trattamento” si intende la differenza tra le medie di due gruppi (per esempio, il gruppo “sperimentale” e il gruppo “di controllo”). Gelman et al. (2020) fanno notare come l’uso della terminologia “effetto” implica un modello causale: una variazione di \\(X\\) produce una variazione di \\(Y\\). In generale, il modello lineare descrive una regolarità osservabile nel campione di dati. Ma questa regolarità (ovvero, la presenza di una relazione approssimativamente lineare tra \\(X\\) e \\(Y\\)) non ci dice nulla della presenza (o dell’assenza) di una relazione di causa/effetto tra queste variabili. L’associazione osservata tra le variabili \\(X\\) e \\(Y\\) potrebbe dipendere dall’effetto di una o più altre variabili non misurate, senza che tra \\(X\\) e \\(Y\\) ci sia alcuna relazione causale. In tali circostanze, l’interpretazione più appropriata dei coefficienti del modello lineare è quella che ci porta a pensare ai coefficienti del modello come ai risultati di un confronto. Nel caso presente, il confronto è quello tra il valore atteso del quoziente di intelligenza dei bambini, quando la madre ha oppure non ha completato il ciclo di istruzione secondaria superiore. Dato che l’affermazione precedente è formulata nei termini del valore atteso, questo significa che facciamo riferimento ad un campione di osservazioni. Niente viene detto della relazione causale tra il quoziente di intelligenza del bambino e l’ottenimento del diploma di scuola superiore da parte della madre all’interno del singolo soggetto. Quindi, quando usiamo il termine “effetto” dobbiamo sempre pensare a tale termine come come se fosse contenuto tra virgolette.\n\n29.1.2 Un esempio concreto\nEsaminiamo nuovamente i dati kid_score discussi da Gelman et al. (2020). La domanda della ricerca è se il QI del figlio (misurato sulla scala PIAT) è associato al livello di istruzione della madre.\nCodifichiamo il livello di istruzione della madre (\\(x\\)) con una variabile indicatrice (ovvero, una variabile che assume solo i valori 0 e 1) tale per cui:\n\n\n\\(x=0\\): la madre non ha completato la scuola secondaria di secondo grado (scuola media superiore);\n\n\\(x=1\\): la madre ha completato la scuola media superiore.\n\nSupponiamo che i dati siano contenuti nel data.frame df.\n\nCodicelibrary(\"rio\")\ndf <- rio::import(here(\"data\", \"kidiq.dta\"))\n\n\nCalcoliamo le statistiche descrittive per i due gruppi:\n\nCodicedf %>% \n  group_by(mom_hs) %>% \n  summarise(\n    mean_kid_score = mean(kid_score),\n    std = sqrt(var(kid_score))\n  )\n#> # A tibble: 2 × 3\n#>   mom_hs mean_kid_score   std\n#>    <dbl>          <dbl> <dbl>\n#> 1      0           77.5  22.6\n#> 2      1           89.3  19.0\n\n\nIl punteggio medio PIAT è pari a 77.5 per i bambini la cui madre non ha il diploma di scuola media superiore e pari a 89.3 per i bambini la cui madre ha completato la scuola media superiore. Questa differenza suggerisce un’associazione tra le variabili, ma tale differenza potrebbe essere soltanto la conseguenza della variabilità campionaria, senza riflettere una caratteristica generale della popolazione. Come possiamo usare il modello statistico lineare per fare inferenza sulla differenza osservata tra i due gruppi? Non dobbiamo fare nient’altro che usare il modello lineare che abbiamo definito in precedenza.\n\nCodicemodelString = \"\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  vector[N] x;\n}\ntransformed data {\n  vector[N] x_std;\n  vector[N] y_std;\n  x_std = (x - mean(x)) / sd(x);\n  y_std = (y - mean(y)) / sd(y);\n}\nparameters {\n  real alpha_std;\n  real beta_std;\n  real<lower=0> sigma_std;\n}\nmodel {\n  alpha_std ~ normal(0, 2);\n  beta_std ~ normal(0, 2);\n  sigma_std ~ cauchy(0, 2);\n  y_std ~ normal(alpha_std + beta_std * x_std, sigma_std);\n}\ngenerated quantities {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n  real cohen_d;\n  alpha = sd(y) * (alpha_std - beta_std * mean(x) / sd(x)) + mean(y);\n  beta = beta_std * sd(y) / sd(x);\n  sigma = sd(y) * sigma_std;\n  cohen_d = beta / sigma;\n}\n\"\nwriteLines(modelString, con = \"code/simpleregstd.stan\")\n\n\nCome in precedenza, salviamo i dati in un oggetto di classe list:\n\nCodicedata_list <- list(\n  N = length(df$kid_score),\n  y = df$kid_score,\n  x = df$mom_hs\n)\n\n\nCompiliamo il modello:\n\nCodicefile <- file.path(\"code\", \"simpleregstd.stan\")\nmod <- cmdstan_model(file)\n\n\nAdattiamo il modello ai dati:\n\nCodicefit <- mod$sample(\n  data = data_list,\n  iter_sampling = 4000L,\n  iter_warmup = 2000L,\n  seed = SEED,\n  chains = 4L,\n  refresh = 0\n)\n\n\nCreiamo un grafico con i valori predetti dal modello lineare:\n\nCodicestanfit <- rstan::read_stan_csv(fit$output_files())\nposterior <- extract(stanfit)\n\n\n\nCodicetibble(\n  kid_score = df$kid_score,\n  mom_hs = df$mom_hs\n) %>% \n  ggplot(aes(mom_hs, kid_score)) + \n  geom_point() + \n  geom_abline(intercept = mean(posterior$alpha), \n              slope = mean(posterior$beta)) +\n  labs(\n    y = \"Quoziente di intelligenza del bambino\",\n    x = \"Diploma di istruzione secondaria di secondo grado della madre\\n(0 = no; 1 = sì)\"\n  ) + \n  scale_x_continuous(breaks=c(0, 1))\n\n\n\n\n\n\n\nLe stime a posteriori dei parametri si ottengono con:\n\nCodicefit$summary(c(\"alpha\", \"beta\", \"sigma\", \"cohen_d\"))\n#> # A tibble: 4 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 alpha    77.6   77.5   2.08  2.06  74.1   81.0    1.00   16538.   12192.\n#> 2 beta     11.8   11.7   2.35  2.34   7.88  15.6    1.00   16718.   12319.\n#> 3 sigma    19.9   19.9   0.676 0.671 18.8   21.0    1.00   15949.   10908.\n#> 4 cohen_d   0.592  0.591 0.120 0.119  0.393  0.788  1.00   16771.   12647.\n\n\nI risultati confermano ciò che ci aspettavamo:\n\nil coefficiente \\(\\texttt{alpha} = 77.56\\) corrisponde alla media del gruppo codificato con \\(x = 0\\), ovvero la media dei punteggi PIAT per i bambini la cui madre non ha completato la scuola media superiore;\nil coefficiente \\(\\texttt{beta} = 11.76\\) corrisponde alla differenza tra le medie dei due gruppi, ovvero 89.32 - 77.55 = 11.77 (con piccoli errori di approssimazione).\n\nLa seguente chiamata ritorna l’intervallo di credibilità al 95% per tutti i parametri del modello:\n\nCodicerstantools::posterior_interval(\n  as.matrix(stanfit), prob = 0.95\n)\n#>                    2.5%         97.5%\n#> alpha_std   -0.09401587    0.09248375\n#> beta_std     0.14360650    0.32886135\n#> sigma_std    0.91337438    1.04372000\n#> alpha       73.43237000   81.62094500\n#> beta         7.13510525   16.33961500\n#> sigma       18.64258750   21.30290500\n#> cohen_d      0.35667085    0.82770125\n#> lp__      -208.90605000 -204.32400000\n\n\nPossiamo dunque concludere che i bambini la cui madre ha completato la scuola superiore ottengono in media circa 12 punti in più rispetto ai bambini la cui madre non ha completato la scuola superiore. L’intervallo di credibilità al 95% ci dice che possiamo essere sicuri al 95% che tale differenza sia di almeno 7 punti e possa arrivare fino a ben 16 punti. Per riassumere, possiamo concludere, con un grado di certezza soggettiva del 95%, che c’è un’associazione positiva tra il livello di scolarità della madre e l’intelligenza del bambino: le madri che hanno livello di istruzione più alto della media tendo ad avere bambini il cui QI è anch’esso più alto della media."
  },
  {
    "objectID": "055_reglin5.html#la-dimensione-delleffetto",
    "href": "055_reglin5.html#la-dimensione-delleffetto",
    "title": "29  Confronto tra due gruppi indipendenti",
    "section": "\n29.2 La dimensione dell’effetto",
    "text": "29.2 La dimensione dell’effetto\nNel caso di due gruppi indipendenti, la dimensione dell’effetto si può stimare con la statistica \\(d\\) di Cohen:\n\\[\nd={\\frac {{\\bar {y}}_{1}-{\\bar {y}}_{2}}{s}}.\n\\]\nNel caso presente, la differenza \\({\\bar {y}}_{1}-{\\bar {y}}_{2}\\) corrisponde a al parametro \\(\\beta\\) del modello lineare. Inoltre, una stima della deviazione starndard comune dei due gruppi è fornita dalla deviazione standard della regressione, ovvero dal parametro \\(\\sigma\\). Nel blocco generated quantities del modello Stan ho calcolato cohen_d = beta / sigma. Ciò significa che Stan calcolerà la distribuzione a posteriori del parametro cohen_d. Possiamo dunque riassumere la distribuzione a posteriori di cohen_d con un qualche indice di tendenza centrale (che sarà la nostra stima della dimensione dell’effetto) e calcolare l’intervallo di credibilità, per esempio al 95%. Questi risultati si ottengono con l’istruzione riportata di seguito:\n\nCodiceposterior::summarise_draws(\n  stanfit,\n  ~ quantile(.x, probs = c(0.025, 0.5, 0.975))\n)\n#> # A tibble: 8 × 4\n#>   variable     `2.5%`       `50%`   `97.5%`\n#>   <chr>         <dbl>       <dbl>     <dbl>\n#> 1 alpha_std   -0.0940   -0.000366    0.0925\n#> 2 beta_std     0.144     0.236       0.329 \n#> 3 sigma_std    0.913     0.974       1.04  \n#> 4 alpha       73.4      77.5        81.6   \n#> 5 beta         7.14     11.7        16.3   \n#> 6 sigma       18.6      19.9        21.3   \n#> 7 cohen_d      0.357     0.591       0.828 \n#> 8 lp__      -209.     -205.       -204.\n\n\nI risultati dell’analisi bayesiana coincidono con quelli che si ottengono utilizzando la formula del \\(d\\) di Cohen con le medie dei due gruppi e una stima della varianza pooled. Il calcolo della statistica \\(d\\) di Cohen è fornita, ad esempio, dal pacchetto effectsize:\n\nCodicelibrary(\"effectsize\")\n(d <- cohens_d(kid_score ~ mom_hs, data = df))\n#> Cohen's d |         95% CI\n#> --------------------------\n#> -0.59     | [-0.83, -0.36]\n#> \n#> - Estimated using pooled SD.\n\n\nIl fatto che l’output abbia un segno negativo dipende dal fatto che è stata sottratta la media maggiore dalla media minore (in altri termini, dobbiamo guardare il risultato in valore assoluto).\nIn conclusione, il valore \\(d\\) di Cohen di entità “media” [\\(d\\) > 0.5; Sawilowsky (2009)] può essere interpretato dicendo che la scolarità delle madri ha un’influenza non trascurabile sul QI dei bambini."
  },
  {
    "objectID": "055_reglin5.html#commenti-e-considerazioni-finali",
    "href": "055_reglin5.html#commenti-e-considerazioni-finali",
    "title": "29  Confronto tra due gruppi indipendenti",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nLa dimensione dell’effetto formulata nei termini dell’indice \\(d\\) di Cohen fornisce un indice che non dipende dall’unità di misura delle variabili, ovvero è una differenza media standardizzata. L’interpretazione di \\(d\\) è semplice: la scala di \\(d\\) è la deviazione standard. Se, per esempio, \\(d = 0.5\\), allora la media di un primo gruppo è mezza deviazione standard più grande della media del secondo gruppo. In questo Capitolo abbiamo visto come \\(d\\) possa essere calcolato mediante un modello lineare bayesiano implementato in linguaggio Stan.\n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\n\n\nSawilowsky, S. S. (2009). New effect size rules of thumb. Journal of Modern Applied Statistical Methods, 8(2), 26."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science per psicologi",
    "section": "",
    "text": "🇺🇦"
  },
  {
    "objectID": "index.html#benvenuti",
    "href": "index.html#benvenuti",
    "title": "Data Science per psicologi",
    "section": "Benvenuti",
    "text": "Benvenuti\nQuesto è il sito web per “Data Science per psicologi”. Viene qui presentato il materiale delle lezioni dell’insegnamento di Psicometria B000286 (A.A. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. Lo scopo di questo insegnamento è quello di fornire agli studenti un’introduzione all’analisi dei dati psicologici. Le conoscenze/competenze che verranno sviluppate in questo insegnamento sono quelle della Data Science applicata alla psicologia, ovvero, un insieme di conoscenze/competenze che si pongono all’intersezione tra psicologia, statistica e informatica."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Data Science per psicologi",
    "section": "License",
    "text": "License\nThis book was created by Corrado Caudek and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Parte 1: Nozioni di base",
    "section": "",
    "text": "In questa prima parte affronteremo i temi della misurazione e delll’analisi descrittiva dei dati. Nel capitolo Capitolo 1 verranno presentati i concetti chiave della Data Science. Nel capitolo Capitolo 2 sarà affrontato il tema della misurazione. Nel capitolo Capitolo 3 verranno introdotti i concetti che vengono utilizzati per descrivere le distribuzioni dei dati. Verranno poi introdotti gli indici di tendenza centrale e di dispersione nel capitolo Capitolo 4. Infine, nel capitolo Capitolo 5 verrà affrontato il problema di descrivere le relazioni tra variabili.\nPrima di affrontare tali temi sarà necessario introdurre il linguaggio di programmazione statistica R (un’introduzione a R è fornita in Appendice)."
  },
  {
    "objectID": "a10_markov_chains.html",
    "href": "a10_markov_chains.html",
    "title": "Appendix E — Le catene di Markov",
    "section": "",
    "text": "Per introdurre il concetto di catena di Markov, supponiamo che una persona esegua una passeggiata casuale sulla retta dei numeri naturali considerando solo i valori 1, 2, 3, 4, 5, 6.1 Se la persona è collocata su un valore interno dei valori possibili (ovvero, 2, 3, 4 o 5), nel passo successivo è altrettanto probabile che rimanga su quel numero o si sposti su un numero adiacente. Se si muove, è ugualmente probabile che si muova a sinistra o a destra. Se la persona si trova su uno dei valori estremi (ovvero, 1 o 6), nel passo successivo è altrettanto probabile che rimanga rimanga su quel numero o si sposti nella posizione adiacente.\nQuesto è un esempio di una catena di Markov discreta. Una catena di Markov descrive il movimento probabilistico tra un numero di stati. Nell’esempio ci sono sei possibili stati, da 1 a 6, i quali corrispondono alle possibili posizioni della passeggiata casuale. Data la sua posizione corrente, la persona si sposterà nelle altre posizioni possibili con delle specifiche probabilità. La probabilità che si sposti in un’altra posizione dipende solo dalla sua posizione attuale e non dalle posizioni visitate in precedenza.\nÈ possibile descrivere il movimento tra gli stati nei termini delle cosiddette probabilità di transizione, ovvero le probabilità di movimento tra tutti i possibili stati in un unico passaggio di una catena di Markov. Le probabilità di transizione sono riassunte in una matrice di transizione \\(P\\):\nLa prima riga della matrice di transizione \\(P\\) fornisce le probabilità di passare a ciascuno degli stati da 1 a 6 in un unico passaggio a partire dalla posizione 1; la seconda riga fornisce le probabilità di transizione in un unico passaggio dalla posizione 2 e così via. Per esempio, il valore \\(P[1, 1]\\) ci dice che, se la persona è nello stato 1, avrà una probabilità di 0.5 di rimanere in quello stato; \\(P[1, 2]\\) ci dice che c’è una probabilità di 0.5 di passare dallo stato 1 allo stato 2. Gli altri elementi della prima riga sono 0 perché, in un unico passaggio, non è possibile passare dallo stato 1 agli stati 3, 4, 5 e 6. Il valore \\(P[2, 1]\\) ci dice che, se la persona è nello stato 1 (seconda riga), avrà una probabilità di 0.25 di passare allo stato 1; avra una probabilità di 0.5 di rimanere in quello stato, \\(P[2, 2]\\); e avrà una probabilità di 0.25 di passare allo stato 3, \\(P[2, 3]\\); eccetera.\nSi notino alcune importanti proprietà di questa particolare catena di Markov.\nUn’importante proprietà di una catena di Markov irriducibile e aperiodica è che il passaggio ad uno stato del sistema dipende unicamente dallo stato immediatamente precedente e non dal come si è giunti a tale stato (dalla storia). Per questo motivo si dice che un processo markoviano è senza memoria. Tale “assenza di memoria” può essere interpretata come la proprietà mediante cui è possibile ottenere un insieme di campioni casuali da una distribuzione di interesse. Nel caso dell’inferenza bayesiana, la distribuzione di interesse è la distribuzione a posteriori, \\(p(\\theta \\mid y)\\). Le catene di Markov consentono di stimare i valori di aspettazione di variabili rispetto alla distribuzione a posteriori.\nLa matrice di transizione che si ottiene dopo un enorme numero di passi di una passeggiata casuale markoviana si chiama distribuzione stazionaria. Se una catena di Markov è irriducibile e aperiodica, allora ha un’unica distribuzione stazionaria \\(w\\). La distribuzione limite di una tale catena di Markov, quando il numero di passi tende all’infinito, è uguale alla distribuzione stazionaria \\(w\\)."
  },
  {
    "objectID": "a10_markov_chains.html#simulare-una-catena-di-markov",
    "href": "a10_markov_chains.html#simulare-una-catena-di-markov",
    "title": "Appendix E — Le catene di Markov",
    "section": "\nE.1 Simulare una catena di Markov",
    "text": "E.1 Simulare una catena di Markov\nUn metodo per dimostrare l’esistenza della distribuzione stazionaria di una catena di Markov è quello di eseguire un esperimento di simulazione. Iniziamo una passeggiata casuale partendo da un particolare stato, diciamo la posizione 3, e quindi simuliamo molti passaggi della catena di Markov usando la matrice di transizione \\(P\\). Al crescere del numero di passi della catena, le frequenze relative che descrivono il passaggio a ciascuno dei sei possibili nodi della catena approssimano sempre meglio la distribuzione stazionaria \\(w\\).\nSenza entrare nei dettagli della simulazione, la Figura E.1 mostra i risultati ottenuti in 10,000 passi di una passeggiata casuale markoviana. Si noti che, all’aumentare del numero di iterazioni, le frequenze relative approssimano sempre meglio le probabilità nella distribuzione stazionaria \\(w = (0.1, 0.2, 0.2, 0.2, 0.2, 0.1)\\).\n\nCodiceset.seed(123)\ns <- vector(\"numeric\", 10000)\ns[1] <- 3\nfor (j in 2:10000) {\n  s[j] <- sample(1:6, size = 1, prob = P[s[j - 1], ])\n}\nS <- data.frame(\n  Iterazione = 1:10000,\n  Location = s\n)\n\nS %>%\n  mutate(\n    L1 = (Location == 1),\n    L2 = (Location == 2),\n    L3 = (Location == 3),\n    L4 = (Location == 4),\n    L5 = (Location == 5),\n    L6 = (Location == 6)\n  ) %>%\n  mutate(\n    Proporzione_1 = cumsum(L1) / Iterazione,\n    Proporzione_2 = cumsum(L2) / Iterazione,\n    Proporzione_3 = cumsum(L3) / Iterazione,\n    Proporzione_4 = cumsum(L4) / Iterazione,\n    Proporzione_5 = cumsum(L5) / Iterazione,\n    Proporzione_6 = cumsum(L6) / Iterazione\n  ) %>%\n  dplyr::select(\n    Iterazione, Proporzione_1, Proporzione_2, Proporzione_3,\n    Proporzione_4, Proporzione_5, Proporzione_6\n  ) -> S1\n\ngather(S1, Outcome, Probability, -Iterazione) -> S2\n\nggplot(S2, aes(Iterazione, Probability)) +\n  geom_line() +\n  facet_wrap(~Outcome, ncol = 3) +\n  ylim(0, .4) +\n  ylab(\"Frequenza relativa\") +\n  # theme(text=element_text(size=14))  +\n  scale_x_continuous(breaks = c(0, 3000, 6000, 9000))\n\n\n\nFigura E.1: Frequenze relative degli stati da 1 a 6 in funzione del numero di iterazioni per la simulazione di una catena di Markov.\n\n\n\n\nIl metodo di campionamento utilizzato dagli algoritmi MCMC consente di creare una catena di Markov irriducibile e aperiodica, la cui distribuzione stazionaria equivale alla distribuzione a posteriori \\(p(\\theta \\mid y)\\)."
  },
  {
    "objectID": "051_reglin1.html#descrivere-le-relazioni-tra-variabili",
    "href": "051_reglin1.html#descrivere-le-relazioni-tra-variabili",
    "title": "25  Introduzione",
    "section": "\n25.1 Descrivere le relazioni tra variabili",
    "text": "25.1 Descrivere le relazioni tra variabili\nIl fine primario dei ricercatori è quello di scoprire associazioni tra variabili e di fare confronti fra condizioni sperimentali. Nel caso della psicologia, il ricercatore vuole descrivere le relazioni tra i costrutti psicologici e le relazioni tra fenomeni psicologici e non psicologici (sociali, economici, storici, …). Abbiamo già visto come la correlazione di Pearson sia uno strumento adatto a descrivere le relazioni tra variabili. Infatti, la correlazione ci informa sulla direzione e sull’intensità della relazione lineare tra due variabili. Tuttavia, la correlazione non è sufficiente, in quanto il ricercatore ha a disposizione solo i dati di un campione, mentre vorrebbe descrivere la relazione tra le variabili nella popolazione. A causa della variabilità campionaria, le proprietà dei campioni sono necessariamente diverse da quelle della popolazione: ciò che si può osservare nella popolazione potrebbe non emergere nel campione e, al contrario, il campione può manifestare caratteristiche che non sono presenti nella popolazione. È dunque necessario chiarire, dal punto di vista statistico, il legame che intercorre tra le proprietà del campione e le proprietà della popolazione da cui esso è stato estratto. Questo è l’obiettivo del modello di regressione lineare. Tale modello utilizza la funzione matematica più semplice per descrivere la relazione fra due variabili, ovvero la funzione lineare. In questo Capitolo vedremo come il modello di regressione lineare possa essere usato per fare inferenza sulla relazione tra variabili. Inizieremo a descrivere le proprietà geometriche della funzione lineare per poi utilizzare questa semplice funzione per costruire un modello statistico secondo un approccio bayesiano."
  },
  {
    "objectID": "052_reglin2.html",
    "href": "052_reglin2.html",
    "title": "26  Regressione lineare bivariata",
    "section": "",
    "text": "In questo Capitolo verrà discusso il modello di regressione bivariata, ovvero il modello che, mediante una relazione lineare, predice una variabile continua \\(y\\) a partire da un unico predittore continuo \\(x\\). Ciò corrisponde ad adattare ai dati (\\(x_i, y_i\\)) la retta di regressione \\(y_i = a + bx_i + e_i\\), con \\(i=1, \\dots, n\\). Usando dei dati reali, vedremo come stimare i coefficienti di regressione \\(a\\) e \\(b\\), e come essi possono essere interpretati. Vedremo anche come si può descrivere la bontà di adattamento del modello ai dati.\nNell’esempio che discuteremo in questo Capitolo verranno usati i dati kidiq:\nIn questo esercizio sulla regressione lineare considererò la relazione tra l’intelligenza del bambino (kid_score) e l’intelligenza della madre (mom_iq). Mi chiederò se l’intelligenza della madre sia in grado di predire l’intelligenza del bambino e in che misura lo faccia.\nLeggo i dati in \\(\\mathsf{R}\\).\nI dati rappresentati in un diagramma a dispersione suggeriscono che, in questo campione, sembra effettivamente esserci un’associazione positiva tra l’intelligenza del bambino (kid_score) e l’intelligenza della madre (mom_iq).\nLa regressione lineare descrive questa associazione mediante una retta.\nCi sono infinite rette che, in linea di principio, possono essere usate per “approssimare” la nube di punti nel diagramma a dispersione. È dunque necessario introdurre dei vincoli per selezionare una di queste possibili rette. Un vincolo che viene introdotto dal modello di regressione è quello di costringere la retta a passare per il punto \\((\\bar{x}, \\bar{y})\\).\nUna retta che passa per il punto \\((\\bar{x}, \\bar{y})\\) ha delle desiderabili proprietà statistiche che verranno descritte in seguito.\nIl campione è costituito da \\(n\\) coppie di osservazioni (\\(x, y\\)). Per ciascuna coppia di valori \\(x_i, y_i\\), il modello di regressione si aspetta che il valore \\(y_i\\) sia associato al corrispondente valore \\(x_i\\) come indicato dalla seguente equazione:\n\\[\ny_i = a + b x_i + e_i\n\\]\nI valori \\(y_i\\) corrispondono, nell’esempio che stiamo discutendo, alla variabile kid_score. I primi 10 valori della variabile \\(y\\) sono i seguenti:\nPer fare riferimento a ciascuna osservazione usiamo l’indice \\(i\\). Quindi, ad esempio, \\(y_3\\) è uguale a\nNel caso presente, la variabile \\(x\\) è mom_iq. I primi 10 valori di \\(x\\) sono\nIn maniera corrispondente alla \\(y\\), uso un indice per fare riferimento ai singoli valori della variabile. Ad esempio, \\(x_3\\) è\nL’equazione precedente ci dice che ciascun valore \\(y\\) è dato dalla somma di due componenti: una componente deterministica e una componente aleatoria. Consideriamo il primo valore \\(y\\) del campione. Per esso, il modello di regressione ci dice che\n\\[\ny_1 = a + b x_1 + e_1,\n\\]\nladdove \\(a + b x_1\\) è la componente deterministica, denotata con \\(\\hat{y}\\), e \\(e_1\\) è la componente aleatoria.\nLa componente deterministica è la componente di ciascun valore \\(y_i\\) che è possibile prevedere conoscendo \\(x_i\\). Tuttavia, non è possibile prevedere perfettamente i valori \\(y\\) – ciò si verificherebbe soltanto se tutti punti del diagramma a dispersione fossero disposti su una retta. Ma non lo sono mai nella pratica concreta: la retta è solo un’approssimazione della relazione (lineare) tra \\(x\\) e \\(y\\). Pertanto, conoscendo \\(x_i\\) possiamo solo prevedere una “componente” del corrispondente valore \\(y_i\\).\nCosa significa che possiamo prevedere una componente di ciascuna osservazione \\(y_i\\)? Significa che il valore osservato \\(y_i\\) sarà diverso dal valore \\(\\hat{y}_i\\) previsto dal modello. Ciascun valore \\(y_i\\) sarà dunque dato dalla seguente somma: \\(y_i = \\hat{y}_i + e_i\\), laddove \\(e_i\\), detto “residuo” è la componente di \\(y_i\\) non predicibile dal modello lineare.\nCi possiamo dunque porre due domande:\nRispondere a tali due domanda definisce i primi due obiettivi del modello statistico della regressione lineare. Il terzo obiettivo è quello dell’inferenza, ovvero quello di capire che relazioni ci sono tra la relazione tra \\(x\\) e \\(y\\) osservata nel campione e la la relazione tra le due variabili nella popolazione."
  },
  {
    "objectID": "052_reglin2.html#stima-dei-coefficienti-di-regressione",
    "href": "052_reglin2.html#stima-dei-coefficienti-di-regressione",
    "title": "26  Regressione lineare bivariata",
    "section": "\n26.1 Stima dei coefficienti di regressione",
    "text": "26.1 Stima dei coefficienti di regressione\nIniziamo con il primo obiettivo, ovvero quello di trovare i coefficienti \\(a\\) e \\(b\\) che consentono di predire una componente di ciascuna osservazione \\(y\\) conoscendo \\(x\\). Quindi, nel caso presente, ci chiediamo quanto segue. Il primo bambino del campione ha un QI uguale a 65. Sua madre ha un QI di 121.12. Qual è la predizione migliore del QI del bambino che possiamo ottenere conoscendo il QI della madre?\nÈ chiaro, guardando i dati del campione, che non c’è una corrispondenza perfetta tra QI della madre e QI del bambino, tutt’altro! Infatti, se guardiamo il diagramma di dispersione ci rendiamo conto che i punti sono piuttosto lontani dalla retta che abbiamo sovrapposto alla nube di punti \\(x_i, y_i\\). Tuttavia, il diagramma di dispersione ci suggerisce che, al di là del rumore, c’è comunque una relazione tra le due variabili. Il nostro obiettivo è trovare un metodo quantitativo per descrivere una tale relazione.\nAbbiamo detto che è possibile prevedere una componente di \\(y_i\\) conoscendo \\(x_i\\). La componente \\(y_i\\) predicibile da \\(x_i\\) viene denotata da \\(\\hat{y}_i\\) e, nei termini del modello di regressione lineare è uguale a\n\\[\n\\hat{y}_i = a_i + bx_i.\n\\]\nL’equazione precedente è un’equazione lineare e, dal punto di vista geometrico, corrisponde ad una retta. Ci sono infinite equazioni che, in linea di principio, possiamo usare per descrivere la relazione tra \\(x\\) e \\(y\\). Abbiamo scelto la relazione lineare perché è la più semplice. Se guardiamo il diagramma di dispersione, infatti, non ci sono ragioni per descrivere la relazione tra il QI del bambino e il QI della madre con qualche curva, anziché con una retta. In altri campioni, una curva potrebbe essere più sensata di una retta, quale descrizione della relazione media tra \\(x\\) e \\(y\\), ma non nel caso presente. Ricordiamo il principio del rasoio di Occam (ovvero, il principio che sta alla base del pensiero scientifico moderno): se un modello semplice funziona, non c’è ragione di usare un modello più complesso.\nDunque, abbiamo capito che vogliamo descrivere la relazione media tra \\(x\\) e \\(y\\) con una retta, ovvero, mediante l’equazione lineare\n\\[\n\\hat{y}_i = a + b x_i.\n\\]\nL’equazione precedente ci dice che il modello lineare \\(a + b x_i\\) non è in grado di prevedere completamente i valori \\(y_i\\). Questo, in generale, non è mai possibile (ovvero, è possibile solo in un caso specifico che, nella realtà empirica, non si verifica mai). L’equazione precedente ci dice che possiamo prevedere solo una componente di ciascuna osservazione \\(y_i\\), ovvero quella componente che abbiamo denotato con \\(\\hat{y}_i\\). La componente che non possiamo prevedere con l’equazione \\(a + b x_i\\) viene detta residuo e si denota con \\(e_i\\):\n\\[\ne_i = y_i - \\hat{y}_i = y_i - (a + bx_i).\n\\]\nDal punto di vista geometrico, la componente erratica del modello, \\(e_i\\), corrisponde alla distanza verticale tra ciascun punto del diagramma a dispersione e la retta di regressione \\(a + bx\\). Diciamo che scomponiamo il valore di ciascuna osservazione \\(y_i\\) in due componenti nel senso che\n\\[\ny_i = \\hat{y}_i + e_i = (a + bx_i) + e_i.\n\\]\nIl primo obiettivo del modello di regressione è quello di trovare i coefficienti dell’equazione\n\\[\na + b x_i\n\\]\nche consente di trovare \\(\\hat{y}_i\\) conoscendo \\(x_i\\). Questi due coefficienti sono detti coefficienti di regressione.\nPer trovare i coefficienti di regressione dobbiamo introdurre dei vincoli per limitare lo spazio delle possibili soluzioni. Il primo di tali vincoli è stato introdotto in precedenza: vogliamo che la retta \\(\\hat{y}_i = a + b x_i\\) passi per il punto \\((\\bar{x}, \\bar{y})\\). Il punto \\((\\bar{x}, \\bar{y})\\) corrisponde al baricentro del diagramma a dispersione.\nCi sono però infinite rette che passano per i punto \\((\\bar{x}, \\bar{y})\\). Tutte queste rette soddisfano la seguente proprietà:\n$$\n_{i=1}^n e_i = 0,\n$$\novvero, fanno in modo che la somma dei residui (positivi, per i punti che si trovano al di sopra della retta di regressione, negativi, per punti che si trovano al di sotto della retta di regressione) sia uguale a zero.\nQuesto significa che non possiamo selezionare una tra le infinite rette che passano per il punto \\((\\bar{x}, \\bar{y})\\) usando il criterio che ci porta a scegliere la retta che rende la più piccola possibile (ovvero, minimizza) la somma dei residui. Infatti, tutte le rette passanti per il punto \\((\\bar{x}, \\bar{y})\\) soddisfano questo requisito (rendono uguale a zero la somma dei residui). Dunque, dobbiamo trovare qualche altri criterio per scegliere una tra le infinite rette che passano per il punto \\((\\bar{x}, \\bar{y})\\).\nIl criterio che viene normalmente scelto è quello di minimizzare la somma dei quadrati dei residui \\((y_i - \\hat{y}_i)^2\\). In altri termini, vogliamo trovare i coefficienti \\(a\\) e \\(b\\) tali per cui la quantità\n$$\n_{i=1}^{n}{(y_i - (a + b x_i))^2}\n$$\nassume il suo valore minimo. I coefficienti \\(a\\) e \\(b\\) che soddisfano questa proprietà si chiamano coefficienti dei minimi quadrati.\nQuesto problema ha una soluzione analitica. La soluzione analitica si trova riconoscendo il fatto che l’equazione precedente definisce una superficie e il problema diventa quello di trovare il punto di minimo di questa superficie. Per trovare la soluzione ci si deve rendere conto che il punto cercato è quello per cui il piano tangente alla superficie (nelle due direzioni \\(a\\) e \\(b\\)) è piatto (le tangenti nelle due direzioni sono uguali a zero). Rendere uguale a zero la tangente ad una curva significa porre uguali a zero la derivata della curva. Nel caso presente, abbiamo una superficie, dunque due tangenti ortogonali e quindi abbiamo il problema di rendere uguali a zero le derivate parziali rispetto ad \\(a\\) e \\(b\\). Così facendo si definisce un sistema di equazioni lineari con due incognite, \\(a\\) e \\(b\\). La soluzione di tali equazioni, che si chiamano equazioni normali, è la seguente:\n$$\na = {y} - b {x},\n$$\n$$\nb = .\n$$\nLe due precedenti equazioni corrispondono alla stima dei minimi quadrati dei coefficienti di regressione della retta che minimizza la somma dei quadrati dei residui.\nNel caso dell’esempio presente, tali coefficienti sono uguali a:\n\nCodiceb <- cov(kidiq$kid_score, kidiq$mom_iq) / var(kidiq$mom_iq)\nb\n#> [1] 0.6099746\n\n\n\nCodicea <- mean(kidiq$kid_score) - b * mean(kidiq$mom_iq)\na\n#> [1] 25.79978\n\n\nIn \\(\\mathsf{R}\\) li possiamo facilmente trovare con la seguente funzione:\n\nCodicefm <- lm(kid_score ~ mom_iq, data = kidiq)\ncoef(fm)\n#> (Intercept)      mom_iq \n#>  25.7997778   0.6099746\n\n\nIn precedenza abbiamo soltanto accennato al problema di come si possono trovano i coefficienti dei minimi quadrati; ritorneremo su questo punto in seguito, con una simulazione. Per ora, chiediamoci cosa significano i due coefficienti che abbiamo appena trovato.\nIl coefficiente \\(a\\) si chiama intercetta. L’intercetta, all’interno del diagramma a dispersione, specifica il punto in cui la retta di regressione interseca l’asse \\(y\\) del sistema di assi cartesiani.\nNel caso presente questo valore non è di alcun interesse, perché corrisponde al valore della retta di regressione quando \\(x = 0\\), ovvero quando l’intelligenza della madre è uguale a 0. Vedremo in seguito come, trasformando i dati, è possibile assegnare al coefficiente \\(a\\) un’interpretazione più utile. Per ora mi limito a fornire l’interpretazione del coefficiente.\nPassando a \\(b\\), possiamo dire che questo secondo coefficiente va sotto il nome di pendenza della retta di regressione. Ovvero ci dice di quanto aumenta (se \\(b\\) è positivo) o diminuisce (se \\(b\\) è negativo) la retta di regressione in corrispondenza di un aumento di 1 punto della variabile \\(x\\).\nNel caso presente, il coefficiente \\(b\\) ci dice che, se il QI delle madri aumenta di 1 punto, il QI dei bambini aumenta in media di 0.61 punti.\nÈ importante capire cosa significa che, in base ai risultati della regressione, \\(y\\) aumenta in media di \\(b\\) punti per ciascun aumento unitario di \\(x\\).\nIl modello statistico di regressione ipotizza che, per ciascun valore osservato \\(x\\) (per esempio, il valore del QI della prima madre del campione, ovvero \\(x = 121.11753\\)) ci sia una distribuzione di valori \\(y\\) nella popolazione, di cui solo uno è stato osservato nel campione. Possiamo facilmente capire che, se consideriamo tutte le madri con QI di 121.12, il punteggio del QI dei loro figli non sia costante, ma assuma tanti valori possibili. Questa distribuzione di valori possibili si chiama distribuzione \\(y\\) condizionata a \\(x\\), ovvero \\(p(y \\mid x_i)\\).\nIl modello statistico della regressione lineare non può in alcun modo prevedere il valore assunto da ciascuna delle possibili osservazioni che fanno parte della distribuzione \\(p(y \\mid x_i)\\). Il modello della regressione lineare ha un obiettivo più limitato, ovvero si propone di prevedere le medie delle distribuzioni \\(p(y \\mid x_i)\\) conoscendo i valori \\(x\\).\nDunque, quando il coefficiente \\(b\\) è uguale a 0.61, questo significa che il modello di regressione predice che la medie della distribuzione condizionata \\(p(y \\mid x_i)\\) aumenta di 0.61 punti se la variabile \\(x\\) (QI delle madri) aumenta di un punto. Questo significa che il modello di regressione non fa una predizione sul punteggio di ciascun valore \\(y_i\\) (in funzione di \\(x\\)), ma solo della media delle distribuzioni condizionate \\(p(y \\mid x_i)\\) di cui il valore osservato \\(y_i\\) è una realizzazione casuale.\nPossiamo dire la stessa cosa con parole diverse dicendo che il modello di regressione fa delle predizioni sulla componente deterministica di ciascuna osservazione. È più semplice capire questo aspetto se rappresentiamo in maniera grafica la componente “deterministica” \\(\\hat{y}_i = a + b x_i\\) predetta dal modello di regressione.\n\nCodicekidiq$yhat <- fm$fitted.values\nkidiq %>% \n  ggplot(aes(x = mom_iq, y = yhat)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    aes(x=mean(mom_iq), y=mean(kid_score)), \n    colour=\"red\", size = 4\n  )\n\n\n\n\n\n\n\nIl diagramma precedente presenta ciascun valore \\(\\hat{y}_i = a + b x_i\\) in funzione di \\(x_i\\). Si vede che i valori predetti dal modello di regressione sono i punti che stanno sulla retta di regressione.\nIn precedenza abbiamo detto che il residuo, ovvero la componente di ciascuna osservazione \\(y_i\\) che non viene predetta dal modello di regressione, corrisponde alla distanza verticale tra il valore \\(y_i\\) osservato e il valore \\(\\hat{y}_i\\) predetto dal modello di regressione:\n$$\ne_i = y_i - (a + b x_i).\n$$\nNel caso nella prima osservazione, ad esempio abbiamo:\n$$\ny_1 = (a + b x_1) + e_1\n$$\nAbbiamo\n\nCodicekidiq$kid_score[1]\n#> [1] 65\n\n\nDunque\n$$\ne_1 = (a + b x_1) - y_1\n$$\n\nCodicee_1 <- kidiq$kid_score[1] - (a + b * kidiq$mom_iq[1])\ne_1\n#> [1] -34.67839\n\n\nCiò significa che il valore osservato \\(y_1 = 65\\) viene scomposto dal modello di regressione in due componenti. La componente deterministica \\(\\hat{y}_1\\), predicibile da \\(x_1\\), è\n\nCodiceyhat_1 <- a + b * kidiq$mom_iq[1]\nyhat_1\n#> [1] 99.67839\n\n\nLa somma della componente deterministica e della componente erratica, ovviamente, riproduce il valore osservato.\n\nCodiceyhat_1 + e_1\n#> [1] 65\n\n\nSe sommiamo tutti i residui calcolati rispetto alla retta di regressione dei minimi quadrati otteniamo zero:\n\nCodicesum(fm$res)\n#> [1] 5.373479e-13\n\n\n\n26.1.1 Trasformazione dei dati\nIn generale, per variabili a livello di scala ad intervalli, non è possibile assegnare un’interpretazione utile all’intercetta del modello di regressione lineare. L’intercetta ci dice infatti qual è il valore atteso della \\(y\\) quando \\(x = 0\\). Ma, se la variabile \\(x\\) è misurata su scala ad intervalli, il valore \\(x = 0\\) è arbitrario e non corrisponde “all’assenza di intensità” della variabile \\(x\\). Un valore pari a 0 del QI della madre non vuol dire che l’intelligenza della madre sia nulla (un’affermazione, questa, che è difficile da capire), ma semplicemente che il punteggio del test usato per misurare il QI della madre assume valore 0 (qualcosa che, comunque, in pratica non succederà mai). Quindi è di poco interesse sapere qual è il valore medio del QI del bambino quando test usato per misurare il QI della madre ha valore 0. Per potere fornire all’intercetta del modello di regressione un’interpretazione più utile dobbiamo trasformare le osservazioni \\(x\\).\nEsprimiamo \\(x\\) come differenza dalla media. Chiamiamo questa nuova variabile \\(xd\\):\n\nCodicekidiq$xd <- kidiq$mom_iq - mean(kidiq$mom_iq)\n\n\nSe ora usiamo le coppie di osservazioni \\(xd_i, y_i\\), il diagramma a dispersione assume la forma seguente.\n\nCodicekidiq %>% \n  ggplot(aes(x = xd, y = kid_score)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    aes(x=mean(xd), y=mean(kid_score)), \n    colour=\"red\", size = 4\n  )\n\n\n\n\n\n\n\nQuello che abbiamo fatto è stato di traslare rigidamente la nube di punti sul piano cartesiano di una quantità pari alla distanza tra \\(\\bar{x}\\) e l’origine. Dunque, le relazioni spaziali tra i punti del diagramma a dispersione restano immutate. Di conseguenza, la pendenza della retta di regressione calcolata sui dati trasformati è uguale a quella che si trova nel caso dei dati non trasformati. Ciò che cambia è il valore dell’intercetta.\n\nCodicefm1 <- lm(kid_score ~ xd, data = kidiq)\ncoef(fm1)\n#> (Intercept)          xd \n#>  86.7972350   0.6099746\n\n\nL’intercetta corrisponde al punto sull’asse \\(y\\) dove la retta di regressione interseca l’ordinata. Ma, nel caso dei dati trasformati, dato che abbiamo traslato i punti di una quantità pari a \\(x - \\bar{x}\\), il valore \\(xd = 0\\) corrisponde a \\(x = \\bar{x}\\) nel caso dei dati grezzi. Dunque, per i dati trasformati \\(xd_i, y_i\\), l’intercetta corrisponderà al valore atteso della \\(y\\) in corrispondenza del valore medio della variabile \\(x\\) sulla scala dei dati non trasformati (ovvero \\(\\bar{x}\\)). In altre parole, l’intercetta del modello di regressione lineare calcolata sui dati trasformati corrisponde al QI medio dei bambini in corrispondenza del QI medio delle madri.\n\n26.1.2 Il metodo dei minimi quadrati\nOra che abbiamo visto come interpretare il coefficienti di regressione, chiediamoci come vengono calcolati. La procedura generale è stata brevemente descritta in precedenza. Vediamo ora come si giunge alla conclusione descritta sopra usando una simulazione.\nIl problema è di trovare i valori \\(a\\) e \\(b\\) tali per cui la quantità \\(\\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}\\) assume il valore minore possibile. Questo è un problema di minimizzazione rispetto a due parametri. Per dare un’idea di come si fa, semplifichiamo il problema e supponiamo che uno dei due parametri sia noto, ad esempio \\(a\\), così ci resta una sola incognita.\nCredo una griglia di valori b_grid possibili, ad esempio:\n\nCodicenrep <- 1e5\nb_grid <- seq(0, 1, length.out = nrep)\n\n\nDefinisco una funzione che calcola la quantità \\(\\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}\\):\n\nCodicesse <- function(a, b, x, y) {\n  sum((y - (a + b * x))^2)\n}\n\n\nCalcolo la somma degli errori quadratici per ciascun possibile valore b_grid, fissando \\(a = 25.79978\\).\n\nCodicesse_res <- rep(NA, nrep)\nfor (i in 1:nrep) {\n  sse_res[i] <- sse(a = 25.79978, b = b_grid[i], x = kidiq$mom_iq, y = kidiq$kid_score)\n}\n\n\nEsaminiamo il risultato ottenuto.\n\nCodiceplot(\n  b_grid, sse_res, type = 'l'\n)\n\n\n\n\n\n\n\nIl risultato ottenuto con la simulazione\n\nCodiceb_grid[which.min(sse_res)]\n#> [1] 0.6099761\n\n\nriproduce quello ottenuto per via analitica:\n\nCodiceb\n#> [1] 0.6099746\n\n\nUna simulazione simile, ma computazionalmente più complessa, può essere usata per stimare simultaneamente entrambi i parametri. Ci siamo limitati qui ad una proof of concept del caso più semplice.\n\n26.1.3 L’errore standard della regressione\nIl secondo obiettivo del modello statistico di regressione lineare è quello di stabilire quanto sia grande la componente \\(y\\) predicibile da \\(x\\), per ciascuna osservazione.\nUn indice assoluto della bontà di adattamento è fornito dalla deviazione standard dei residui, \\(s_e\\), chiamata anche errore standard della stima. Uno stimatore non distorto della varianza dei residui nella popolazione è dato da\n$$\ns^2_e = e_i^2\n$$\ne quindi l’errore standard della stima sarà\n\\[\\begin{equation}\ns_e = \\sqrt{\\frac{1}{n-2}\\sum e_i^2}.\n\\end{equation}\\]\nSi noti che questa è la stessa formula della varianza (dato che la media dei residui è zero), tranne per il fatto che al denominatore abbiamo \\(n-2\\). Dato che, per calcolare \\(\\hat{y}\\) abbiamo usato due coefficienti (\\(a\\) e \\(b\\)), si dice che “abbiamo perso due gradi di libertà”.\nDato che \\(s_e\\) possiede la stessa unità di misura della variabile \\(y\\), l’errore standard della stima può essere considerato come una sorta di “residuo medio.” – usando la stessa interpretazione che diamo alla deviazione standard in generale.\nSi noti che la formula precedente non fornisce la “deviazione standard dei residui nel campione” (quella formula avrebbe \\(n\\) al denominatore). Invece, fornisce una stima della deviazione standard dei residui nella popolazione da cui il campione è stato estratto.\nVerifichiamo quanto detto con i dati a disposizione.\nI residui possono essere trovati nel modo seguente.\n\nCodicee <- kidiq$kid_score - (a + b * kidiq$mom_iq)\ne[1:10]\n#>  [1] -34.678390  17.691747 -11.217173  -3.461529  32.627697   6.382845\n#>  [7] -41.521041   3.864881  26.414387  11.208068\n\n\nOppure nel modo seguente.\n\nCodicefm$residuals[1:10]\n#>          1          2          3          4          5          6          7 \n#> -34.678390  17.691747 -11.217173  -3.461529  32.627697   6.382845 -41.521041 \n#>          8          9         10 \n#>   3.864881  26.414387  11.208068\n\n\nCalcolo il residuo medio, prendendo il valore assoluto.\n\nCodicemean(abs(e))\n#> [1] 14.4686\n\n\nL’errore standard della regressione è\n\nCodicesqrt(sum(e^2) / (length(e) - 2))\n#> [1] 18.26612\n\n\nI due numeri non sono uguali, ma possiamo dire che hanno lo stesso ordine di grandezza.\nSe usiamo la funzione lm() otteniamo lo stesso valore, chiamato Residual standard error.\n\nCodicesummary(fm)\n#> \n#> Call:\n#> lm(formula = kid_score ~ mom_iq, data = kidiq)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -56.753 -12.074   2.217  11.710  47.691 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 25.79978    5.91741    4.36 1.63e-05 ***\n#> mom_iq       0.60997    0.05852   10.42  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 18.27 on 432 degrees of freedom\n#> Multiple R-squared:  0.201,  Adjusted R-squared:  0.1991 \n#> F-statistic: 108.6 on 1 and 432 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "052_reglin2.html#indice-di-determinazione",
    "href": "052_reglin2.html#indice-di-determinazione",
    "title": "26  Regressione lineare bivariata",
    "section": "\n26.2 Indice di determinazione",
    "text": "26.2 Indice di determinazione\nUn importante risultato dei minimi quadrati riguarda la cosiddetta scomposizione della devianza mediante la quale si definisce l’indice di determinazione, il quale fornisce una misura relativa della bontà di adattamento del modello di regressione ai dati del campione. Per una generica osservazione \\(x_i, y_i\\), la variazione di \\(y_i\\) rispetto alla media \\(\\bar{y}\\) può essere descritta come la somma di due componenti: il residuo \\(e_i=y_i- \\hat{y}_i\\) e lo scarto di \\(\\hat{y}_i\\) rispetto alla media \\(\\bar{y}\\):\n$$\ny_i - {y} = (y_i- _i) + (_i - {y}) = e_i + (_i - {y}).\n$$\nSe consideriamo tutte le osservazioni, la devianza delle \\(y\\) può essere scomposta nel seguente modo:\n\\[\\begin{align}\n\\sum (y_i - \\bar{y})^2 &= \\sum \\left[ e_i + (\\hat{y}_i - \\bar{y})\n\\right]^2\n= \\sum e_i^2 + \\sum (\\hat{y}_i - \\bar{y})^2 + 2 \\sum e_i (\\hat{y}_i -\n\\bar{y}) \\notag\n\\end{align}\\]\nPer i vincoli imposti sul modello statistico di regressione, il doppio prodotto si annulla, infatti\n\\[\\begin{align}\n\\sum e_i (\\hat{y}_i - \\bar{y}) &= \\sum e_i \\hat{y}_i - \\bar{y}\\sum e_i = \\sum e_i (a + b x_i) \\notag \\\\\n&= a \\sum e_i + b \\sum e_i x_i = 0 \\notag\n\\end{align}\\]\nIl termine \\(b \\sum e_i x_i\\) è uguale a zero perché, come vedremo in seguito, i coefficienti di regressione vengono calcolati in modo tale da rendere nulla \\(\\mbox{Cov}(e, x)\\). Di conseguenza, il termine precedente deve essere nullo.\nPossiamo dunque concludere che la devianza totale (\\(\\mbox{dev}_T\\)) si scompone nella somma di devianza d’errore (o devianza non spiegata) (\\(\\mbox{dev}_E\\)) e devianza di regressione (o devianza spiegata) (\\(\\mbox{dev}_T\\)):\n\\[\\begin{align}\n\\underbrace{\\sum_{i=1}^n (y_i - \\bar{y})^2}_{\\tiny{\\text{Devianza\ntotale}}} &= \\underbrace{\\sum_{i=1}^n e_i^2}_{\\tiny{\\text{Devianza\ndi dispersione}}} + \\underbrace{\\sum_{i=1}^n  (\\hat{y}_i -\n\\bar{y})^2}_{\\tiny{\\text{Devianza di regressione}}} \\notag\n\\end{align}\\]\nLa devianza di regressione, \\(\\mbox{dev_R} \\triangleq \\mbox{dev_T} - \\mbox{dev_E}\\), indica dunque la riduzione degli errori al quadrato che è imputabile alla regressione lineare. Il rapporto \\(\\mbox{dev_R}/\\mbox{dev_T}\\), detto indice di determinazione, esprime tale riduzione degli errori in termini proporzionali e definisce il coefficiente di correlazione al quadrato:\n\\[\\begin{equation}\nR^2 \\triangleq \\frac{\\mbox{dev_R}}{\\mbox{dev_T}} = 1 - \\frac{\\mbox{dev_E}}{\\mbox{dev_T}}.\n\\end{equation}\\]\nQuando l’insieme di tutte le deviazioni della \\(y\\) dalla media è spiegato dall’insieme di tutte le deviazioni della variabile teorica \\(\\hat{y}\\) dalla media, si ha che l’adattamento (o accostamento) del modello al campione di dati è perfetto, la devianza residua è nulla ed \\(r^2 = 1\\); nel caso opposto, la variabilità totale coincide con quella residua, per cui \\(r^2 = 0\\). Tra questi due estremi, \\(r\\) indica l’intensità della relazione lineare tra le due variabili e \\(r^2\\), con \\(0 \\leq r^2 \\leq 1\\), esprime la porzione della devianza totale della \\(y\\) che è spiegata dalla regressione lineare sulla \\(x\\).\nPer l’esempio in discussione abbiamo quanto segue. La devianza totale è\n\nCodicedev_t <- sum((kidiq$kid_score - mean(kidiq$kid_score))^2)\ndev_t\n#> [1] 180386.2\n\n\nLa devianza spiegata è\n\nCodicedev_r <- sum((fm$fitted.values - mean(kidiq$kid_score))^2)\ndev_r\n#> [1] 36248.82\n\n\nL’indice di determinazione è\n\nCodiceR2 <- dev_r / dev_t\nR2\n#> [1] 0.2009512\n\n\nNell’output di lm() un tale valore è chiamato Multiple R-squared.\n\nCodicesummary(fm)\n#> \n#> Call:\n#> lm(formula = kid_score ~ mom_iq, data = kidiq)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -56.753 -12.074   2.217  11.710  47.691 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 25.79978    5.91741    4.36 1.63e-05 ***\n#> mom_iq       0.60997    0.05852   10.42  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 18.27 on 432 degrees of freedom\n#> Multiple R-squared:  0.201,  Adjusted R-squared:  0.1991 \n#> F-statistic: 108.6 on 1 and 432 DF,  p-value: < 2.2e-16\n\n\nIl risultato ottenuto si può interpretare dicendo che circa il 20% della variabilità dei punteggi del QI dei bambini può essere predetto conoscendo il QI delle madri.\n\n26.2.1 Inferenza sul modello di regressione\nLa discussione precedente era tutta basata sulla trattazione “classica” del modello lineare, ovvero una trattazione basata sulle stime di massima verosimiglianza (se \\(y \\sim \\mathcal{N}(\\alpha + \\beta x, \\sigma)\\), allora le stime dei minimi quadrati coincidono con le stime di massima verosimiglianza). In altre parole, nella discussione precedente non abbiamo considerato in alcun modo le distribuzioni a priori dei parametri \\(\\alpha\\) e \\(\\beta\\). I risultati precedenti si confermano, in un contesto bayesiano, se e solo se imponiamo sui parametri delle distribuzioni a priori non informative (cioè, uniformi). In tali circostanze, le stime di massima verosimiglianza risultano identiche al massimo a posteriori bayesiano.\nDetto questo, il tema dell’inferenza viene trattato dall’approccio frequentista costruendo la “distribuzione campionaria” dei parametri (ovvero la distribuzione dei valori che i parametri otterrebbero in infiniti campioni casuali (\\(x, y\\)) di ampiezza \\(n\\) estratti dalla medesima popolazione) e poi calcolando gli errori standard dei parametri e gli intervalli di fiducia dei parametri. Una domanda frequente è, per esempio, se la pendenza della retta di regressione sia maggiore di zero. Per rispondere a tale domanda l’approccio frequentista calcola l’intervallo di fiducia al 95% per il parametro \\(\\beta\\). Se tale intervallo non include lo zero, e se il limite inferiore di tale intervallo è maggiore di zero, allora si conclude, con un grado di confidenza del 95%, che il vero parametro \\(\\beta\\) nella popolazione è maggiore di zero. Ovvero, si conclude che vi sono evidenze di un’associazione lineare positiva tra \\(x\\) e \\(y\\).\nAlla stessa conclusione si può arrivare calcolando, in un ottica bayesiana, l’intervallo di credibilità al 95% per il parametro \\(\\beta\\). I due intervalli sono identici se usiamo una distribuzione a priori piatta. Sono invece diversi se usiamo una distribuzione a priori debolmente informativa, oppure informativa.\nSolitamente si usa una distribuzione a priori debolmente informativa centrata sullo zero. In tali circostanze, l’uso della distribuzione a priori ha solo un effetto di regolarizzazione, ovvero di riduzione del peso delle osservazioni estreme – un tale risultato statistico è molto desiderabile, ma è difficile da ottenere in un contesto frequentista. Vedremo nel prossimo capitolo come può essere svolta l’inferenza sui coefficienti del modello di regressione lineare in un contesto bayesiano."
  },
  {
    "objectID": "052_reglin2.html#commenti-e-considerazioni-finali",
    "href": "052_reglin2.html#commenti-e-considerazioni-finali",
    "title": "26  Regressione lineare bivariata",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nIl modello lineare bivariato viene usato per descrivere la relazione tra due variabili e per determinare il segno e l’intensità di tale relazione. Inoltre, il modello lineare ci consente di prevedere il valore della variabile dipendente in base al valore assunto dalla variabile indipendente."
  }
]