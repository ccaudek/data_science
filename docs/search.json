[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science per psicologi",
    "section": "",
    "text": "Questo è il sito web per “Data Science per psicologi”. Viene qui presentato il materiale delle lezioni dell’insegnamento di Psicometria B000286 (A.A. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. Lo scopo di questo insegnamento è quello di fornire agli studenti un’introduzione all’analisi dei dati psicologici. Le conoscenze/competenze che verranno sviluppate in questo insegnamento sono quelle della Data Science applicata alla psicologia, ovvero, un insieme di conoscenze/competenze che si pongono all’intersezione tra psicologia, statistica e informatica."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Data Science per psicologi",
    "section": "License",
    "text": "License\nThis book was created by Corrado Caudek and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Prefazione",
    "section": "",
    "text": "Questo libro ti insegnerà i principi base della Data Science, con esempi pratici.\nSembra sensato spendere due parole su una domanda che è importante per gli studenti: perché dobbiamo perdere tanto tempo a studiare l’analisi dei dati psicologici quando in realtà quello che ci interessa è tutt’altro? Questa è una bella domanda. C’è una ragione molto semplice che dovrebbe farci capire perché la Data Science sia così importante per la psicologia. Infatti, a ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, in certi casi, predire. In questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili a quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che i problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data Science in psicologia: perché la Data Science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.\nSono sicuro che, leggendo queste righe, a molti studenti sarà venuta in mente la seguente domanda: perché non chiediamo a qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data Science? La risposta a questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data Science. Le tematiche della Data Science non possono essere ignorate né dai ricercatori in psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche i professionisti al di fuori dall’università non possono fare a meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data Science! Basta aprire a caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano i risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.\nLe considerazioni precedenti cercano di chiarire il seguente punto: la Data Science non è qualcosa da studiare a malincuore, in un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data Science in tantissimi ambiti della loro attività professionale: in particolare quando costruiscono, somministrano e interpretano i test psicometrici. È dunque chiaro che possedere delle solide basi di Data Science è un tassello imprescindibile del bagaglio professionale dello psicologo. In questo insegnamento verrano trattati i temi base della Data Science e verrà adottato un punto di vista bayesiano, che corrisponde all’approccio più recente e sempre più diffuso in psicologia."
  },
  {
    "objectID": "preface.html#come-studiare",
    "href": "preface.html#come-studiare",
    "title": "Prefazione",
    "section": "Come studiare",
    "text": "Come studiare\nIl giusto metodo di studio per prepararsi all’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare i concetti via via che essi vengono presentati e verificare in autonomia le procedure presentate a lezione. Incoraggio gli studenti a farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti a utilizzare i forum attivi su Moodle e, soprattutto, a svolgere gli esercizi proposti su Moodle. I problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino a quel punto sono sufficienti rispetto alle richieste dell’esame.\nLa prima fase dello studio, che è sicuramente individuale, è quella in cui lo studente deve acquisire le conoscenze teoriche relative ai problemi che saranno presentati all’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta lo studente ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (\\(\\textsf{R}\\)) per applicare i concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso aiuta a capire meglio."
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Parte 2: Il calcolo delle probabilità",
    "section": "",
    "text": "Nel capitolo ?sec-intro-prob-1 verrà presentata la legge dei grandi numeri, il concetto di variabile casuale e la funzione di massa di probabilità."
  },
  {
    "objectID": "023_cont_rv_distr.html",
    "href": "023_cont_rv_distr.html",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "",
    "text": "Dopo avere introdotto con una simulazione il concetto di funzione di densità nel Capitolo ?sec-intro-density-function, prendiamo ora in esame alcune delle densità di probabilità più note. La più importante di esse è sicuramente la distribuzione Normale."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-gaussiana",
    "href": "023_cont_rv_distr.html#distribuzione-gaussiana",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.1 Distribuzione Gaussiana",
    "text": "1.1 Distribuzione Gaussiana\nNon c’è un’unica distribuzione gaussiana (o normale): la distribuzione gaussiana è una famiglia di distribuzioni. Tali distribuzioni sono dette “gaussiane” in onore di Carl Friedrich Gauss (uno dei più grandi matematici della storia il quale, tra le altre cose, scoprì l’utilità di tale funzione di densità per descrivere gli errori di misurazione). Adolphe Quetelet, il padre delle scienze sociali quantitative, fu il primo ad applicare tale funzione di densità alle misurazioni dell’uomo. Karl Pearson usò per primo il termine “distribuzione normale” anche se ammise che questa espressione “ha lo svantaggio di indurre le persone a credere che le altre distribuzioni, in un senso o nell’altro, non siano normali.”\n\n1.1.1 Limite delle distribuzioni binomiali\nIniziamo con un un breve excursus storico. Nel 1733, Abraham de Moivre notò che, aumentando il numero di prove di una distribuzione binomiale, la distribuzione risultante diventava quasi simmetrica e a forma campanulare. Con 10 prove e una probabilità di successo di 0.9 in ciascuna prova, la distribuzione è chiaramente asimmetrica.\n\nCodiceN <- 10\nx <- 0:10\ny <- dbinom(x, N, 0.9)\nbinomial_limit_plot <-\n  tibble(x = x, y = y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_bar(\n    stat = \"identity\", color = \"black\", size = 0.2\n  ) +\n  xlab(\"y\") +\n  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +\n  ylab(\"Binomial(y | 10, 0.9)\")\nbinomial_limit_plot\n\n\n\nFigura 1.1: Probabilità del numero di successi in \\(N = 10\\) prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione \\(\\mbox{Bin}(y \\mid 10, 0.9)\\). Con solo dieci prove, la distribuzione è fortemente asimmetrica negativa.\n\n\n\n\nMa se aumentiamo il numero di prove di un fattore di 100 a N = 1000, senza modificare la probabilità di successo di 0.9, la distribuzione assume una forma campanulare quasi simmetrica. Dunque, de Moivre scoprì che, quando N è grande, la funzione gaussiana (che introdurremo qui sotto), nonostante sia la densità di v.a. continue, fornisce una buona approssimazione alla funzione di massa di probabilità binomiale.\n\nCodiceN <- 1000\nx <- 0:1000\ny <- dbinom(x, N, 0.9)\nbinomial_limit_plot <-\n  tibble(x = x, y = y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_bar(stat = \"identity\", color = \"black\", size = 0.2) +\n  xlab(\"y\") +\n  ylab(\"Binomial(y | 1000, 0.9)\") +\n  xlim(850, 950)\nbinomial_limit_plot\n\n\n\nFigura 1.2: Probabilità del numero di successi in \\(N = 1000\\) prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione \\(\\mbox{Bin}(y \\mid 1000, 0.9)\\). Con mille prove, la distribuzione è quasi simmetrica a forma campanulare.\n\n\n\n\nLa distribuzione Normale fu scoperta da Gauss nel 1809. Il Paragrafo successivo illustra come si possa giungere alla Normale mediante una simulazione."
  },
  {
    "objectID": "023_cont_rv_distr.html#normal-random-walk",
    "href": "023_cont_rv_distr.html#normal-random-walk",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.2 La Normale prodotta con una simulazione",
    "text": "1.2 La Normale prodotta con una simulazione\nMcElreath (2020) illustra come sia possibile giungere alla distribuzione Normale mediante una simulazione. Supponiamo che vi siano mille persone tutte allineate su una linea di partenza. Quando viene dato un segnale, ciascuna persona lancia una moneta e fa un passo in avanti oppure all’indietro a seconda che sia uscita testa o croce. Supponiamo che la lunghezza di ciascun passo vari da 0 a 1 metro. Ciascuna persona lancia una moneta 16 volte e dunque compie 16 passi.\nAlla conclusione di queste passeggiate casuali (random walk) non possiamo sapere con esattezza dove si troverà ciascuna persona, ma possiamo conoscere con certezza le caratteristiche della distribuzione delle mille distanze dall’origine. Per esempio, possiamo predire in maniera accurata la proporzione di persone che si sono spostate in avanti oppure all’indietro. Oppure, possiamo predire accuratamente la proporzione di persone che si troveranno ad una certa distanza dalla linea di partenza (es., a 1.5 m dall’origine).\nQueste predizioni sono possibili perché tali distanze si distribuiscono secondo la legge Normale. È facile simulare questo processo usando . I risultati della simulazione sono riportati nella Figura 1.3.\n\nCodicepos <-\n  replicate(100, runif(16, -1, 1)) %>%\n  as_tibble() %>%\n  rbind(0, .) %>%\n  mutate(step = 0:16) %>%\n  gather(key, value, -step) %>%\n  mutate(person = rep(1:100, each = 17)) %>%\n  group_by(person) %>%\n  mutate(position = cumsum(value)) %>%\n  ungroup()\n\nggplot(\n  data = pos,\n  aes(x = step, y = position, group = person)\n) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_line(aes(color = person < 2, alpha = person < 2)) +\n  scale_color_manual(values = c(\"gray\", \"black\")) +\n  scale_alpha_manual(values = c(1 / 5, 1)) +\n  scale_x_continuous(\n    \"Numero di passi\",\n    breaks = c(0, 4, 8, 12, 16)\n  ) +\n  labs(y = \"Posizione\") +\n  theme(legend.position = \"none\")\n\n\n\nFigura 1.3: Passeggiata casuale di 4, 8 e 16 passi. La spezzata nera indica la media delle distanze dall’origine come funzione del numero di passi.\n\n\n\n\nUn kernel density plot delle distanze ottenute dopo 4, 8 e 16 passi è riportato nella Figura 1.4. Nel pannello di destra, al kernel density plot è stata sovrapposta una densità Normale di opportuni parametri (linea tratteggiata).\n\nCodicep1 <-\n  pos %>%\n  filter(step == 4) %>%\n  ggplot(aes(x = position)) +\n  geom_line(stat = \"density\", color = \"black\") +\n  labs(title = \"4 passi\")\n\np2 <-\n  pos %>%\n  filter(step == 8) %>%\n  ggplot(aes(x = position)) +\n  geom_density(color = \"black\", outline.type = \"full\") +\n  labs(title = \"8 passi\")\n\nsd <-\n  pos %>%\n  filter(step == 16) %>%\n  summarise(sd = sd(position)) %>%\n  pull(sd)\n\np3 <-\n  pos %>%\n  filter(step == 16) %>%\n  ggplot(aes(x = position)) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = 0, sd = sd),\n    linetype = 2\n  ) +\n  geom_density(color = \"black\", alpha = 1 / 2) +\n  labs(\n    title = \"16 passi\",\n    y = \"Densità\"\n  )\n\n(p1 | p2 | p3) & coord_cartesian(xlim = c(-6, 6))\n\n\n\nFigura 1.4: Kernel density plot dei risultati della passeggiata casuale riportata nella figura precente, dopo 4, 8 e 16 passi. Nel pannello di destra, una densità Normale di opportuni parametri è sovrapposta all’istogramma lisciato.\n\n\n\n\nQuesta simulazione mostra che qualunque processo nel quale viene sommato un certo numero di valori casuali, tutti provenienti dalla medesima distribuzione, converge ad una distribuzione Normale. Non importa quale sia la forma della distribuzione di partenza: essa può essere uniforme, come nell’esempio presente, o di qualunque altro tipo. La forma della distribuzione da cui viene realizzato il campionamento determina la velocità della convergenza alla Normale. In alcuni casi la convergenza è lenta; in altri casi la convergenza è molto rapida (come nell’esempio presente).\nDa un punto di vista formale, diciamo che una variabile casuale continua \\(Y\\) ha una distribuzione Normale se la sua densità è\n\\[\nf(y; \\mu, \\sigma) = {1 \\over {\\sigma\\sqrt{2\\pi} }} \\exp \\left\\{-\\frac{(y -  \\mu)^2}{2 \\sigma^2} \\right\\},\n\\tag{1.1}\\]\ndove \\(\\mu \\in \\mathbb{R}\\) e \\(\\sigma > 0\\) sono i parametri della distribuzione.\nLa densità normale è unimodale e simmetrica con una caratteristica forma a campana e con il punto di massima densità in corrispondenza di \\(\\mu\\).\nIl significato dei parametri \\(\\mu\\) e \\(\\sigma\\) che appaiono nell’Equazione 1.1 viene chiarito dalla dimostrazione che\n\\[\n\\mathbb{E}(Y) = \\mu, \\qquad \\mathbb{V}(Y) = \\sigma^2.\n\\]\nLa rappresentazione grafica di quattro densità Normali tutte con media 0 e con deviazioni standard 0.25, 0.5, 1 e 2 è fornita nella Figura 1.5.\n\n\n\n\nFigura 1.5: Alcune distribuzioni Normali.\n\n\n\n\n\n1.2.1 Concentrazione\nÈ istruttivo osservare il grado di concentrazione della distribuzione Normale attorno alla media:\n\\[\n\\begin{align}\nP(\\mu - \\sigma < Y < \\mu + \\sigma) &= P (-1 < Z < 1) \\simeq 0.683, \\notag\\\\\nP(\\mu - 2\\sigma < Y < \\mu + 2\\sigma) &= P (-2 < Z < 2) \\simeq 0.956, \\notag\\\\\nP(\\mu - 3\\sigma < Y < \\mu + 3\\sigma) &= P (-3 < Z < 3) \\simeq 0.997. \\notag\n\\end{align}\n\\]\nSi noti come un dato la cui distanza dalla media è superiore a 3 volte la deviazione standard presenti un carattere di eccezionalità perché meno del 0.3% dei dati della distribuzione Normale presentano questa caratteristica.\nPer indicare la distribuzione Normale si usa la notazione \\(\\mathcal{N}(\\mu, \\sigma)\\).\n\n1.2.2 Funzione di ripartizione\nIl valore della funzione di ripartizione di \\(Y\\) nel punto \\(y\\) è l’area sottesa alla curva di densità \\(f(y)\\) nella semiretta \\((-\\infty, y]\\). Non esiste alcuna funzione elementare per la funzione di ripartizione\n\\[\nF(y) = \\int_{-\\infty}^y {1 \\over {\\sigma\\sqrt{2\\pi} }} \\exp \\left\\{-\\frac{(y - \\mu)^2}{2\\sigma^2} \\right\\} dy,\n\\tag{1.2}\\]\npertanto le probabilità \\(P(Y < y)\\) vengono calcolate mediante integrazione numerica approssimata. I valori della funzione di ripartizione di una variabile casuale Normale sono dunque forniti da un software.\n\nEsercizio 1.1 Usiamo \\(\\mathsf{R}\\) per calcolare la funzione di ripartizione della Normale. La funzione pnorm(q, mean, sd) restituisce la funzione di ripartizione della Normale con media mean e deviazione standard sd, ovvero l’area sottesa alla funzione di densità di una Normale con media mean e deviazione standard sd nell’intervallo \\([-\\infty, q]\\).\nPer esempio, in precedenza abbiamo detto che il 68% circa dell’area sottesa ad una Normale è compresa nell’intervallo \\(\\mu \\pm \\sigma\\). Verifichiamo per la distribuzione del QI \\(\\sim \\mathcal{N}(\\mu = 100, \\sigma = 15)\\):\n\nCodicepnorm(100+15, 100, 15) - pnorm(100-15, 100, 15)\n#> [1] 0.6826895\n\n\nIl 95% dell’area è compresa nell’intervallo \\(\\mu \\pm 1.96 \\cdot\\sigma\\):\n\nCodicepnorm(100 + 1.96 * 15, 100, 15) - pnorm(100 - 1.96 * 15, 100, 15)\n#> [1] 0.9500042\n\n\nQuasi tutta la distribuzione è compresa nell’intervallo \\(\\mu \\pm 3 \\cdot\\sigma\\):\n\nCodicepnorm(100 + 3 * 15, 100, 15) - pnorm(100 - 3 * 15, 100, 15)\n#> [1] 0.9973002\n\n\n\n\n1.2.3 Distribuzione Normale standard\nLa distribuzione Normale di parametri \\(\\mu = 0\\) e \\(\\sigma = 1\\) viene detta distribuzione Normale standard. La famiglia Normale è l’insieme avente come elementi tutte le distribuzioni Normali con parametri \\(\\mu\\) e \\(\\sigma\\) diversi. Tutte le distribuzioni Normali si ottengono dalla Normale standard mediante una trasformazione lineare: se \\(Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y)\\) allora\n\\[\nX = a + b Y \\sim \\mathcal{N}(\\mu_X = a+b \\mu_Y, \\sigma_X = \\left|b\\right|\\sigma_Y).\n\\]\nL’area sottesa alla curva di densità di \\(\\mathcal{N}(\\mu, \\sigma)\\) nella semiretta \\((-\\infty, y]\\) è uguale all’area sottesa alla densità Normale standard nella semiretta \\((-\\infty, z]\\), in cui \\(z = (y -\\mu_Y )/\\sigma_Y\\) è il punteggio standard di \\(Y\\). Per la simmetria della distribuzione, l’area sottesa nella semiretta \\([1, \\infty)\\) è uguale all’area sottesa nella semiretta \\((-\\infty, 1]\\) e quest’ultima coincide con \\(F(-1)\\). Analogamente, l’area sottesa nell’intervallo \\([y_a, y_b]\\), con \\(y_a < y_b\\), è pari a \\(F(z_b) - F(z_a)\\), dove \\(z_a\\) e \\(z_b\\) sono i punteggi standard di \\(y_a\\) e \\(y_b\\).\nSi ha anche il problema inverso rispetto a quello del calcolo delle aree: dato un numero \\(0 \\leq p \\leq 1\\), il problema è quello di determinare un numero \\(z \\in \\mathbb{R}\\) tale che \\(P(Z < z) = p\\). Il valore \\(z\\) cercato è detto quantile di ordine \\(p\\) della Normale standard e può essere trovato mediante un software.\n\nEsercizio 1.2 \nSupponiamo che l’altezza degli individui adulti segua la distribuzione Normale di media \\(\\mu = 1.7\\) m e deviazione standard \\(\\sigma = 0.1\\) m. Vogliamo sapere la proporzione di individui adulti con un’altezza compresa tra \\(1.7\\) e \\(1.8\\) m.\n\n\nSoluzione. Il problema ci chiede di trovare l’area sottesa alla distribuzione \\(\\mathcal{N}(\\mu = 1.7, \\sigma = 0.1)\\) nell’intervallo \\([1.7, 1.8]\\):\n\nCodicedf <- tibble(x = seq(1.4, 2.0, length.out = 100)) %>%\n  mutate(y = dnorm(x, mean = 1.7, sd = 0.1))\n\nggplot(df, aes(x, y)) +\n  geom_area(fill = \"sky blue\") +\n  gghighlight(x < 1.8 & x > 1.7) +\n  labs(\n    x = \"Altezza\",\n    y = \"Densità\"\n  )\n\n\n\n\n\n\n\nLa risposta si trova utilizzando la funzione di ripartizione \\(F(X)\\) della legge \\(\\mathcal{N}(1.7, 0.1)\\) in corrispondenza dei due valori forniti dal problema: \\(F(X = 1.8) - F(X = 1.7)\\). Utilizzando la seguente istruzione\n\nCodicepnorm(1.8, 1.7, 0.1) - pnorm(1.7, 1.7, 0.1)\n#> [1] 0.3413447\n\n\notteniamo il \\(31.43\\%\\).\nIn maniera equivalente, possiamo standardizzare i valori che delimitano l’intervallo considerato e utilizzare la funzione di ripartizione della normale standardizzata. I limiti inferiore e superiore dell’intervallo sono\n\\[\nz_{\\text{inf}} = \\frac{1.7 - 1.7}{0.1} = 0, \\quad z_{\\text{sup}} = \\frac{1.8 - 1.7}{0.1} = 1.0,\n\\]\nquindi otteniamo\n\nCodicepnorm(1.0, 0, 1) - pnorm(0, 0, 1)\n#> [1] 0.3413447\n\n\nIl modo più semplice per risolvere questo problema resta comunque quello di rendersi conto che la probabilità richiesta non è altro che la metà dell’area sottesa dalle distribuzioni Normali nell’intervallo \\([\\mu - \\sigma, \\mu + \\sigma]\\), ovvero \\(0.683/2\\).\n\n\n1.2.3.1 Funzione di ripartizione della normale standard e funzione logistica\nSi noti che la funzione logistica (in blu), pur essendo del tutto diversa dalla Normale dal punto di vista formale, assomiglia molto alla Normale standard quando le due cdf hanno la stessa varianza.\n\nCodicetibble(x = c(-3, 3)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = pnorm) +\n  stat_function(\n    fun = plogis,\n    args = list(scale = 0.56)\n  )"
  },
  {
    "objectID": "023_cont_rv_distr.html#teorema-del-limite-centrale",
    "href": "023_cont_rv_distr.html#teorema-del-limite-centrale",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.3 Teorema del limite centrale",
    "text": "1.3 Teorema del limite centrale\nLaplace dimostrò il teorema del limite centrale (TLC) nel 1812. Il TLC ci dice che se prendiamo una sequenza di variabili casuali indipendenti e le sommiamo, tale somma tende a distribuirsi come una Normale. Il TLC specifica inoltre, sulla base dei valori attesi e delle varianze delle v.c. che vengono sommate, quali sono i parametri della distribuzione Normale così ottenuta.\n\nTeorema 1.1 Si supponga che \\(Y = Y_1, \\dots, Y_i, \\ldots, Y_n\\) sia una sequenza di v.a. i.i.d. con \\(\\mathbb{E}(Y_i) = \\mu\\) e \\(\\mbox{SD}(Y_i) = \\sigma\\). Si definisca una nuova v.c. come:\n\\[\nZ = \\frac{1}{n} \\sum_{i=1}^n Y_i.\n\\]\nCon \\(n \\rightarrow \\infty\\), \\(Z\\) tenderà ad una Normale con lo stesso valore atteso di \\(Y_i\\) e una deviazione standard che sarà più piccola della deviazione standard originaria di un fattore pari a \\(\\frac{1}{\\sqrt{n}}\\):\n\\[\np_Z(z) \\rightarrow \\mathcal{N}\\left(z \\ \\Bigg| \\ \\mu, \\, \\frac{1}{\\sqrt{n}} \\cdot \\sigma \\right).\n\\]\n\nIl TLC può essere generalizzato a variabili che non hanno la stessa distribuzione purché siano indipendenti e abbiano aspettative e varianze finite. Molti fenomeni naturali, come l’altezza dell’uomo adulto di entrambi i generi, sono il risultato di una serie di effetti additivi relativamente piccoli, la cui combinazione porta alla normalità, indipendentemente da come gli effetti additivi sono distribuiti. Questo è il motivo per cui la distribuzione normale forniscre una buona rappresentazione della distribuzione di molti fenomeni naturali."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-chi-quadrato",
    "href": "023_cont_rv_distr.html#distribuzione-chi-quadrato",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.4 Distribuzione Chi-quadrato",
    "text": "1.4 Distribuzione Chi-quadrato\nDalla Normale deriva la distribuzione \\(\\chi^2\\). La distribuzione \\(\\chi^2_{~k}\\) con \\(k\\) gradi di libertà descrive la variabile casuale\n\\[\nZ_1^2 + Z_2^2 + \\dots + Z_k^2,\n\\]\ndove \\(Z_1, Z_2, \\dots, Z_k\\) sono variabili casuali i.i.d. che seguono la distribuzione Normale standard \\(\\mathcal{N}(0, 1)\\). La variabile casuale chi-quadrato dipende dal parametro intero positivo \\(\\nu = k\\) che ne identifica il numero di gradi di libertà. La densità di probabilità di \\(\\chi^2_{~\\nu}\\) è\n\\[\nf(x) = C_{\\nu} x^{\\nu/2-1} \\exp (-x/2), \\qquad \\text{se } x > 0,\n\\]\ndove \\(C_{\\nu}\\) è una costante positiva.\nLa Figura 1.6 mostra alcune distribuzioni Chi-quadrato variando il parametro \\(\\nu\\).\n\n\n\n\nFigura 1.6: Alcune distribuzioni Chi-quadrato.\n\n\n\n\n\n1.4.1 Proprietà\n\nLa distribuzione di densità \\(\\chi^2_{~\\nu}\\) è asimmetrica.\nIl valore atteso di una variabile \\(\\chi^2_{~\\nu}\\) è uguale a \\(\\nu\\).\nLa varianza di una variabile \\(\\chi^2_{~\\nu}\\) è uguale a \\(2\\nu\\).\nPer \\(k \\rightarrow \\infty\\), la \\(\\chi^2_{~\\nu} \\rightarrow \\mathcal{N}\\).\nSe \\(X\\) e \\(Y\\) sono due variabili casuali chi-quadrato indipendenti con \\(\\nu_1\\) e \\(\\nu_2\\) gradi di libertà, ne segue che \\(X + Y \\sim \\chi^2_m\\), con \\(m = \\nu_1 + \\nu_2\\). Tale principio si estende a qualunque numero finito di variabili casuali chi-quadrato indipendenti.\n\n\nEsercizio 1.3 Usiamo \\(\\mathsf{R}\\) per disegnare la densità chi-quadrato con 3 gradi di libertà dividendo l’area sottesa alla curva di densità in due parti uguali.\n\nCodicedf <- tibble(x = seq(0, 15.0, length.out = 100)) %>%\n  mutate(y = dchisq(x, 3))\n\nggplot(df, aes(x, y)) +\n  geom_area(fill = \"sky blue\") +\n  gghighlight(x < 3) +\n  labs(\n    x = \"V.a. chi-quadrato con 3 gradi di libertà\",\n    y = \"Densità\"\n  )"
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-t-di-student",
    "href": "023_cont_rv_distr.html#distribuzione-t-di-student",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.5 Distribuzione \\(t\\) di Student",
    "text": "1.5 Distribuzione \\(t\\) di Student\nDalle distribuzioni Normale e Chi-quadrato deriva un’altra distribuzione molto nota, la \\(t\\) di Student. Se \\(Z \\sim \\mathcal{N}\\) e \\(W \\sim \\chi^2_{~\\nu}\\) sono due variabili casuali indipendenti, allora il rapporto\n\\[\nT = \\frac{Z}{\\Big( \\frac{W}{\\nu}\\Big)^{\\frac{1}{2}}}\n\\tag{1.3}\\]\ndefinisce la distribuzione \\(t\\) di Student con \\(\\nu\\) gradi di libertà. Si usa scrivere \\(T \\sim t_{\\nu}\\). L’andamento della distribuzione \\(t\\) di Student è simile a quello della distribuzione Normale, ma ha una maggiore dispersione (ha le code più pesanti di una Normale, ovvero ha una varianza maggiore di 1).\nLa Figura 1.7 mostra alcune distribuzioni \\(t\\) di Student variando il parametro \\(\\nu\\).\n\n\n\n\nFigura 1.7: Alcune distribuzioni \\(t\\) di Student.\n\n\n\n\n\n1.5.1 Proprietà\nLa variabile casuale \\(t\\) di Student soddisfa le seguenti proprietà:\n\nPer \\(\\nu \\rightarrow \\infty\\), \\(t_{\\nu}\\) tende alla normale standard \\(\\mathcal{N}(0, 1)\\).\nLa densità della \\(t_{\\nu}\\) è una funzione simmetrica con valore atteso nullo.\nPer \\(\\nu > 2\\), la varianza della \\(t_{\\nu}\\) vale \\(\\nu/(\\nu - 2)\\); pertanto è sempre maggiore di 1 e tende a 1 per \\(\\nu \\rightarrow \\infty\\)."
  },
  {
    "objectID": "023_cont_rv_distr.html#funzione-beta",
    "href": "023_cont_rv_distr.html#funzione-beta",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.6 Funzione beta",
    "text": "1.6 Funzione beta\nLa funzione beta di Eulero è una funzione matematica, non una densità di probabilità. La menzioniamo qui perché viene utilizzata nella distribuzione Beta. La funzione beta si può scrivere in molti modi diversi; per i nostri scopi la scriveremo così:\n\\[\nB(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\,,\n\\tag{1.4}\\]\ndove \\(\\Gamma(x)\\) è la funzione Gamma, ovvero il fattoriale discendente, cioè\n\\[\n(x-1)(x-2)\\ldots (x-n+1)\\notag\\,.\n\\]\n\nEsercizio 1.4 \nPer esempio, posti \\(\\alpha = 3\\) e \\(\\beta = 9\\), la funzione beta assume il valore\n\nCodicealpha <- 3\nbeta <- 9\nbeta(alpha, beta)\n#> [1] 0.002020202\n\n\nPer chiarire, lo stesso risultato si ottiene con\n\nCodice((2) * (8 * 7 * 6 * 5 * 4 * 3 * 2)) / \n  (11 * 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2)\n#> [1] 0.002020202\n\n\novvero\n\nCodicegamma(alpha) * gamma(beta) / gamma(alpha + beta)\n#> [1] 0.002020202"
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-beta",
    "href": "023_cont_rv_distr.html#distribuzione-beta",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.7 Distribuzione Beta",
    "text": "1.7 Distribuzione Beta\nLa distribuzione Beta è una distribuzione usata per modellare percentuali e proporzioni in quanto è definita sull’intervallo \\((0; 1)\\) – ma non include i valori 0 o 1.\n\nDefinizione 1.1 Sia \\(\\pi\\) una variabile casuale che può assumere qualsiasi valore compreso tra 0 e 1, cioè \\(\\pi \\in [0, 1]\\). Diremo che \\(\\pi\\) segue la distribuzione Beta di parametri \\(\\alpha\\) e \\(\\beta\\), \\(\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\), se la sua densità è\n\\[\n\\begin{align}\n\\text{Beta}(\\pi \\mid \\alpha, \\beta) &= \\frac{1}{B(\\alpha, \\beta)}\\pi^{\\alpha-1} (1-\\pi)^{\\beta-1}\\notag\\\\\n&=  \\frac{\\Gamma(\\alpha+ \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\quad \\text{per } \\pi \\in [0, 1]\\,,\n\\end{align}\n\\tag{1.5}\\]\nladdove \\(B(\\alpha, \\beta)\\) è la funzione beta.\n\nI termini \\(\\alpha\\) e \\(\\beta\\) sono i parametri della distribuzione Beta e devono essere entrambi positivi. Tali parametri possono essere interpretati come l’espressione delle nostre credenze a priori relative ad una sequenza di prove Bernoulliane. Il parametro \\(\\alpha\\) rappresenta il numero di “successi” e il parametro \\(\\beta\\) il numero di “insuccessi”:\n\\[\n\\frac{\\text{Numero di successi}}{\\text{Numero di successi} + \\text{Numero di insuccessi}} = \\frac{\\alpha}{\\alpha + \\beta}\\notag\\,.\n\\]\nIl rapporto \\(\\frac{1}{B(\\alpha, \\beta)} = \\frac{\\Gamma(\\alpha+b)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\) è una costante di normalizzazione:\n\\[\n\\int_0^1 \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} = \\frac{\\Gamma(\\alpha+b)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\,.\n\\]\nAd esempio, con \\(\\alpha = 3\\) e \\(\\beta = 9\\) abbiamo\n\nCodicea <- 3\nb <- 9\nintegrand <- function(p) {p^{a - 1}*(1 - p)^{b - 1}}\nintegrate(integrand, lower = 0, upper = 1)\n#> 0.002020202 with absolute error < 2.2e-17\n\n\novvero\n\nCodice1 / (gamma(a + b) / (gamma(a) * gamma(b)))\n#> [1] 0.002020202\n\n\novvero\n\nCodicebeta(alpha, beta)\n#> [1] 0.002020202\n\n\nIl valore atteso, la moda e la varianza di una distribuzione Beta sono dati dalle seguenti equazioni:\n\\[\n\\mathbb{E}(\\pi) = \\frac{\\alpha}{\\alpha+\\beta}\\,,\n\\tag{1.6}\\]\n\\[\n\\mbox{Mo}(\\pi) = \\frac{\\alpha-1}{\\alpha+\\beta-2}\\,,\n\\tag{1.7}\\]\n\\[\n\\mathbb{V}(\\pi) = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)}\\,.\n\\tag{1.8}\\]\nAl variare di \\(\\alpha\\) e \\(\\beta\\) si ottengono molte distribuzioni di forma diversa; un’illustrazione è fornita dalla seguente GIF animata.\nSi può ottenere una rappresentazione grafica della distribuzione \\(\\mbox{Beta}(\\pi \\mid \\alpha, \\beta)\\) con la funzione plot_beta() del pacchetto bayesrules. Per esempio:\n\nCodicebayesrules::plot_beta(alpha = 3, beta = 9)\n\n\n\n\n\n\n\nLa funzione bayesrules::summarize_beta() ci restituisce la media, moda e varianza della distribuzione Beta. Per esempio:\n\nCodicebayesrules::summarize_beta(alpha = 3, beta = 9)\n#>   mean mode        var        sd\n#> 1 0.25  0.2 0.01442308 0.1200961\n\n\n\nNota. Attenzione alle parole: in questo contesto, il termine “beta” viene utilizzato con tre significati diversi:\n\nla distribuzione di densità Beta,\nla funzione matematica beta,\nil parametro \\(\\beta\\).\n\n\n\nEsercizio 1.5 \nNel disturbo depressivo la recidiva è definita come la comparsa di un nuovo episodio depressivo che si manifesta dopo un prolungato periodo di recupero (6-12 mesi) con stato di eutimia (umore relativamente normale). Supponiamo che una serie di studi mostri una comparsa di recidiva in una proporzione che va dal 20% al 60% dei casi, con una media del 40% (per una recente discussione, si veda Nuggerud-Galeas et al., 2020). Sulla base di queste informazioni, si usi la distribuzione Beta per rappresentare le credenze a priori relativamente alla probabilità di recidiva.\n\n\nSoluzione. Il problema richiede di trovare i parametri della distribuzione Beta tali per cui la massa della densità sia compresa tra 0.2 e 0.6, con la media in corrispondenza di 0.4. Procedendo per tentativi ed errori, ed usando la funzione bayesrules::plot_beta(), un risultato possibile è \\(\\mbox{Beta}(16, 24)\\).\nLa funzione find_pars() prende in input la media e \\(\\alpha + \\beta\\), ritorna i valori dei parametri:\n\nCodicefind_pars <- function(ev, n) {\n  a <- ev * n\n  b <- n - a\n  return(c(round(a), round(b)))\n}\n\npars <- find_pars(.4, 40)\npars\n#> [1] 16 24\nbayesrules::plot_beta(pars[1], pars[2])\n\n\n\n\n\n\n\nVerifichiamo il valore della media della distribuzione:\n\nCodice16 / (16 + 24)\n#> [1] 0.4\n\n\nLa moda è\n\nCodice(16 - 1) / (16 + 24 - 2)\n#> [1] 0.3947368\n\n\nLa deviazione standard della distribuzione è uguale a circa 8 punti percentuali:\n\nCodicesqrt((16 * 24) / ((16 + 24)^2 * (16 + 24 + 1)))\n#> [1] 0.07650921\n\n\nGli stessi risultati si ottengono usando la funzione bayesrules::summarize_beta():\n\nCodicebayesrules::summarize_beta(alpha = 16, beta = 24)\n#>   mean      mode         var         sd\n#> 1  0.4 0.3947368 0.005853659 0.07650921\n\n\nPossiamo concludere dicendo che, se utilizziamo la distribuzione \\(\\mbox{Beta}(16, 24)\\) per rappresentare le nostre credenze (a priori) rispetto la possibilità di recidiva, ciò significa che pensiamo che la nostra incertezza sia quantificabile nei termini di una deviazione standard di circa 8 punti percentuali rispetto a tutti i valori possibili di recidiva, per i quali il valore più verosimile (ovvero, la media della distribuzione) è 0.40.\n\n\nEsercizio 1.6 \nPoniamoci ora il problema di verificare la nostra comprensione delle funzioni \\(\\mathsf{R}\\) che possono essere usate per la funzione Beta. Si risolva l’Esercizio 1.5 usando \\(\\mathsf{R}\\).\n\n\nSoluzione. Utilizziamo i seguenti parametri per la distribuzione Beta:\n\nCodicealpha <- 16\nbeta <- 24\n\n\nLa media di una \\(\\mbox{Beta}(16, 24)\\) è\n\nCodicealpha / (alpha + beta)\n#> [1] 0.4\n\n\nIn corrispondenza della media la densità della funzione è\n\nCodicedbeta(pi, alpha, beta)\n#> [1] 0\n\n\novvero\n\nCodicegamma(alpha + beta) / (gamma(alpha) * gamma(beta)) * \n  pi^(alpha - 1) * (1 - pi)^(beta - 1)\n#> [1] -6.99499e+26\n\n\nUsando la funzione dbeta() possiamo costruire un grafico della funzione \\(\\mbox{Beta}(16, 24)\\) nel modo seguente:\n\nCodicex <- seq(0, 1, length.out = 1e4)\ntibble(x) %>% \n  ggplot(aes(x, dbeta(x, alpha, beta))) +\n  geom_line() +\n  labs(\n    x = \"Probabilità di recidiva\",\n    y = \"Densità Beta(16, 24)\"\n  )"
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-di-cauchy",
    "href": "023_cont_rv_distr.html#distribuzione-di-cauchy",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.8 Distribuzione di Cauchy",
    "text": "1.8 Distribuzione di Cauchy\nLa distribuzione di Cauchy è un caso speciale della distribuzione di \\(t\\) di Student con 1 grado di libertà. È definita da una densità di probabilità che corrisponde alla seguente funzione, dipendente da due parametri \\(\\theta\\) e \\(d\\) (con la condizione \\(d > 0\\)),\n\\[\nf(x; \\theta, d) = \\frac{1}{\\pi d} \\frac{1}{1 + \\left(\\frac{x - \\theta}{d} \\right)^2},\n\\tag{1.9}\\]\ndove \\(\\theta\\) è la mediana della distribuzione e \\(d\\) ne misura la larghezza a metà altezza."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-log-normale",
    "href": "023_cont_rv_distr.html#distribuzione-log-normale",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.9 Distribuzione log-normale",
    "text": "1.9 Distribuzione log-normale\nSia \\(y\\) una variabile casuale avente distribuzione normale con media \\(\\mu\\) e varianza \\(\\sigma^2\\). Definiamo poi una nuova variabile casuale \\(x\\) attraverso la relazione\n\\[\nx = e^y \\quad \\Longleftrightarrow \\quad y = \\log x.\n\\] Il dominio di definizione della \\(x\\) è il semiasse \\(x > 0\\) e la densità di probabilità \\(f(x)\\) è data da\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\frac{1}{x} \\exp \\left\\{-\\frac{(\\log x -  \\mu)^2}{2 \\sigma^2} \\right\\}.\n\\tag{1.10}\\]\nQuesta funzione di densità è chiamata log-normale.\nIl valore atteso e la varianza di una distribuzione log-normale sono dati dalle seguenti equazioni:\n\\[\n\\mathbb{E}(x) = \\exp \\left\\{\\mu + \\frac{\\sigma^2}{2} \\right\\}.\n\\]\n\\[\n\\mathbb{V}(x) = \\exp \\left\\{2 \\mu + \\sigma^2 \\right\\} \\left(\\exp \\left\\{\\sigma^2 \\right\\}  -1\\right).\n\\]\nSi può dimostrare che il prodotto di variabili casuali log-normali ed indipendenti segue una distribuzione log-normale."
  },
  {
    "objectID": "023_cont_rv_distr.html#distribuzione-di-pareto",
    "href": "023_cont_rv_distr.html#distribuzione-di-pareto",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "\n1.10 Distribuzione di Pareto",
    "text": "1.10 Distribuzione di Pareto\nLa distribuzione paretiana (o distribuzione di Pareto) è una distribuzione di probabilità continua e così chiamata in onore di Vilfredo Pareto. La distribuzione di Pareto è una distribuzione di probabilità con legge di potenza utilizzata nella descrizione di fenomeni sociali e molti altri tipi di fenomeni osservabili. Originariamente applicata per descrivere la distribuzione del reddito in una società, adattandosi alla tendenza che una grande porzione di ricchezza è detenuta da una piccola frazione della popolazione, la distribuzione di Pareto è diventata colloquialmente nota e indicata come il principio di Pareto, o “regola 80-20”. Questa regola afferma che, ad esempio, l’80% della ricchezza di una società è detenuto dal 20% della sua popolazione. Viene spesso applicata nello studio della distribuzione del reddito, della dimensione dell’impresa, della dimensione di una popolazione e nelle fluttuazioni del prezzo delle azioni.\nLa densità di una distribuzione di Pareto è\n\\[\nf(x)=(x_m/x)^\\alpha,\n\\tag{1.11}\\]\ndove \\(x_m\\) (parametro di scala) è il minimo (necessariamente positivo) valore possibile di \\(X\\) e \\(\\alpha\\) è un parametro di forma.\n\n\n\n\n\n\n\n\nLa distribuzione di Pareto ha una asimmetria positiva. Il supporto della distribuzione di Pareto è la retta reale positiva. Tutti i valori devono essere maggiori del parametro di scala \\(x_m\\), che è in realtà un parametro di soglia."
  },
  {
    "objectID": "023_cont_rv_distr.html#commenti-e-considerazioni-finali",
    "href": "023_cont_rv_distr.html#commenti-e-considerazioni-finali",
    "title": "1  Distribuzioni di v.c. continue",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nIn questa dispensa le densità continue che useremo più spesso sono la distribuzione gaussiana e la distribuzione Beta. Faremo un uso limitato della distribuzione \\(t\\) di Student e della distribuzione di Cauchy. Le altre distribuzioni qui descritte sono stato presentate per completezza.\n\n\n\n\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nNuggerud-Galeas, S., Sáez-Benito Suescun, L., Berenguer Torrijo, N., Sáez-Benito Suescun, A., Aguilar-Latorre, A., Magallón Botaya, R., & Oliván Blázquez, B. (2020). Analysis of depressive episodes, their recurrence and pharmacologic treatment in primary care patients: A retrospective descriptive study. Plos One, 15(5), e0233454."
  },
  {
    "objectID": "024_likelihood.html",
    "href": "024_likelihood.html",
    "title": "2  La funzione di verosimiglianza",
    "section": "",
    "text": "La verosimiglianza viene utilizzata sia nell’inferenza bayesiana che in quella frequentista. In entrambi i paradigmi di inferenza, il suo ruolo è quantificare la forza con la quale i dati osservati supportano i possibili valori dei parametri sconosciuti di un modello statistico."
  },
  {
    "objectID": "024_likelihood.html#definizione",
    "href": "024_likelihood.html#definizione",
    "title": "2  La funzione di verosimiglianza",
    "section": "\n2.1 Definizione",
    "text": "2.1 Definizione\n\nDefinizione 2.1 \nLa funzione di verosimiglianza \\(\\mathcal{L}(\\theta \\mid y) = f(y \\mid \\theta), \\theta \\in \\Theta,\\) è la funzione di massa o di densità di probabilità dei dati \\(y\\) vista come una funzione del parametro sconosciuto (o dei parametri sconosciuti) \\(\\theta\\).\n\nDetto in altre parole, la funzione di verosimiglianza e la funzione di (massa o densità di) probabilità sono formalmente identiche, ma è completamente diversa la loro interpretazione:\n\nnel caso della funzione di massa o di densità di probabilità, la distribuzione del vettore casuale delle osservazioni campionarie \\(y\\) dipende dai valori assunti dal parametro (o dai parametri) \\(\\theta\\);\nnel caso della la funzione di verosimiglianza la credibilità assegnata a ciascun possibile valore \\(\\theta\\) viene determinata avendo acquisita l’informazione campionaria \\(y\\) che rappresenta l’elemento condizionante.\n\nLa funzione di verosimiglianza descrive in termini relativi il sostegno empirico che \\(\\theta \\in \\Theta\\) riceve da \\(y\\). Infatti, la funzione di verosimiglianza assume forme diverse al variare di \\(y\\). Possiamo dunque pensare alla funzione di verosimiglianza come alla risposta alla seguente domanda: avendo osservato i dati \\(y\\), quanto risultano (relativamente) credibili i diversi valori del parametro \\(\\theta\\)? In termini più formali possiamo dire: sulla base dei dati, \\(\\theta_1 \\in \\Theta\\) risulta più credibile di \\(\\theta_2 \\in \\Theta\\) quale indice del modello probabilistico generatore dei dati se \\(\\mathcal{L}(\\theta_1) > \\mathcal{L}(\\theta_1)\\).\nSi noti un punto importante: la funzione \\(\\mathcal{L}(\\theta \\mid y)\\) non è una funzione di densità. Infatti, essa non racchiude un’area unitaria."
  },
  {
    "objectID": "024_likelihood.html#notazione",
    "href": "024_likelihood.html#notazione",
    "title": "2  La funzione di verosimiglianza",
    "section": "\n2.2 Notazione",
    "text": "2.2 Notazione\nSeguendo una pratica comune, in questa dispensa all’interno di un framework bayesiano spesso useremo la notazione \\(p(\\cdot)\\) per rappresentare due quantità differenti, ovvero la funzione di verosimiglianza e la distribuzione a priori. Questo piccolo abuso di notazione riflette il seguente punto di vista: anche se la verosimiglianza non è una funzione di densità di probabilità, noi non vogliamo stressare questo aspetto, ma vogliamo piuttosto pensare alla verosimiglianza e alla distribuzione a priori come a due elementi che sono egualmente necessari per calcolare la distribuzione a posteriori. In altri termini, per così dire, questa notazione assegna lo stesso status epistemico alle due diverse quantità che si trovano al numeratore della regola di Bayes."
  },
  {
    "objectID": "024_likelihood.html#caso-binomiale",
    "href": "024_likelihood.html#caso-binomiale",
    "title": "2  La funzione di verosimiglianza",
    "section": "\n2.3 Caso binomiale",
    "text": "2.3 Caso binomiale\nIniziamo a discutere la funzione di verosimiglianza considerando il caso più semplice, ovvero quello Binomiale.\n\n2.3.1 Funzione di verosimiglianza\nPer \\(n\\) prove Bernoulliane indipendenti, le quali producono \\(y\\) successi e (\\(n-y\\)) insuccessi, la funzione nucleo di verosimiglianza (ovvero, la funzione di verosimiglianza da cui sono state escluse tutte le costanti moltiplicative che non hanno alcun effetto su \\(\\hat{\\theta}\\)) è\n\\[\n\\mathcal{L}(p \\mid y) = \\theta^y (1-\\theta)^{n - y}.\\notag\n\\tag{2.1}\\]\nPer fare un esempio pratico, consideriamo la ricerca di Zetsche et al. (2019). Questi ricercatori hanno trovato che, su 30 pazienti clinicamente depressi, 23 manifestavano delle aspettative relative al loro umore futuro distorsione negativamente. Se i dati di Zetsche et al. (2019) vengono riassunti da una proporzione (ovvero, 23/30), allora è sensato adottare un modello probabilistico binomiale quale meccanismo generatore dei dati:\n\\[\ny  \\sim \\mbox{Bin}(n, \\theta),\n\\tag{2.2}\\]\nladdove \\(\\theta\\) è la probabiltà che una prova Bernoulliana assuma il valore 1 e \\(n\\) corrisponde al numero di prove Bernoulliane. Questo modello assume che le prove Bernoulliane \\(y_i\\) che costituiscono il campione \\(y\\) siano tra loro indipendenti e che ciascuna abbia la stessa probabilità \\(\\theta \\in [0, 1]\\) di essere un “successo” (valore 1). In altre parole, il modello generatore dei dati avrà una funzione di massa di probabilità\n\\[\np(y \\mid \\theta)\n\\ = \\\n\\mbox{Bin}(y \\mid n, \\theta).\n\\]\nNei capitoli precedenti è stato mostrato come, sulla base del modello binomiale, sia possibile assegnare una probabilità a ciascun possibile valore \\(y \\in \\{0, 1, \\dots, n\\}\\) assumendo noto il valore del parametro \\(\\theta\\). Ma ora abbiamo il problema inverso, ovvero quello di fare inferenza su \\(\\theta\\) alla luce dei dati campionari \\(y\\). In altre parole, riteniamo di conoscere il modello probabilistico che ha generato i dati, ma di tale modello non conosciamo i parametri: vogliamo dunque ottenere informazioni su \\(\\theta\\) avendo osservato i dati \\(y\\). Per fare questo, in un ottica bayesiana, è innanzitutto necessario definire la funzione di verosimiglianza.\nPer i dati di Zetsche et al. (2019) la funzione di verosimiglianza corrisponde alla funzione binomiale di parametro \\(\\theta \\in [0, 1]\\) sconosciuto. Abbiamo osservato 23 successi, \\(y = 23\\), in 30 prove, \\(n = 30\\).\n\nCodicen <- 30\ny <- 23\n\n\nLa funzione di verosimiglianza dunque diventa\n\\[\n\\mathcal{L}(\\theta \\mid y) = \\frac{(23 + 7)!}{23!7!} \\theta^{23} + (1-\\theta)^7.\n\\tag{2.3}\\]\nPer costruire la funzione di verosimiglianza dobbiamo applicare l’Equazione 2.3 tante volte, cambiando ogni volta il valore \\(\\theta\\), ma tenendo sempre costante il valore dei dati. Nella simulazione considereremo 100 possibili valori \\(\\theta \\in [0, 1]\\).\n\nCodicetheta <- seq(0, 1, length.out = 100)\ntheta\n#>   [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n#>   [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n#>  [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n#>  [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n#>  [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n#>  [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n#>  [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n#>  [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n#>  [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n#>  [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n#>  [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n#>  [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n#>  [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n#>  [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n#>  [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n#>  [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n#>  [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n\nPer esempio, ponendo \\(\\theta = 0.1\\) otteniamo un valore dell’ordinata della funzione di verosimiglianza\n\\[\n\\mathcal{L}(\\theta \\mid y) = \\frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.\n\\]\n\nCodicedbinom(23, 30, 0.1)\n#> [1] 9.737168e-18\n\n\nPonendo \\(\\theta = 0.2\\) otteniamo un altro valore dell’ordinata della funzione di verosimiglianza\n\\[\n\\mathcal{L}(\\theta \\mid y) = \\frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.\n\\]\n\nCodicedbinom(23, 30, 0.2)\n#> [1] 3.581417e-11\n\n\nSe ripetiamo questo processo 100 volte, una volta per ciascuno dei valori \\(\\theta\\) considerati, otteniamo 100 coppie di punti \\(\\theta\\) e \\(f(\\theta)\\).\n\nCodicelike <- choose(n, y) * theta^y * (1 - theta)^(n - y)\n\n\nLa curva che interpola tali punti è la funzione di verosimiglianza. La Figura 2.1 fornisce una rappresentazione grafica di tale funzione.\n\nCodicetibble(theta, like) %>%\n  ggplot(aes(x = theta, y = like)) +\n  geom_line() +\n  vline_at(0.7676768, color = \"lightgray\", linetype=\"dashed\") +\n  labs(\n    y = expression(L(theta)),\n    x = expression(\"Valori possibili di\" ~ theta)\n  )\n\n\n\nFigura 2.1: Funzione di verosimiglianza nel caso di 23 successi in 30 prove.\n\n\n\n\n\n2.3.2 Interpretazione\nCome possiamo interpretare la curva che abbiamo ottenuto? Per alcuni valori \\(\\theta\\) la funzione di verosimiglianza assume valori piccoli; per altri valori \\(\\theta\\) la funzione di verosimiglianza assume valori più grandi. Questi ultimi sono i valori di \\(\\theta\\) più credibili e il valore 23/30 = 0.767 (la moda della funzione di verosimiglianza) è il valore più credibile di tutti.\n\nCodiced <- tibble(theta, like)\nd[which.max(like), ]$theta\n#> [1] 0.7676768\n\n\n\n2.3.3 La log-verosimiglianza\nDal punto di vista pratico risulta più conveniente utilizzare, al posto della funzione di verosimiglianza, il suo logaritmo naturale, ovvero la funzione di log-verosimiglianza:\n\\[\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta).\n\\tag{2.4}\\]\nPoiché il logaritmo è una funzione strettamente crescente (usualmente si considera il logaritmo naturale), allora \\(\\mathcal{L}(\\theta)\\) e \\(\\ell(\\theta)\\) assumono il massimo (o i punti di massimo) in corrispondenza degli stessi valori di \\(\\theta\\):\n\\[\n\\hat{\\theta} = \\mbox{argmax}_{\\theta \\in \\Theta} \\ell(\\theta) = \\mbox{argmax}_{\\theta \\in \\Theta} \\mathcal{L}(\\theta).\n\\]\nPer le proprietà del logaritmo, la funzione nucleo di log-verosimiglianza è\n\\[\n\\begin{aligned}\n\\ell(\\theta \\mid y) &= \\log \\mathcal{L}(\\theta \\mid y) \\notag\\\\\n          &= \\log \\left(\\theta^y (1-\\theta)^{n - y} \\right) \\notag\\\\\n          &= \\log \\theta^y + \\log \\left( (1-\\theta)^{n - y} \\right) \\notag\\\\\n          &= y \\log \\theta + (n - y) \\log (1-\\theta).\\notag\n\\end{aligned}\n\\]\nSi noti che non è necessario lavorare con i logaritmi, ma è fortemente consigliato. Il motivo è che i valori della verosimiglianza, in cui si moltiplicano valori di probabilità molto piccoli, possono diventare estremamente piccoli – qualcosa come \\(10^{-34}\\). In tali circostanze, non è sorprendente che i programmi dei computer mostrino problemi di arrotondamento numerico. Le trasformazioni logaritmiche risolvono questo problema.\nSvolgiamo nuovamente il problema precedente usando la log-verosimiglianza per trovare il massimo della funzione di log-verosimiglianza. Nella funzione dbinom() ora utilizziamo l’argomento log = TRUE.\n\nCodicell <- dbinom(y, n, theta, log = TRUE) \n\n\nLa funzione di log-verosimiglianza è rappresentata nella Figura 2.2.\n\nCodicetibble(theta, ll) %>%\n  ggplot(aes(x = theta, y = ll)) +\n  geom_line() +\n  vline_at(0.7676768, color = \"lightgray\", linetype=\"dashed\") +\n  labs(\n    y = expression(\"log-likelihood\" ~ (theta)),\n    x = expression(\"Valori possibili di\" ~ theta)\n  )\n\n\n\nFigura 2.2: Funzione di log-verosimiglianza nel caso di 23 successi in 30 prove.\n\n\n\n\nIl risultato replica quello trovato in precedenza con la funzione di verosimiglianza.\n\nCodiced <- tibble(theta, ll)\nd[which.max(ll), ]$theta\n#> [1] 0.7676768"
  },
  {
    "objectID": "024_likelihood.html#funzione-di-verosimiglianza-gaussiana",
    "href": "024_likelihood.html#funzione-di-verosimiglianza-gaussiana",
    "title": "2  La funzione di verosimiglianza",
    "section": "\n2.4 Funzione di verosimiglianza Gaussiana",
    "text": "2.4 Funzione di verosimiglianza Gaussiana\nOra che abbiamo capito come costruire la funzione verosimiglianza di una binomiale è relativamente semplice fare un passo ulteriore e considerare la verosimiglianza del caso di una funzione di densità, ovvero nel caso di una variabile casuale continua. Consideriamo qui il caso della Normale. La densità di una distribuzione Normale di parametri \\(\\mu\\) e \\(\\sigma\\) è\n\\[\nf(y \\mid \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left\\{-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right\\}.\n\\tag{2.5}\\]\nCi poniamo ora il problema di costruire la funzione di verosimiglianza per la densità dell’Equazione 2.5.\n\n2.4.1 Una singola osservazione\nEsaminiamo prima il caso in cui i dati corrispondono ad una singola osservazione \\(y\\).\n\nCodicey <- 114\n\n\nL’Equazione 2.5 dipende dai parametri \\(\\mu\\) e \\(\\sigma\\) e dai dati \\(y\\). Per semplicità, ipotizzeremo \\(\\sigma\\) noto e uguale a 15. Nella simulazione considereremo 1000 valori \\(\\mu\\) compresi tra 70 e 160.\n\nCodicemu <- seq(70, 160, length.out = 1e3)\n\n\nDato che consideriamo 1000 possibili valori \\(\\mu\\), per costruire la funzione di verosimiglianza applicheremo l’Equazione 2.5 1000 volte. In ciascun passo della simulazione inseriremo nell’Equazione 2.5\n\nil singolo valore \\(y\\) considerato,\nil valore \\(\\sigma\\) assunto noto,\nciascuno dei valori \\(\\mu\\) che abbiamo considerato.\n\nLa distribuzione Gaussiana è implementata in \\(\\mathsf{R}\\) nella funzione dnorm(). La funzione dnorm() ha tre argomenti: il valore \\(y\\) (o il vettore \\(y\\)), la media, ovvero il parametro \\(\\mu\\) e la deviazione standard, ovvero il parametro \\(\\sigma\\).\nUsando la funzione dnorm() 1000 volte, una volta per ciascuno dei valori \\(\\mu\\) che abbiamo considerato (e tenendo fissi \\(y = 114\\) e \\(\\sigma = 15\\)) otteniamo 1000 valori \\(f(\\mu)\\).\n\nCodicef_mu <- dnorm(y, mean = mu, sd = 15)\n\n\nLa funzione di verosimiglianza è la curva che interpola i punti \\(\\big(\\mu, f(\\mu)\\big)\\).\n\nCodicetibble(mu, f_mu) %>% \n  ggplot(\n    aes(x = mu, y = f_mu)\n  ) +\n    geom_line() +\n    vline_at(114, color = \"lightgray\", linetype=\"dashed\") +\n      labs(\n      y = \"Verosimiglianza\",\n      x = c(\"Parametro \\u03BC\")\n    ) \n\n\n\n\n\n\n\nSi noti che la funzione di verosimiglianza così trovata ha la forma della distribuzione Gaussiana. Nel caso di una singola osservazione (ma solo in questo caso) ha un’area unitaria.\n\nCodiceintegrand <- function(mu) {\n  y = 114\n  sigma = 15\n  dnorm(y, mu, sigma)\n}\nintegrate(integrand, lower = -10000, upper = 10000)\n#> 1 with absolute error < 1.6e-06\n\n\nLa moda della funzione di verosimiglianza è 114.\n\n2.4.2 Un campione di osservazioni\nConsideriamo ora il caso più generale, ovvero quello di un campione di \\(n\\) osservazioni. Possiamo immaginare un campione casuale \\(y_1, y_2, \\dots, y_n\\) estratto da una popolazione \\(\\mathcal{N}(\\mu, \\sigma)\\) come una sequenza di realizzazioni indipendenti ed identicamente distribuite (di seguito, i.i.d.) della medesima variabile casuale \\(Y \\sim \\mathcal{N}(\\mu, \\sigma)\\). I parametri sconosciuti sono \\(\\theta = \\{\\mu, \\sigma\\}\\).\nSe le variabili casuali \\(y_1, y_2, \\dots, y_n\\) sono i.i.d., la loro densità congiunta è data da: \\[\\begin{align}\nf(y \\mid \\theta) &= f(y_1 \\mid \\theta) \\cdot f(y_2 \\mid \\theta) \\cdot \\; \\dots \\; \\cdot f(y_n \\mid \\theta)\\notag\\\\\n                 &= \\prod_{i=1}^n f(y_i \\mid \\theta),\n\\end{align}\\]\nladdove \\(f(\\cdot)\\) è la densità Gaussiana di parametri \\(\\mu, \\sigma\\). Tenendo costanti i dati \\(y\\), la funzione di verosimiglianza diventa:\n\\[\\begin{equation}\n\\mathcal{L}(\\theta \\mid y) = \\prod_{i=1}^n f(y_i \\mid \\theta).\n\\end{equation}\\]\nPer chiarire la formula precedente, consideriamo un esempio che utilizza come dati i valori BDI-II dei trenta soggetti del campione clinico di Zetsche et al. (2020).\n\nCodiced <- tibble(\n  y = c(\n    26, 35, 30, 25, 44, 30, 33, 43, 22, 43, 24, 19, 39, 31, 25, 28, 35, 30, 26, \n    31, 41, 36, 26, 35, 33, 28, 27, 34, 27, 22\n    )\n)\n\n\nCi poniamo l’obiettivo di creare la funzione di verosimiglianza per questi dati, supponendo di sapere (in base ai risultati di ricerche precedenti) che i punteggi BDI-II si distribuiscono secondo la legge Normale e supponendo \\(\\sigma\\) noto e uguale alla deviazione standard del campione.\n\nCodicetrue_sigma <- sd(d$y)\ntrue_sigma \n#> [1] 6.606858\n\n\nAbbiamo visto in precedenza che, per una singola osservazione, la funzione di verosimiglianza è la densità Gaussiana espressa in funzione dei parametri (in questo caso, solo \\(\\mu\\)). Per un campione di osservazioni i.i.d., ovvero \\(y = (y_1, y_2, \\dots, y_n)\\), la verosimiglianza è la funzione di densità congiunta \\(f(y \\mid \\mu, \\sigma)\\) espressa in funzione dei parametri. Dato che le osservazioni sono i.i.d., la densità congiunta è data dal prodotto delle densità delle singole osservazioni.\nPer l’osservazione \\(y_i\\) abbiamo\n\\[\nf(y_i \\mid \\mu, \\sigma) = \\frac{1}{{6.61 \\sqrt {2\\pi}}}\\exp\\left\\{{-\\frac{(y_i - \\mu)^2}{2\\cdot 6.61^2}}\\right\\}.\n\\]\nLa densità congiunta è dunque\n\\[\nf(y \\mid \\mu, \\sigma) = \\, \\prod_{i=1}^n f(y_i \\mid \\mu, \\sigma)\n\\]\nAlla luce dei dati osservati (e assumendo \\(\\sigma = 6.61\\)), la verosimiglianza diventa\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mu, \\sigma \\mid y) =& \\, \\prod_{i=1}^n f(y_i \\mid \\mu, \\sigma) = \\notag\\\\\n& \\frac{1}{{6.61 \\sqrt {2\\pi}}}\\exp\\left\\{{-\\frac{(26 - \\mu)^2}{2\\cdot 6.61^2}}\\right\\} \\times \\notag\\\\\n& \\frac{1}{{6.61 \\sqrt {2\\pi}}}\\exp\\left\\{{-\\frac{(35 - \\mu)^2}{2\\cdot 6.61^2}}\\right\\} \\times  \\notag\\\\\n& \\vdots \\notag\\\\\n& \\frac{1}{{6.61 \\sqrt {2\\pi}}}\\exp\\left\\{{-\\frac{(22 - \\mu)^2}{2\\cdot 6.61^2}}\\right\\}.\n\\end{aligned}\n\\]\nÈ più conveniente svolgere l’esercizio usando il logaritmo della verosimiglianza. In \\(\\textsf{R}\\), definiamo la funzione di log-verosimiglianza, log_likelihood(), che prende come argomenti y, mu e sigma = 6.61.\n\nCodicelog_likelihood <- function(y, mu, sigma = true_sigma) {\n  sum(dnorm(y, mu, sigma, log = TRUE))\n}\n\n\nL’argomento y è un vettore di 30 elementi; gli argomenti mu e sigma sono scalari.\nPer ciascuno valore y, la funzione dnorm() trova la densità Normale utilizzando il valore mu in input e sigma = 6.61. L’argomento log = TRUE specifica che deve essere preso il logaritmo. I 30 valori così ottenuti vengono poi sommati dalla funzione sum().\nUtilizzando un singolo valore \\(\\mu\\) otteniamo l’ordinata della funzione di log-verosimiglianza in corrispondenza del valore \\(\\mu\\) utilizzato. Vogliamo però trovare l’ordinata della log-verosimiglianza per tutti i possibili valori che \\(\\mu\\) può assumere. Nella simulazione, usiamo 100,000 valori possibili del parametro \\(\\mu \\in [\\bar{y} - \\mbox{SD}, \\bar{y} + \\mbox{SD}]\\). Mediante un ciclo for(), ripetiamo dunque i calcoli descritti sopra 100,000 volte, una volta per ciascuno dei valori \\(\\mu\\) considerati. I 100,000 risultati vengono salvati nel vettore ll.\n\nCodicenrep <- 1e5\nmu <- seq(\n  mean(d$y) - sd(d$y), \n  mean(d$y) + sd(d$y), \n  length.out = nrep\n)\n\nll <- rep(NA, nrep)\nfor (i in 1:nrep) {\n  ll[i] <- log_likelihood(d$y, mu[i], true_sigma)\n}\n\n\nNel caso di un solo parametro sconosciuto (nel caso presente, \\(\\mu\\)) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (mu, ll).\n\nCodicetibble(mu, ll) %>% \nggplot(aes(x = mu, y = ll)) +\n  geom_line() +\n  vline_at(mean(d$y), color = \"gray\", linetype = \"dashed\") +\n  labs(\n    y = \"Log-verosimiglianza\",\n    x = expression(\"Parametro\"~mu)\n  ) \n\n\n\nFigura 2.3: Log-verosimiglianza del parametro \\(\\mu\\) per i dati di Zetsche et al. (2019).\n\n\n\n\nTale funzione descrive la credibilità relativa che può essere attribuita ai valori del parametro \\(\\mu\\) alla luce dei dati osservati.\n\n2.4.3 Massima verosimiglianza\nIl valore \\(\\mu\\) più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto stima di massima verosimiglianza.\nIl massimo della funzione di log-verosimiglianza, ovvero 30.93 nel caso dell’esempio presente, è identico alla media dei dati campionari. Tale risultato, ottenuto per via numerica, può essere dimostrato formalmente nel modo seguente.\nUsando la notazione matematica possiamo dire che cerchiamo l’argmax dell’equazione precedente rispetto a \\(\\theta\\), ovvero\n\\[\n\\hat{\\theta} = \\text{argmax}_{\\theta} \\prod_{i=1}^n f(y_i \\mid \\theta).\n\\]\nQuesto problema si risolve calcolando le derivate della funzione rispetto a \\(\\theta\\), ponendo le derivate uguali a zero e risolvendo. Saltando tutti i passaggi algebrici di questo procedimento, per \\(\\mu\\) troviamo\n\\[\\begin{equation}\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n y_i\n\\end{equation}\\]\ne per \\(\\sigma\\) abbiamo\n\\[\\begin{equation}\n\\hat{\\sigma} = \\sqrt{\\sum_{i=1}^n\\frac{1}{n}(y_i- \\mu)^2}.\n\\end{equation}\\]\nIn altri termini, la s.m.v. del parametro \\(\\mu\\) è la media del campione e la s.m.v. del parametro \\(\\sigma\\) è la deviazione standard del campione.\n\nEsercizio 2.1 \nDalla Figura 2.3 notiamo che il massimo della funzione di log-verosimiglianza calcolata per via numerica, ovvero 30.93, è identico alla media dei dati campionari e corrisponde al risultato teorico atteso."
  },
  {
    "objectID": "024_likelihood.html#commenti-e-considerazioni-finali",
    "href": "024_likelihood.html#commenti-e-considerazioni-finali",
    "title": "2  La funzione di verosimiglianza",
    "section": "Commenti e considerazioni finali",
    "text": "Commenti e considerazioni finali\nNella funzione di verosimiglianza i dati (osservati) vengono trattati come fissi, mentre i valori del parametro (o dei parametri) \\(\\theta\\) vengono variati: la verosimiglianza è una funzione di \\(\\theta\\) per il dato fisso \\(y\\). Pertanto, la funzione di verosimiglianza riassume i seguenti elementi: un modello statistico che genera stocasticamente i dati (in questo capitolo abbiamo esaminato due modelli statistici: quello binomiale e quello Normale), un intervallo di valori possibili per \\(\\theta\\) e i dati osservati \\(y\\).\nNella statistica frequentista l’inferenza si basa solo sui dati a disposizione e qualunque informazione fornita dalle conoscenze precedenti non viene presa in considerazione. Nello specifico, nella statistica frequentista l’inferenza viene condotta massimizzando la funzione di (log) verosimiglianza, condizionatamente ai valori assunti dalle variabili casuali campionarie. Le basi dell’inferenza frequentista, dunque, sono state riassunte in questo Capitolo. Nella statistica bayesiana, invece, l’inferenza statistica viene condotta combinando la funzione di verosimiglianza con le distribuzioni a priori dei parametri incogniti \\(\\theta\\). Ciò verrà discusso nei Capitoli successivi.\nLa differenza fondamentale tra inferenza bayesiana e frequentista è dunque che i frequentisti non ritengono utile descrivere i parametri in termini probabilistici: i parametri dei modelli statistici vengono concepiti come fissi ma sconosciuti. Nell’inferenza bayesiana, invece, i parametri sconosciuti sono intesi come delle variabili casuali e ciò consente di quantificare in termini probabilistici il nostro grado di intertezza relativamente al loro valore.\n\n\n\n\n\n\nZetsche, U., Bürkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: Biased or realistic? Journal of Abnormal Psychology, 128(7), 678–688."
  },
  {
    "objectID": "999_refs.html",
    "href": "999_refs.html",
    "title": "Riferimenti bibliografici",
    "section": "",
    "text": "McElreath, R. (2020). Statistical rethinking: A\nBayesian course with examples in R and\nStan (2nd Edition). CRC Press.\n\n\nNuggerud-Galeas, S., Sáez-Benito Suescun, L., Berenguer Torrijo, N.,\nSáez-Benito Suescun, A., Aguilar-Latorre, A., Magallón Botaya, R., &\nOliván Blázquez, B. (2020). Analysis of depressive episodes, their\nrecurrence and pharmacologic treatment in primary care patients: A\nretrospective descriptive study. Plos One, 15(5),\ne0233454.\n\n\nZetsche, U., Bürkner, P.-C., & Renneberg, B. (2019). Future\nexpectations in clinical depression: Biased or realistic?\nJournal of Abnormal Psychology, 128(7), 678–688."
  }
]